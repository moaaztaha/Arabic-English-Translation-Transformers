{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "simple transformer_other_dataset_rmsprop.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R53kjrYGpSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbae9b3-14d3-4ee0-89bf-aa0b104a2916"
      },
      "source": [
        "!git clone https://github.com/moaaztaha/Arabic-English-Translation-Transformers"
      ],
      "id": "0R53kjrYGpSc",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Arabic-English-Translation-Transformers'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 24 (delta 0), reused 24 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9e97fa1"
      },
      "source": [
        "# modules\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "id": "e9e97fa1",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1710411f"
      },
      "source": [
        "### Data Preprocessing "
      ],
      "id": "1710411f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RNDMX9cizGW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "7e416132-5e7c-4191-9814-743257d554cd"
      },
      "source": [
        "en = pd.read_table('/content/Arabic-English-Translation-Transformers/data/eng/ac-test.en', delimiter='\\\\n', names=['en'])\n",
        "ar = pd.read_table('/content/Arabic-English-Translation-Transformers/data/ara/test.en_ref.ar', delimiter='\\\\n', names=['ar'])\n",
        "en['ar'] = ar['ar']\n",
        "df = en.copy()\n",
        "df.head()"
      ],
      "id": "1RNDMX9cizGW",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return read_csv(**locals())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>ar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>THE COUNCIL OF THE EUROPEAN ECONOMIC COMMUNITY,</td>\n",
              "      <td>مجلس الجماعة الاقتصادية الأوروبية</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Whereas the adoption of a common transport pol...</td>\n",
              "      <td>حيث أن اعتماد سياسة نقل مشتركة تنطوي من بين أم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Article 1</td>\n",
              "      <td>المادة 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3. The types of carriage listed in Annex II sh...</td>\n",
              "      <td>3. لا تخضع أنواع النقل المدرجة في الملحق الثان...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Member States shall inform the Commission of t...</td>\n",
              "      <td>تبلغ الدول الأعضاء المفوضية الأوروبية بالتدابي...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  en                                                 ar\n",
              "0    THE COUNCIL OF THE EUROPEAN ECONOMIC COMMUNITY,                  مجلس الجماعة الاقتصادية الأوروبية\n",
              "1  Whereas the adoption of a common transport pol...  حيث أن اعتماد سياسة نقل مشتركة تنطوي من بين أم...\n",
              "2                                          Article 1                                           المادة 1\n",
              "3  3. The types of carriage listed in Annex II sh...  3. لا تخضع أنواع النقل المدرجة في الملحق الثان...\n",
              "4  Member States shall inform the Commission of t...  تبلغ الدول الأعضاء المفوضية الأوروبية بالتدابي..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be905b2c"
      },
      "source": [
        "text_pairs = []\n",
        "for idx, row in df.iterrows():\n",
        "    # split sentences\n",
        "    if '.' in row['en'] and '.' in row['ar'] and len(row['en'].split()) > 100:\n",
        "        en_sents = row['en'].split('.')\n",
        "        ar_sents = row['ar'].split('.')\n",
        "    \n",
        "        for en_sent, ar_sent in zip(en_sents, ar_sents):\n",
        "            ar_sent = \"[start] \" + ar_sent + \" [end]\"\n",
        "            text_pairs.append((en_sent, ar))\n",
        "    else:\n",
        "        en, ar = row['en'], row['ar']\n",
        "        ar = \"[start] \" + ar + \" [end]\"\n",
        "        text_pairs.append((en, ar))"
      ],
      "id": "be905b2c",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "296e4594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d55f0f3-8804-42bd-a864-4c1687842c92"
      },
      "source": [
        "for _ in range(2):\n",
        "    print(random.choice(text_pairs))"
      ],
      "id": "296e4594",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('(8) In the light of new scientific knowledge, testing methods should be reviewed, including testing methods for analysing 4-amino azobenzene.', '[start] (8) في ضوء المعرفة العلمية الجديدة، ينبغي إعادة النظر في أساليب الاختبار، بما في ذلك أساليب الاختبار المخصصة لتحليل الأزوبنزين الأميني 4. [end]')\n",
            "('Member States shall communicate to the Commission provisions laid down by law, regulation or administrative action which they have adopted for the application of legal acts of the Community relating to the common agricultural policy in so far as those acts have financial consequences for the Fund.', '[start] تقوم الدول الأعضاء بالإرسال إلى المفوضية جميع الأحكام التي نص عليها القانون واللوائح أو الإجراء الإداري الذي قاموا بالتصديق عليه لتطبيق الأعمال القانونية الخاصة بالمجموعة فيما يتعلق بالسياسة الزراعية المشتركة طالما أن هذه الأعمال لديها نتائج مالية خاصة بالصندوق. [end]')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1283f6f"
      },
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) -  num_val_samples\n",
        "train_pairs = text_pairs[: num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]"
      ],
      "id": "c1283f6f",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7a1f8cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d6e729-e32e-46bd-c6d3-95dd2a84d97d"
      },
      "source": [
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ],
      "id": "a7a1f8cd",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4262 total pairs\n",
            "3623 training pairs\n",
            "639 validation pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5f6d84e"
      },
      "source": [
        "#### Vectorizing the text data "
      ],
      "id": "d5f6d84e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487ed66f"
      },
      "source": [
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "vocab_size = 25000\n",
        "sequence_length = 100\n",
        "batch_size = 128\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    return tf.strings.regex_replace(input_string, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "            max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\n",
        "\n",
        "ar_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization)\n",
        "\n",
        "eng_texts = [pair[0] for pair in text_pairs]\n",
        "ar_texts = [pair[1] for pair in text_pairs]\n",
        "eng_vectorization.adapt(eng_texts)\n",
        "ar_vectorization.adapt(ar_texts)"
      ],
      "id": "487ed66f",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5qiqjOWdVz1"
      },
      "source": [
        "def format_dataset(eng, ar):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ar = ar_vectorization(ar)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ar_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ar_texts = list(ar_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "id": "V5qiqjOWdVz1",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a3ed140"
      },
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "id": "7a3ed140",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3902789f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0a46f57-8f88-425c-a833-1ce033ebfa04"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "id": "3902789f",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (128, 100)\n",
            "inputs[\"decoder_inputs\"].shape: (128, 100)\n",
            "targets.shape: (128, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fa0936c"
      },
      "source": [
        "### Building the Model "
      ],
      "id": "4fa0936c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3baf452"
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "          'embed_dim': self.embed_dim,\n",
        "          'dense_dim': self.dense_dim,\n",
        "          'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "      \n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'sequence_length': self.sequence_length,\n",
        "      'vocab_size': self.vocab_size,\n",
        "      'embed_dim': self.embed_dim,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'embed_dim': self.embed_dim,\n",
        "      'latent_dim': self.latent_dim,\n",
        "      'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "id": "b3baf452",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c57e1e"
      },
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "id": "04c57e1e",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt9sp6Yrb4pE"
      },
      "source": [
        "googledrive_path = '/content/drive/MyDrive/Transformers/new_ds_25k/'"
      ],
      "id": "vt9sp6Yrb4pE",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqmaEQ94kyB9"
      },
      "source": [
        "from keras import callbacks\n",
        "early_stopping_cb = callbacks.EarlyStopping(monitor='val_accuracy', patience=4, verbose=True)\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(googledrive_path+'weights_rmsprop.ckpt', monitor='val_accuracy', save_weights_only=True,verbose=True, save_best_only=True)\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=googledrive_path+\"logs\")\n",
        "cbs = [early_stopping_cb, checkpoint_cb, tensorboard_callback]"
      ],
      "id": "wqmaEQ94kyB9",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50DkdfyLyqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1943c6-03a5-453f-b5c9-4fe173167b0f"
      },
      "source": [
        "epochs = 30  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
      ],
      "id": "H50DkdfyLyqt",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding_2 (Positio (None, None, 256)    6425600     encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_1 (Transfor (None, None, 256)    3155456     positional_embedding_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "model_3 (Functional)            (None, None, 25000)  18110120    decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder_1[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 27,691,176\n",
            "Trainable params: 27,691,176\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "29/29 [==============================] - 23s 696ms/step - loss: 1.8421 - accuracy: 0.0606 - val_loss: 1.6587 - val_accuracy: 0.0764\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.07640, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 2/30\n",
            "29/29 [==============================] - 18s 618ms/step - loss: 1.6498 - accuracy: 0.1007 - val_loss: 1.5654 - val_accuracy: 0.1225\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.07640 to 0.12247, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 3/30\n",
            "29/29 [==============================] - 18s 639ms/step - loss: 1.5035 - accuracy: 0.1684 - val_loss: 1.4114 - val_accuracy: 0.2158\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.12247 to 0.21584, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 4/30\n",
            "29/29 [==============================] - 18s 625ms/step - loss: 1.3681 - accuracy: 0.2178 - val_loss: 1.3366 - val_accuracy: 0.2505\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.21584 to 0.25055, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 5/30\n",
            "29/29 [==============================] - 18s 615ms/step - loss: 1.2385 - accuracy: 0.2635 - val_loss: 1.2796 - val_accuracy: 0.2744\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.25055 to 0.27444, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 6/30\n",
            "29/29 [==============================] - 18s 618ms/step - loss: 1.1371 - accuracy: 0.3022 - val_loss: 1.2432 - val_accuracy: 0.2969\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.27444 to 0.29689, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 7/30\n",
            "29/29 [==============================] - 18s 624ms/step - loss: 1.0522 - accuracy: 0.3347 - val_loss: 1.2176 - val_accuracy: 0.3062\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.29689 to 0.30620, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 8/30\n",
            "29/29 [==============================] - 18s 626ms/step - loss: 0.9827 - accuracy: 0.3597 - val_loss: 1.1997 - val_accuracy: 0.3178\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.30620 to 0.31784, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 9/30\n",
            "29/29 [==============================] - 18s 619ms/step - loss: 0.9213 - accuracy: 0.3850 - val_loss: 1.1970 - val_accuracy: 0.3237\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.31784 to 0.32373, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 10/30\n",
            "29/29 [==============================] - 18s 618ms/step - loss: 0.8587 - accuracy: 0.4137 - val_loss: 1.1775 - val_accuracy: 0.3343\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.32373 to 0.33427, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 11/30\n",
            "29/29 [==============================] - 18s 624ms/step - loss: 0.8007 - accuracy: 0.4430 - val_loss: 1.1955 - val_accuracy: 0.3404\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.33427 to 0.34036, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 12/30\n",
            "29/29 [==============================] - 18s 624ms/step - loss: 0.7556 - accuracy: 0.4678 - val_loss: 1.1726 - val_accuracy: 0.3379\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.34036\n",
            "Epoch 13/30\n",
            "29/29 [==============================] - 18s 623ms/step - loss: 0.7055 - accuracy: 0.4947 - val_loss: 1.1885 - val_accuracy: 0.3515\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.34036 to 0.35145, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 14/30\n",
            "29/29 [==============================] - 18s 623ms/step - loss: 0.6512 - accuracy: 0.5269 - val_loss: 1.1997 - val_accuracy: 0.3384\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.35145\n",
            "Epoch 15/30\n",
            "29/29 [==============================] - 18s 624ms/step - loss: 0.6214 - accuracy: 0.5439 - val_loss: 1.1782 - val_accuracy: 0.3473\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.35145\n",
            "Epoch 16/30\n",
            "29/29 [==============================] - 18s 623ms/step - loss: 0.5631 - accuracy: 0.5777 - val_loss: 1.1993 - val_accuracy: 0.3528\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.35145 to 0.35275, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 17/30\n",
            "29/29 [==============================] - 18s 624ms/step - loss: 0.5519 - accuracy: 0.5869 - val_loss: 1.1873 - val_accuracy: 0.3558\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.35275 to 0.35576, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 18/30\n",
            "29/29 [==============================] - 18s 619ms/step - loss: 0.4873 - accuracy: 0.6261 - val_loss: 1.2023 - val_accuracy: 0.3621\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.35576 to 0.36213, saving model to /content/drive/MyDrive/Transformers/new_ds_25k/weights_rmsprop.ckpt\n",
            "Epoch 19/30\n",
            "29/29 [==============================] - 18s 624ms/step - loss: 0.4591 - accuracy: 0.6443 - val_loss: 1.2366 - val_accuracy: 0.3556\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.36213\n",
            "Epoch 20/30\n",
            "29/29 [==============================] - 18s 627ms/step - loss: 0.4150 - accuracy: 0.6722 - val_loss: 1.2235 - val_accuracy: 0.3528\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.36213\n",
            "Epoch 21/30\n",
            "29/29 [==============================] - 18s 622ms/step - loss: 0.3819 - accuracy: 0.7000 - val_loss: 1.2541 - val_accuracy: 0.3451\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.36213\n",
            "Epoch 22/30\n",
            "29/29 [==============================] - 18s 621ms/step - loss: 0.3490 - accuracy: 0.7256 - val_loss: 1.2582 - val_accuracy: 0.3613\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.36213\n",
            "Epoch 00022: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9320022410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5Qvebl8FHsG",
        "outputId": "8e8a2c79-bd40-47fb-b7f6-30a11f973564"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(googledrive_path)\n",
        "transformer.load_weights(latest)"
      ],
      "id": "j5Qvebl8FHsG",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f92d8d00e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8447490b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0751103f-873b-46ba-e4d4-fa8158bba7db"
      },
      "source": [
        "ar_vocab = ar_vectorization.get_vocabulary()\n",
        "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in val_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts[:30])\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, '\\n', translated)\n",
        "    print('*'*50)"
      ],
      "id": "8447490b",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article 10 \n",
            " [start] المادة 10 [end]\n",
            "**************************************************\n",
            "19. Articles 247, 248 and 249 shall be replaced by the following: \n",
            " [start] 19 يجب على الأقل لا يمكن أن لا يمكن أن تسمح لصناديق التزامات الاستثمار [end]\n",
            "**************************************************\n",
            "amending the Annexes to Council Directives 76/895/EEC, 86/362/EEC, 86/363/EEC and 90/642/EEC as regards the fixing of maximum levels for certain pesticide residues in and on cereals, foodstuffs of animal origin and certain products of plant origin, including fruit and vegetables \n",
            " [start] اللائحة المعدلة رقم 9992001 الصادرة عن المجلس رقم 237790 بتاريخ 14 يونيوحزيران 1990 المتعلق بتعديل النظام في ما يتعلق بالمنتجات\n",
            "**************************************************\n",
            "Having regard to Council Regulation (EC) No 2991/94 of 5 December 1994 laying down standards for spreadable fats (1), and in particular Article 8 thereof, \n",
            " [start] مع الأخذ في الاعتبار نظام المجلس المفوضية الأوروبية EEC رقم 5 ديسمبركانون الأول 1994 بشأن بعض القواعد المتعلقة بتطبيق لائحة\n",
            "**************************************************\n",
            "2. The list of goods in respect of which the temporary importation procedure with partial relief from import duties may not be used and the conditions subject to which the procedure may be used shall be determined in accordance with the committee procedure.\"; \n",
            " [start] 2 يجب أن تكون هذه اللائحة المفوضية الأوروبية بانتظام بنتائج التفتيشات أو المراقبة التي أجريت وفقاً لهذا النظام، بما في\n",
            "**************************************************\n",
            "Before 1 June each year Member States shall transmit to the Commission a report on the application of this Regulation over the previous calendar year, containing an assessment of the technical and human resources used and measures which may help to alleviate any shortcomings discovered \n",
            " [start] قبل 1 ينايركانون الثاني 1997، يتعين على الدول الأعضاء إبلاغ المفوضية الأوروبية [end]\n",
            "**************************************************\n",
            "Article 3 \n",
            " [start] المادة 3 [end]\n",
            "**************************************************\n",
            "- superficial skin defects, provided that the total area affected does not exceed 1 cm2; \n",
            " [start] يجب أن تكون في كل من أن تكون قائمة على النحو المنصوص عليها في المادة 6 [end]\n",
            "**************************************************\n",
            " \n",
            " [start] المادة 6 [end]\n",
            "**************************************************\n",
            "COUNCIL DIRECTIVE of 27 July 1976 on the approximation of the laws of the Member States relating to alcoholometers and alcohol hydrometers (76/765/EEC) \n",
            " [start] توجيه المجلس في 24 يوليو 1975 بشأن من المفوضية الأوروبية بشأن التدابير المتعلقة بوضع بطاقات تعريف للمواد الفعالة المعنية على\n",
            "**************************************************\n",
            "5. The data referred to in paragraph 1 shall not be used for any purpose other than that provided for in this Regulation unless the authorities providing the data give their express consent and on condition that the provisions in force in the Member State of the authority receiving the data do not prohibit such use or communication. \n",
            " [start] 5 يتعين على البيانات المشار إليها في الفقرة 1 في الفقرة 1 من خلال شخص يتصرف باسمه ولكن نيابة عن\n",
            "**************************************************\n",
            "Article 10 \n",
            " [start] المادة 10 [end]\n",
            "**************************************************\n",
            " \n",
            " [start] المادة 6 [end]\n",
            "**************************************************\n",
            "Directive 92/6/EEC is hereby amended as follows: \n",
            " [start] يتم تعديل التوجيه رقم هذا على الشكل التالي [end]\n",
            "**************************************************\n",
            "Article 3 \n",
            " [start] المادة 3 [end]\n",
            "**************************************************\n",
            "Whereas, in establishing maximum residue limits for residues of veterinary medicinal products in foodstuffs of animal origin, it is necessary to specify the animal species in which residues may be present, the levels which may be present in each of the relevant meat tissues obtained from the treated animal (target tissue) and the nature of the residue which is relevant for the monitoring of residues (marker residue); \n",
            " [start] حيث أنه في الجماعة من المنتجات الطبية البيطرية المخصصة للعام الذي يتم الحصول على المواد الغذائية من الضروري تحديد مستويات\n",
            "**************************************************\n",
            "5. The data referred to in paragraph 1 shall not be used for any purpose other than that provided for in this Regulation unless the authorities providing the data give their express consent and on condition that the provisions in force in the Member State of the authority receiving the data do not prohibit such use or communication. \n",
            " [start] 5 يتعين على البيانات المشار إليها في الفقرة 1 في الفقرة 1 من خلال شخص يتصرف باسمه ولكن نيابة عن\n",
            "**************************************************\n",
            "THE COMMISSION OF THE EUROPEAN COMMUNITIES, \n",
            " [start] المفوضية الأوروبية للجماعات الأوروبية، [end]\n",
            "**************************************************\n",
            "Whereas Council Directive 72/276/EEC of 17 July 1972 on the approximation of the laws of the Member States relating to certain methods for the quantitative analysis of binary textile fibre mixtures (4) has been amended frequently and substantially; whereas, for reasons of clarity and rationality, the said Directive should be consolidated; \n",
            " [start] حيث إن التوجيه ‎72425EEC من 17 من المفوضية بشأن حماية حقوق معاشات التقاعد الذي تم الحصول على الدول الأعضاء المتعلقة\n",
            "**************************************************\n",
            "Having regard to Council Directive 77/93/EEC of 21 December 1976 on protective measures against the introduction into the Member States of organisms harmful to plants or plant products (1), as last amended by Commission Directive 91/27/EEC (2), and in particular Annex III, part B (10) thereof, \n",
            " [start] مع الأخذ في الاعتبار التوجيه الصادر عن المجلس EEC رقم 237790 بتاريخ 21 ديسمبركانون الأول 1997 بشأن التنظيم المشترك لسوق\n",
            "**************************************************\n",
            "- the index represents an adequate benchmark for the market to which it refers, \n",
            " [start] يجب أن يكون من قبل السلطة المختصة ذات الصلة في ما يتعلق [end]\n",
            "**************************************************\n",
            "25. Article 10 (1) shall be replaced by the following: \n",
            " [start] 25 يجب استبدال المادة 10 1 بما يلي [end]\n",
            "**************************************************\n",
            "THE COMMISSION OF THE EUROPEAN COMMUNITIES, \n",
            " [start] المفوضية الأوروبية للجماعات الأوروبية، [end]\n",
            "**************************************************\n",
            "Whereas, in establishing maximum residue limits for residues of veterinary medicinal products in foodstuffs of animal origin, it is necessary to specify the animal species in which residues may be present, the levels which may be present in each of the relevant meat tissues obtained from the treated animal (target tissue) and the nature of the residue which is relevant for the monitoring of residues (marker residue); \n",
            " [start] حيث أنه في الجماعة من المنتجات الطبية البيطرية المخصصة للعام الذي يتم الحصول على المواد الغذائية من الضروري تحديد مستويات\n",
            "**************************************************\n",
            "SECTION 1 \n",
            " [start] القسم 1 [end]\n",
            "**************************************************\n",
            "CHAPTER V \n",
            " [start] الفصل 5 [end]\n",
            "**************************************************\n",
            "Article 7 \n",
            " [start] المادة 7 [end]\n",
            "**************************************************\n",
            "Having regard to Council Directive 77/93/EEC of 21 December 1976 on protective measures against the introduction into the Member States of organisms harmful to plants or plant products (1), as last amended by Commission Directive 91/27/EEC (2), and in particular Annex III, part B (10) thereof, \n",
            " [start] مع الأخذ في الاعتبار التوجيه الصادر عن المجلس EEC رقم 237790 بتاريخ 21 ديسمبركانون الأول 1997 بشأن التنظيم المشترك لسوق\n",
            "**************************************************\n",
            "- where appropriate, for particular types of packaging such as mobile gas cylinders, in accordance with the specific requirements referred to in Annex VI. \n",
            " [start] حيث أنه، في حالة وجود منتجات النقل أو هذه الوثيقة على النحو المحدد في الملحق I [end]\n",
            "**************************************************\n",
            "Having regard to Council Regulation (EC) No 2991/94 of 5 December 1994 laying down standards for spreadable fats (1), and in particular Article 8 thereof, \n",
            " [start] مع الأخذ في الاعتبار نظام المجلس المفوضية الأوروبية EEC رقم 5 ديسمبركانون الأول 1994 بشأن بعض القواعد المتعلقة بتطبيق لائحة\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_QDHnhO1lW_"
      },
      "source": [
        ""
      ],
      "id": "D_QDHnhO1lW_",
      "execution_count": null,
      "outputs": []
    }
  ]
}