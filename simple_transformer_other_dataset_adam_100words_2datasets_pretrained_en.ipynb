{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "simple transformer_other_dataset_adam_100words_2datasets_pretrained_en.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R53kjrYGpSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe4858d1-50a3-45ae-a5fb-0421a6c9ae68"
      },
      "source": [
        "!git clone https://github.com/moaaztaha/Arabic-English-Translation-Transformers"
      ],
      "id": "0R53kjrYGpSc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Arabic-English-Translation-Transformers'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 36 (delta 11), reused 32 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (36/36), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9e97fa1"
      },
      "source": [
        "# modules\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "id": "e9e97fa1",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1710411f"
      },
      "source": [
        "### Data Preprocessing "
      ],
      "id": "1710411f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RNDMX9cizGW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "4411f717-0e7c-4b11-c67a-843451ec8151"
      },
      "source": [
        "en = pd.read_table('/content/Arabic-English-Translation-Transformers/data/eng/ac-test.en', delimiter='\\\\n', names=['en'])\n",
        "ar = pd.read_table('/content/Arabic-English-Translation-Transformers/data/ara/test.en_ref.ar', delimiter='\\\\n', names=['ar'])\n",
        "en['ar'] = ar['ar']\n",
        "df = en.copy()\n",
        "df.head()"
      ],
      "id": "1RNDMX9cizGW",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return read_csv(**locals())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>ar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>THE COUNCIL OF THE EUROPEAN ECONOMIC COMMUNITY,</td>\n",
              "      <td>مجلس الجماعة الاقتصادية الأوروبية</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Whereas the adoption of a common transport pol...</td>\n",
              "      <td>حيث أن اعتماد سياسة نقل مشتركة تنطوي من بين أم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Article 1</td>\n",
              "      <td>المادة 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3. The types of carriage listed in Annex II sh...</td>\n",
              "      <td>3. لا تخضع أنواع النقل المدرجة في الملحق الثان...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Member States shall inform the Commission of t...</td>\n",
              "      <td>تبلغ الدول الأعضاء المفوضية الأوروبية بالتدابي...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  en                                                 ar\n",
              "0    THE COUNCIL OF THE EUROPEAN ECONOMIC COMMUNITY,                  مجلس الجماعة الاقتصادية الأوروبية\n",
              "1  Whereas the adoption of a common transport pol...  حيث أن اعتماد سياسة نقل مشتركة تنطوي من بين أم...\n",
              "2                                          Article 1                                           المادة 1\n",
              "3  3. The types of carriage listed in Annex II sh...  3. لا تخضع أنواع النقل المدرجة في الملحق الثان...\n",
              "4  Member States shall inform the Commission of t...  تبلغ الدول الأعضاء المفوضية الأوروبية بالتدابي..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be905b2c"
      },
      "source": [
        "text_pairs = []\n",
        "for idx, row in df.iterrows():\n",
        "    # split sentences\n",
        "    if '.' in row['en'] and '.' in row['ar'] and len(row['en'].split()) > 100:\n",
        "        en_sents = row['en'].split('.')\n",
        "        ar_sents = row['ar'].split('.')\n",
        "    \n",
        "        for en_sent, ar_sent in zip(en_sents, ar_sents):\n",
        "            ar_sent = \"[start] \" + ar_sent + \" [end]\"\n",
        "            text_pairs.append((en_sent, ar))\n",
        "    else:\n",
        "        en, ar = row['en'], row['ar']\n",
        "        ar = \"[start] \" + ar + \" [end]\"\n",
        "        text_pairs.append((en, ar))"
      ],
      "id": "be905b2c",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "296e4594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab190d8c-8072-4a2c-ed07-ff91285fa603"
      },
      "source": [
        "for _ in range(2):\n",
        "    print(random.choice(text_pairs))"
      ],
      "id": "296e4594",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Article 2', '[start] المادة 2 [end]')\n",
            "('2. Member States shall also notify the Commission every month of the quantities sold during the previous month which may qualifiy for the allowance, broken down by commercial category and type of processing carried out, and of the expenditure relating to the grant of the allowance in question.', '[start] 2. يجب على الدول الأعضاء أيضاً إبلاغ المفوضية الأوروبية كل شهر بالكميات المباعة خلال الشهر السابق والتي قد تؤهل إلى الحصول على بدلات، مع توزيعها بحسب الفئة التجارية ونوع عملية المعالجة، وبالنفقات المتعلقة بمنح البدل المعني. [end]')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6kRoro9iBOe",
        "outputId": "fb964ff8-9ae6-4e8f-d8b1-3b228d59b161"
      },
      "source": [
        "len(text_pairs)"
      ],
      "id": "o6kRoro9iBOe",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "c9H3kM4OiDWv",
        "outputId": "68b5b93d-17bf-43d3-c2f0-ba5117403d76"
      },
      "source": [
        "en = pd.read_table('/content/Arabic-English-Translation-Transformers/data/eng/ac-dev.en', delimiter='\\\\n', names=['en'])\n",
        "ar = pd.read_table('/content/Arabic-English-Translation-Transformers/data/ara/tune.en_ref.ar', delimiter='\\\\n', names=['ar'])\n",
        "#2725 to 3742\n",
        "ar.drop(ar.loc[2725:3742].index,inplace=True)\n",
        "#2720 to 3707\n",
        "en.drop(en.loc[2725:3742].index,inplace=True)\n",
        "en['ar'] = ar['ar']\n",
        "df = en.copy()\n",
        "df.head()"
      ],
      "id": "c9H3kM4OiDWv",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return read_csv(**locals())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>ar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Having regard to the Treaty establishing the E...</td>\n",
              "      <td>مع الأخذ في الاعتبار المعاهدة التي أنشئت بموجب...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Whereas the progressive establishment of the c...</td>\n",
              "      <td>وحيث أنه لا يجب أن تواجه عملية الإنشاء التدريج...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1. Each Member State shall, by the end of 1962...</td>\n",
              "      <td>1. يتعيّن على كلّ دولة عضو بحلول نهاية العام 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4. The two Annexes to this Directive shall for...</td>\n",
              "      <td>4 يشكّل الملحقان المرفقان بهذا التوجيه جزءاً ل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Article 3</td>\n",
              "      <td>المادة 3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  en                                                 ar\n",
              "0  Having regard to the Treaty establishing the E...  مع الأخذ في الاعتبار المعاهدة التي أنشئت بموجب...\n",
              "1  Whereas the progressive establishment of the c...  وحيث أنه لا يجب أن تواجه عملية الإنشاء التدريج...\n",
              "2  1. Each Member State shall, by the end of 1962...  1. يتعيّن على كلّ دولة عضو بحلول نهاية العام 1...\n",
              "3  4. The two Annexes to this Directive shall for...  4 يشكّل الملحقان المرفقان بهذا التوجيه جزءاً ل...\n",
              "4                                          Article 3                                           المادة 3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEsz9VaiiNfw"
      },
      "source": [
        "for idx, row in df.iterrows():\n",
        "    # split sentences\n",
        "    if '.' in row['en'] and '.' in row['ar'] and len(row['en'].split()) > 100:\n",
        "        en_sents = row['en'].split('.')\n",
        "        ar_sents = row['ar'].split('.')\n",
        "    \n",
        "        for en_sent, ar_sent in zip(en_sents, ar_sents):\n",
        "            ar_sent = \"[start] \" + ar_sent + \" [end]\"\n",
        "            text_pairs.append((en_sent, ar))\n",
        "    else:\n",
        "        en, ar = row['en'], row['ar']\n",
        "        ar = \"[start] \" + ar + \" [end]\"\n",
        "        text_pairs.append((en, ar))"
      ],
      "id": "yEsz9VaiiNfw",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBeN07JJRt_y",
        "outputId": "51fd95d0-9bd7-49bd-fb15-f7ea7f192e73"
      },
      "source": [
        "random.choice(text_pairs)"
      ],
      "id": "qBeN07JJRt_y",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('(f) the product or its ingredients have not been subjected to treatments involving the use of ionizing radiation;',\n",
              " '[start] (و) لم يخضع المنتج أو مكوناته للعلاجات التي تنطوي على استخدام  الإشعاعات المؤينة. [end]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1283f6f"
      },
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) -  num_val_samples\n",
        "train_pairs = text_pairs[: num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]"
      ],
      "id": "c1283f6f",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7a1f8cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857e00f9-f59b-45ec-ba3b-efc9e833e7eb"
      },
      "source": [
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ],
      "id": "a7a1f8cd",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7421 total pairs\n",
            "6308 training pairs\n",
            "1113 validation pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5f6d84e"
      },
      "source": [
        "#### Vectorizing the text data "
      ],
      "id": "d5f6d84e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487ed66f"
      },
      "source": [
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "# vocab_size = 10000\n",
        "sequence_length = 50\n",
        "batch_size = 265\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    return tf.strings.regex_replace(input_string, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "            # max_tokens=vocab_size, \n",
        "            output_mode='int', \n",
        "            output_sequence_length=sequence_length)\n",
        "\n",
        "ar_vectorization = TextVectorization(\n",
        "    # max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization)\n",
        "\n",
        "eng_texts = [pair[0] for pair in text_pairs]\n",
        "ar_texts = [pair[1] for pair in text_pairs]\n",
        "eng_vectorization.adapt(eng_texts)\n",
        "ar_vectorization.adapt(ar_texts)"
      ],
      "id": "487ed66f",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5qiqjOWdVz1"
      },
      "source": [
        "def format_dataset(eng, ar):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ar = ar_vectorization(ar)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ar_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ar_texts = list(ar_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "id": "V5qiqjOWdVz1",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a3ed140"
      },
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "id": "7a3ed140",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3902789f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67a9d08f-0dc4-4705-ccda-87f213713184"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "id": "3902789f",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (265, 50)\n",
            "inputs[\"decoder_inputs\"].shape: (265, 50)\n",
            "targets.shape: (265, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fa0936c"
      },
      "source": [
        "### Building the Model "
      ],
      "id": "4fa0936c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3baf452"
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "          'embed_dim': self.embed_dim,\n",
        "          'dense_dim': self.dense_dim,\n",
        "          'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, pretrained=False, weights=False, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        if not pretrained:\n",
        "          self.token_embeddings = layers.Embedding(\n",
        "              input_dim=vocab_size, output_dim=embed_dim\n",
        "          )\n",
        "        else:\n",
        "          # pre-trained\n",
        "          self.token_embeddings = layers.Embedding(\n",
        "              input_dim=vocab_size, output_dim=embed_dim, weights=[weights]\n",
        "          ) \n",
        "\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "      \n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'sequence_length': self.sequence_length,\n",
        "      'vocab_size': self.vocab_size,\n",
        "      'embed_dim': self.embed_dim,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'embed_dim': self.embed_dim,\n",
        "      'latent_dim': self.latent_dim,\n",
        "      'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "id": "b3baf452",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpXVu3eA_b9P"
      },
      "source": [
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip -q glove.6B.zip"
      ],
      "id": "fpXVu3eA_b9P",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afjYy3g6BM2K",
        "outputId": "53bf18a2-0eae-4920-876d-0523871e14bf"
      },
      "source": [
        "import os\n",
        "path_to_glove_file = os.path.join(\n",
        "    os.path.expanduser(\"~\"), \"/content/glove.6B.300d.txt\"\n",
        ")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "id": "afjYy3g6BM2K",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRNRUzOCtccc"
      },
      "source": [
        "vocab = eng_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocab, range(len(vocab))))"
      ],
      "id": "XRNRUzOCtccc",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_rOhjwPuqkN",
        "outputId": "b8bbb9e0-6ce1-4c21-f9b3-eeaec5fbc4fc"
      },
      "source": [
        "num_tokens = len(vocab)\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        embedding_matrix[i] = np.random.uniform(-.1, .1, size=(embedding_dim))\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "id": "o_rOhjwPuqkN",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 6455 words (1522 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWvsoAzszKiS",
        "outputId": "e23b9398-8260-417f-f412-47c468c58d94"
      },
      "source": [
        "num_tokens"
      ],
      "id": "XWvsoAzszKiS",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcFB7h_o0JZC",
        "outputId": "c3e0aaff-bb65-42d9-e1ad-96a7f6671964"
      },
      "source": [
        "len(vocab)"
      ],
      "id": "FcFB7h_o0JZC",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7977"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0rA0C4s1OVW"
      },
      "source": [
        "ar_vocab_size = len(ar_vectorization.get_vocabulary())"
      ],
      "id": "U0rA0C4s1OVW",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c57e1e"
      },
      "source": [
        "embed_dim = 300\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, num_tokens, embed_dim, pretrained=True, weights=embedding_matrix)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(ar_vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "id": "04c57e1e",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt9sp6Yrb4pE"
      },
      "source": [
        "googledrive_path = '/content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/'"
      ],
      "id": "vt9sp6Yrb4pE",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqmaEQ94kyB9"
      },
      "source": [
        "from keras import callbacks\n",
        "early_stopping_cb = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=True)\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(googledrive_path+'weights_adam.ckpt', monitor='val_accuracy', save_weights_only=True,verbose=True, save_best_only=True)\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=googledrive_path+\"logs\")\n",
        "cbs = [early_stopping_cb, checkpoint_cb, tensorboard_callback]"
      ],
      "id": "wqmaEQ94kyB9",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50DkdfyLyqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b8ba93-7837-49cf-9171-e73786af387f"
      },
      "source": [
        "epochs = 100  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
      ],
      "id": "H50DkdfyLyqt",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding_4 (Positio (None, None, 300)    2408100     encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_2 (Transfor (None, None, 300)    4119848     positional_embedding_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "model_5 (Functional)            (None, None, 18492)  18136640    decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder_2[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 24,664,588\n",
            "Trainable params: 24,664,588\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "24/24 [==============================] - 22s 828ms/step - loss: 3.4499 - accuracy: 0.0881 - val_loss: 2.9362 - val_accuracy: 0.1170\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.11701, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 2/100\n",
            "24/24 [==============================] - 18s 755ms/step - loss: 2.8484 - accuracy: 0.1534 - val_loss: 2.6366 - val_accuracy: 0.1994\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.11701 to 0.19935, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 3/100\n",
            "24/24 [==============================] - 17s 723ms/step - loss: 2.4580 - accuracy: 0.2341 - val_loss: 2.3933 - val_accuracy: 0.2543\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.19935 to 0.25430, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 4/100\n",
            "24/24 [==============================] - 17s 726ms/step - loss: 2.1372 - accuracy: 0.2968 - val_loss: 2.2554 - val_accuracy: 0.2852\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.25430 to 0.28523, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 5/100\n",
            "24/24 [==============================] - 18s 748ms/step - loss: 1.8914 - accuracy: 0.3461 - val_loss: 2.1712 - val_accuracy: 0.3079\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.28523 to 0.30793, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 6/100\n",
            "24/24 [==============================] - 18s 746ms/step - loss: 1.7024 - accuracy: 0.3855 - val_loss: 2.1144 - val_accuracy: 0.3251\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.30793 to 0.32514, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 7/100\n",
            "24/24 [==============================] - 18s 734ms/step - loss: 1.5368 - accuracy: 0.4261 - val_loss: 2.0788 - val_accuracy: 0.3324\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.32514 to 0.33238, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 8/100\n",
            "24/24 [==============================] - 18s 734ms/step - loss: 1.3892 - accuracy: 0.4606 - val_loss: 2.0519 - val_accuracy: 0.3408\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.33238 to 0.34077, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 9/100\n",
            "24/24 [==============================] - 18s 740ms/step - loss: 1.2460 - accuracy: 0.4915 - val_loss: 2.0395 - val_accuracy: 0.3480\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.34077 to 0.34801, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 10/100\n",
            "24/24 [==============================] - 18s 736ms/step - loss: 1.1115 - accuracy: 0.5223 - val_loss: 2.0463 - val_accuracy: 0.3488\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.34801 to 0.34882, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 11/100\n",
            "24/24 [==============================] - 18s 740ms/step - loss: 0.9997 - accuracy: 0.5528 - val_loss: 2.0495 - val_accuracy: 0.3521\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.34882 to 0.35210, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 12/100\n",
            "24/24 [==============================] - 18s 742ms/step - loss: 0.8980 - accuracy: 0.5852 - val_loss: 2.0619 - val_accuracy: 0.3523\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.35210 to 0.35227, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 13/100\n",
            "24/24 [==============================] - 18s 736ms/step - loss: 0.8001 - accuracy: 0.6213 - val_loss: 2.0811 - val_accuracy: 0.3539\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.35227 to 0.35385, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 14/100\n",
            "24/24 [==============================] - 18s 739ms/step - loss: 0.7097 - accuracy: 0.6592 - val_loss: 2.0940 - val_accuracy: 0.3605\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.35385 to 0.36050, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 15/100\n",
            "24/24 [==============================] - 18s 747ms/step - loss: 0.6304 - accuracy: 0.6933 - val_loss: 2.1152 - val_accuracy: 0.3607\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.36050 to 0.36067, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 16/100\n",
            "24/24 [==============================] - 18s 741ms/step - loss: 0.5634 - accuracy: 0.7226 - val_loss: 2.1383 - val_accuracy: 0.3587\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.36067\n",
            "Epoch 17/100\n",
            "24/24 [==============================] - 18s 733ms/step - loss: 0.5017 - accuracy: 0.7522 - val_loss: 2.1792 - val_accuracy: 0.3659\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.36067 to 0.36591, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 18/100\n",
            "24/24 [==============================] - 18s 738ms/step - loss: 0.4424 - accuracy: 0.7791 - val_loss: 2.1849 - val_accuracy: 0.3622\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.36591\n",
            "Epoch 19/100\n",
            "24/24 [==============================] - 18s 743ms/step - loss: 0.3871 - accuracy: 0.8057 - val_loss: 2.2348 - val_accuracy: 0.3557\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.36591\n",
            "Epoch 20/100\n",
            "24/24 [==============================] - 18s 744ms/step - loss: 0.3407 - accuracy: 0.8295 - val_loss: 2.2401 - val_accuracy: 0.3644\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.36591\n",
            "Epoch 21/100\n",
            "24/24 [==============================] - 18s 738ms/step - loss: 0.2916 - accuracy: 0.8540 - val_loss: 2.2844 - val_accuracy: 0.3667\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.36591 to 0.36667, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 22/100\n",
            "24/24 [==============================] - 18s 742ms/step - loss: 0.2465 - accuracy: 0.8767 - val_loss: 2.3228 - val_accuracy: 0.3672\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.36667 to 0.36718, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 23/100\n",
            "24/24 [==============================] - 18s 739ms/step - loss: 0.2053 - accuracy: 0.9000 - val_loss: 2.3315 - val_accuracy: 0.3656\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.36718\n",
            "Epoch 24/100\n",
            "24/24 [==============================] - 18s 743ms/step - loss: 0.1706 - accuracy: 0.9178 - val_loss: 2.3598 - val_accuracy: 0.3673\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.36718 to 0.36727, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 25/100\n",
            "24/24 [==============================] - 18s 739ms/step - loss: 0.1449 - accuracy: 0.9308 - val_loss: 2.3854 - val_accuracy: 0.3648\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.36727\n",
            "Epoch 26/100\n",
            "24/24 [==============================] - 18s 742ms/step - loss: 0.1248 - accuracy: 0.9421 - val_loss: 2.4228 - val_accuracy: 0.3681\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.36727 to 0.36808, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 27/100\n",
            "24/24 [==============================] - 18s 740ms/step - loss: 0.1084 - accuracy: 0.9500 - val_loss: 2.4603 - val_accuracy: 0.3683\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.36808 to 0.36833, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 28/100\n",
            "24/24 [==============================] - 18s 738ms/step - loss: 0.0939 - accuracy: 0.9573 - val_loss: 2.4925 - val_accuracy: 0.3672\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.36833\n",
            "Epoch 29/100\n",
            "24/24 [==============================] - 18s 741ms/step - loss: 0.0828 - accuracy: 0.9629 - val_loss: 2.4824 - val_accuracy: 0.3674\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.36833\n",
            "Epoch 30/100\n",
            "24/24 [==============================] - 18s 734ms/step - loss: 0.0741 - accuracy: 0.9667 - val_loss: 2.5065 - val_accuracy: 0.3697\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.36833 to 0.36974, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 31/100\n",
            "24/24 [==============================] - 18s 742ms/step - loss: 0.0649 - accuracy: 0.9709 - val_loss: 2.5431 - val_accuracy: 0.3690\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.36974\n",
            "Epoch 32/100\n",
            "24/24 [==============================] - 18s 743ms/step - loss: 0.0565 - accuracy: 0.9752 - val_loss: 2.5420 - val_accuracy: 0.3682\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.36974\n",
            "Epoch 33/100\n",
            "24/24 [==============================] - 18s 735ms/step - loss: 0.0500 - accuracy: 0.9789 - val_loss: 2.5615 - val_accuracy: 0.3684\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.36974\n",
            "Epoch 34/100\n",
            "24/24 [==============================] - 18s 736ms/step - loss: 0.0439 - accuracy: 0.9814 - val_loss: 2.5802 - val_accuracy: 0.3702\n",
            "\n",
            "Epoch 00034: val_accuracy improved from 0.36974 to 0.37017, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 35/100\n",
            "24/24 [==============================] - 18s 741ms/step - loss: 0.0391 - accuracy: 0.9838 - val_loss: 2.6028 - val_accuracy: 0.3682\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.37017\n",
            "Epoch 36/100\n",
            "24/24 [==============================] - 18s 743ms/step - loss: 0.0358 - accuracy: 0.9853 - val_loss: 2.6150 - val_accuracy: 0.3687\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.37017\n",
            "Epoch 37/100\n",
            "24/24 [==============================] - 18s 738ms/step - loss: 0.0327 - accuracy: 0.9867 - val_loss: 2.6308 - val_accuracy: 0.3711\n",
            "\n",
            "Epoch 00037: val_accuracy improved from 0.37017 to 0.37110, saving model to /content/drive/MyDrive/Transformers/adam_100words2ds_pre_en/weights_adam.ckpt\n",
            "Epoch 38/100\n",
            "24/24 [==============================] - 18s 736ms/step - loss: 0.0299 - accuracy: 0.9876 - val_loss: 2.6456 - val_accuracy: 0.3674\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.37110\n",
            "Epoch 39/100\n",
            "24/24 [==============================] - 18s 741ms/step - loss: 0.0276 - accuracy: 0.9886 - val_loss: 2.6544 - val_accuracy: 0.3673\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.37110\n",
            "Epoch 40/100\n",
            "24/24 [==============================] - 18s 743ms/step - loss: 0.0258 - accuracy: 0.9891 - val_loss: 2.6688 - val_accuracy: 0.3662\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.37110\n",
            "Epoch 41/100\n",
            "24/24 [==============================] - 18s 744ms/step - loss: 0.0246 - accuracy: 0.9896 - val_loss: 2.6707 - val_accuracy: 0.3669\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.37110\n",
            "Epoch 42/100\n",
            "24/24 [==============================] - 18s 740ms/step - loss: 0.0234 - accuracy: 0.9899 - val_loss: 2.6842 - val_accuracy: 0.3655\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.37110\n",
            "Epoch 00042: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3503e7d950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5Qvebl8FHsG",
        "outputId": "b5c07712-bf72-4f3d-c3df-b86b08be36a1"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(googledrive_path)\n",
        "transformer.load_weights(latest)"
      ],
      "id": "j5Qvebl8FHsG",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f35141efb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8447490b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0536a014-6f9e-4054-81b2-89d84205eb9b"
      },
      "source": [
        "ar_vocab = ar_vectorization.get_vocabulary()\n",
        "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
        "max_decoded_sentence_length = sequence_length\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in val_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts[:30])\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, '\\n', translated)\n",
        "    print('*'*50)"
      ],
      "id": "8447490b",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Difficulties may be encountered in ensuring that new types of fuel meet current technical standards, which, to a large extent, have been developed for conventional fossil fuels \n",
            " [start] 3 قد تكون الكميات المخزنة في ذلك بالشكل المناسب من أجل ضمان شروط ظهور المصطلح على الوقود [end]\n",
            "**************************************************\n",
            " Difficulties may be encountered in ensuring that new types of fuel meet current technical standards, which, to a large extent, have been developed for conventional fossil fuels \n",
            " [start] 3 قد تكون الكميات المخزنة في ذلك بالشكل المناسب من أجل ضمان شروط ظهور المصطلح على الوقود [end]\n",
            "**************************************************\n",
            "(vi) principles for monitoring and evaluating EURES activities; \n",
            " [start] 6 لتنفيذ المهام المسندة إلى الوكالة بشكل صحيح، [end]\n",
            "**************************************************\n",
            "- \"type approval\" in United Kingdom law. \n",
            " [start] الموافقة على النوع في منطقة الآلية عندما [end]\n",
            "**************************************************\n",
            "THIRD COUNTRIES \n",
            " [start] الثالثة القائمة [end]\n",
            "**************************************************\n",
            "(vi) principles for monitoring and evaluating EURES activities; \n",
            " [start] 6 لتنفيذ المهام المسندة إلى الوكالة بشكل صحيح، [end]\n",
            "**************************************************\n",
            "- or prohibit the entry into service of vehicles, \n",
            " [start] أو يحظر تشغيل الآليات، [end]\n",
            "**************************************************\n",
            "Article 1 \n",
            " [start] المادة 1 [end]\n",
            "**************************************************\n",
            "The marketing years for products qualifying for Community withdrawal compensation in accordance with Article 23 (3) of Regulation (EC) No 2200/96 shall be as listed in Annex I to this Regulation. \n",
            " [start] يتم التسويق للمنتجات الواردة في هذه اللائحة رقم 220096 [end]\n",
            "**************************************************\n",
            "Article 11 \n",
            " [start] المادة 11 [end]\n",
            "**************************************************\n",
            "Article 1 \n",
            " [start] المادة 1 [end]\n",
            "**************************************************\n",
            "(10) In Common Position (EC) No 33/2003 of 20 March 2003 adopted by the Council with a view to adopting a directive of the European Parliament and the Council on the coordination of procedures for the award of public works contracts, public supply contracts and public service contracts(4) and Common Position (EC) No 34/2003 of 20 March 2003 adopted by the Council with a view to adopting a directive of the European Parliament and the Council coordinating the procurement procedures of entities operating in the water, energy, transport and postal services sectors(5), product areas are not established by means of reference to the statistical classification of products by activity (CPA) \n",
            " [start] 10 في ضوء التطورات على تحديد الفترات الفاصلة الأسبوعية بين تحديد مستويات قصوى لمتبقيات المبيدات الحشرية الموجودة في 9 1 كانون الأول 1995 وبالتالي، يتعين على مستوى الجماعة الأوروبية رقم 222396 بتاريخ 20 2 87181EEC، ولا سيما البطاريات وأجهزة التبريد المنزلية التي تنص عليها إبلاغ المفوضية الأوروبية 11 حول التنظيم\n",
            "**************************************************\n",
            "Article 1 \n",
            " [start] المادة 1 [end]\n",
            "**************************************************\n",
            "of 29 July 2002 \n",
            " [start] بتاريخ 29 يوليو 2002 [end]\n",
            "**************************************************\n",
            "(d) evidence that the agricultural product or the foodstuff originates in the geographical area, within the meaning of Article 2 (2) (a) or (b), whichever is applicable; \n",
            " [start] د أن تكون الوثائق الزراعية يجب أن تستوفي المنتجات الزراعية أو المواد الغذائية التي تقع ضمن إطار العناوين في المادة 2 أو أ أو المستورد أو عائلة المنتجات الموصوفة [end]\n",
            "**************************************************\n",
            "(c) have not been subject, during the preceding marketing year, to proceedings for irregularities noted during checks made pursuant to Article 14 and pursuant to this Article as regards approval for the 1984/85 marketing year; \n",
            " [start] ج لم يتم بعد إجراء عملية التحكم في بلد ثالث لإنتاج غير المشاركين في للجماعات الأوروبية [end]\n",
            "**************************************************\n",
            " Difficulties may be encountered in ensuring that new types of fuel meet current technical standards, which, to a large extent, have been developed for conventional fossil fuels \n",
            " [start] 3 قد تكون الكميات المخزنة في ذلك بالشكل المناسب من أجل ضمان شروط ظهور المصطلح على الوقود [end]\n",
            "**************************************************\n",
            "(vi) principles for monitoring and evaluating EURES activities; \n",
            " [start] 6 لتنفيذ المهام المسندة إلى الوكالة بشكل صحيح، [end]\n",
            "**************************************************\n",
            "4. Investment companies the assets of which are invested through the intermediary of subsidiary companies mainly otherwise than in transferable securities shall not, however, be subject to this Directive. \n",
            " [start] 4 يتكون إن شركات التأمين من أصولها التي يقع فيها شركة استثمارية برأسمال ثابت والسُلف المتعلقة ببعض الضمانات ، وعلى وجه الخصوص، تضمن الأوراق المالية القابلة للتحويل إلى أنها معدة لأغراض الملاحة في هذا التوجيه [end]\n",
            "**************************************************\n",
            "Article 1 \n",
            " [start] المادة 1 [end]\n",
            "**************************************************\n",
            "(vi) principles for monitoring and evaluating EURES activities; \n",
            " [start] 6 لتنفيذ المهام المسندة إلى الوكالة بشكل صحيح، [end]\n",
            "**************************************************\n",
            "- or prohibit the entry into service of vehicles, \n",
            " [start] أو يحظر تشغيل الآليات، [end]\n",
            "**************************************************\n",
            "1. A UCITS must re-purchase or redeem its units at the request of any unit-holder. \n",
            " [start] 1 يجب أن يُعهد بأصول الشركات أو أحد صناديق حصص الاستثمار أو الأوراق المالية القابلة للتحويل أو أحد التجمعات في الأوراق المالية القابلة للتحويل UCITS [end]\n",
            "**************************************************\n",
            "- 'mixed-activity holding company' shall mean a parent undertaking, other than a financial holding company or an investment firm or a mixed financial holding company within the meaning of Directive 2002/87/EC, the subsidiaries of which include at least one investment firm.\" \n",
            " [start] شركات التأمين القابضة بالمعني المقصود في المادة 1 1 1 المجموعة أو شركة أم لا، كافية عندما يكون عملها أو الكيان المنظَّم بموجب الشركة القابضة المالية والأراضي أو شركة تابعة للشركة الاستثمارية والسماح للمساهمين نتيجة الإخفاق في المادة 15 [end]\n",
            "**************************************************\n",
            " (a) By way of derogation from paragraph 1 (c), seeds and vegetative propagating material not obtained by the organic production method may, during a transitional period expiring on 31 December 2000 and with the approval of the competent authority of the Member State, be used in so far as users of such propagating material can show to the satisfaction of the inspection body or authority of the Member State that they were unable to obtain on the market propagating material for an appropriate variety of the species in question and satisfying the requirements of paragraph 2 \n",
            " [start] أ التقديرات المشار إليها في المادة 14 د من الفقرة 1 ب و ج من المواد الكيميائية التجارية التي لم يعد يلبي متطلبات الجودة لتسويق المنتوجات؛ وحيث أنّه لا يجوز للسلطة المتلقية للبيانات لا يمكن الموافقة على النحو المشار إليها في الفقرة الفرعية الثانية من 1 للتوجيه 90220EEC [end]\n",
            "**************************************************\n",
            "(a) a payment of up to 5 percentage points above the ESCB's marginal lending rate or twice the ESCB's marginal lending rate, in both cases applied to the amount of the minimum reserves which the relevant institution fails to provide; \n",
            " [start] أ يتم جمع الرسوم عندما تذبح الحيوانات المشار إليها في الفقرة الفرعية أ و 8، المرفق 1 فقط وأي قواعد زيادة رأس المال المكتتب فيه سعر الفائدة من قِبل الحد من قِبل صندوق الإرشاد والضمان الزراعي الأوروبي [end]\n",
            "**************************************************\n",
            "(b) the market share does not exceed 5 % in any Member State, measured in terms of the balance sheet total in the banking or investment services sectors and in terms of gross premiums written in the insurance sector. \n",
            " [start] ب يكون السوق النقدي على المعلومات المقدمة في حالة عدم وجود أي زيادة بلغة رسمية واحدة أو في الدولة العضو التي يفوق العتبة المُعرب عنها على الأقل في غضون فترة مرجعية [end]\n",
            "**************************************************\n",
            "4. Investment companies the assets of which are invested through the intermediary of subsidiary companies mainly otherwise than in transferable securities shall not, however, be subject to this Directive. \n",
            " [start] 4 يتكون إن شركات التأمين من أصولها التي يقع فيها شركة استثمارية برأسمال ثابت والسُلف المتعلقة ببعض الضمانات ، وعلى وجه الخصوص، تضمن الأوراق المالية القابلة للتحويل إلى أنها معدة لأغراض الملاحة في هذا التوجيه [end]\n",
            "**************************************************\n",
            " Difficulties may be encountered in ensuring that new types of fuel meet current technical standards, which, to a large extent, have been developed for conventional fossil fuels \n",
            " [start] 3 قد تكون الكميات المخزنة في ذلك بالشكل المناسب من أجل ضمان شروط ظهور المصطلح على الوقود [end]\n",
            "**************************************************\n",
            "\"bestrahlt\" or \"mit ionisierenden Strahlen behandelt\", \n",
            " [start] irradiato أو trattato con radiazioni ionizzanti، [end]\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2i4SbVoP_Yz"
      },
      "source": [
        "def get_bleu():\n",
        "  \n",
        "  preds, src = [], []\n",
        "\n",
        "  with tqdm(total=100, position=0, leave=True) as pbar:\n",
        "    for en_sent, ar_sent in tqdm(random.choices(val_pairs, k=100), position=0, leave=True):\n",
        "      translated = decode_sequence(en_sent)\n",
        "      preds.append(translated)\n",
        "      src.append(ar_sent)\n",
        "      pbar.update()\n",
        "\n",
        "    return src, preds\n",
        "    # print_scores(src, preds)\n",
        "\n"
      ],
      "id": "p2i4SbVoP_Yz",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9uh1623AwUW"
      },
      "source": [
        "def print_scores(trgs, preds):\n",
        "    print('----- Bleu-n Scores -----')\n",
        "    print(\"1:\", corpus_bleu(trgs, preds, weights=[1.0/1.0])*100)\n",
        "    print(\"2:\", corpus_bleu(trgs, preds, weights=[1.0/2.0, 1.0/2.0])*100)\n",
        "    print(\"3:\", corpus_bleu(trgs, preds, weights=[1.0/3.0, 1.0/3.0, 1.0/3.0])*100)\n",
        "    print(\"4:\", corpus_bleu(trgs, preds)*100)\n",
        "    print('-'*25)"
      ],
      "id": "A9uh1623AwUW",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaXlIeHdRGqi",
        "outputId": "a3b156de-f2f8-4228-d937-451bc3f58df9"
      },
      "source": [
        "src, preds = get_bleu()"
      ],
      "id": "YaXlIeHdRGqi",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n",
            "100%|██████████| 100/100 [00:58<00:00,  1.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSinG65pSGkC"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "id": "tSinG65pSGkC",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbUG-qgaUiHD",
        "outputId": "48141023-8adb-49bb-e828-7b661b277611"
      },
      "source": [
        "print_scores(preds, src)"
      ],
      "id": "zbUG-qgaUiHD",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 20.97472102026873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2: 45.79816701601575\n",
            "3: 59.41535974766278\n",
            "4: 67.67434300827439\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkBkD4yLT5CF",
        "outputId": "407b0a82-b4d3-4ffb-9e16-127a0866196b"
      },
      "source": [
        "print_scores(preds, src)"
      ],
      "id": "kkBkD4yLT5CF",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 17.369519832985386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2: 41.676755911401486\n",
            "3: 55.79508424061051\n",
            "4: 64.55753705912385\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mBYxHDaET9ya",
        "outputId": "14a19259-3e36-41d1-a9ec-4c06dbfdb307"
      },
      "source": [
        "preds[1]"
      ],
      "id": "mBYxHDaET9ya",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[start] ب النكهة الجيدة، عندما يتم جمع الرسوم عندما تذبح الحيوانات المشار إليها في النقطة أ و [end]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Qm7rkbxvUCfX",
        "outputId": "d9984b9e-9124-4ab4-c10b-1ae7f6ca0534"
      },
      "source": [
        "src[1]"
      ],
      "id": "Qm7rkbxvUCfX",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[start] (ب) تم الامتثال لفترة تحويل من 12 شهرا على الأقل قبل موسم الحصاد. [end]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9QmaJH0UEV7"
      },
      "source": [
        ""
      ],
      "id": "S9QmaJH0UEV7",
      "execution_count": null,
      "outputs": []
    }
  ]
}