{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Final",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R53kjrYGpSc",
        "execution": {
          "iopub.status.busy": "2021-06-08T01:34:16.627960Z",
          "iopub.execute_input": "2021-06-08T01:34:16.628366Z",
          "iopub.status.idle": "2021-06-08T01:34:26.014739Z",
          "shell.execute_reply.started": "2021-06-08T01:34:16.628281Z",
          "shell.execute_reply": "2021-06-08T01:34:26.013775Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2375c122-0f47-4fc3-f46f-2eec6567fb0f"
      },
      "source": [
        "!pip install pyarabic\n",
        "!git clone https://github.com/moaaztaha/Arabic-English-Translation-Transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyarabic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
            "\r\u001b[K     |███                             | 10kB 19.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 26.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30kB 24.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 40kB 18.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 92kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 102kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 8.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
            "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp37-none-any.whl size=113324 sha256=a60674902d7fdf54ac8084e12405eabd5b1c2a7a7931e22614c79372022a5f96\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
            "Successfully built pyarabic\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.10\n",
            "Cloning into 'Arabic-English-Translation-Transformers'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 46 (delta 16), reused 40 (delta 10), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9e97fa1",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:47.414915Z",
          "iopub.execute_input": "2021-06-08T02:16:47.415253Z",
          "iopub.status.idle": "2021-06-08T02:16:47.421088Z",
          "shell.execute_reply.started": "2021-06-08T02:16:47.415225Z",
          "shell.execute_reply": "2021-06-08T02:16:47.419702Z"
        },
        "trusted": true
      },
      "source": [
        "# modules\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "import pyarabic.araby as araby\n",
        "from pyarabic.araby import strip_tashkeel, strip_tatweel"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1710411f"
      },
      "source": [
        "### Data Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RNDMX9cizGW",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:47.955355Z",
          "iopub.execute_input": "2021-06-08T02:16:47.955686Z",
          "iopub.status.idle": "2021-06-08T02:16:48.136995Z",
          "shell.execute_reply.started": "2021-06-08T02:16:47.955644Z",
          "shell.execute_reply": "2021-06-08T02:16:48.136060Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8167ea6-e4ad-4614-e635-3c3938556fd0"
      },
      "source": [
        "ar = pd.read_table('/content/Arabic-English-Translation-Transformers/ArabicNewData.txt', delimiter='\\\\n', names=['ar'])\n",
        "en = pd.read_table('/content/Arabic-English-Translation-Transformers/EnglishNewData.txt', delimiter='\\\\n', names=['en'])\n",
        "\n",
        "en['ar'] = ar['ar']\n",
        "df = en.copy()\n",
        "df = df.iloc[:35118]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return read_csv(**locals())\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2YTqqOTGl5c",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:48.455116Z",
          "iopub.execute_input": "2021-06-08T02:16:48.455428Z",
          "iopub.status.idle": "2021-06-08T02:16:48.461638Z",
          "shell.execute_reply.started": "2021-06-08T02:16:48.455402Z",
          "shell.execute_reply": "2021-06-08T02:16:48.460738Z"
        },
        "trusted": true
      },
      "source": [
        "morphs = [strip_tashkeel, strip_tatweel]\n",
        "\n",
        "def fix_ar(sent):\n",
        "  sent = split_al_sent(sent)\n",
        "  tokens = araby.tokenize(sent, morphs=morphs)\n",
        "  sent = araby.normalize_hamza(' '.join(tokens), method='tasheel')\n",
        "  return sent"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WhmXTv1KRkd",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:48.992161Z",
          "iopub.execute_input": "2021-06-08T02:16:48.992476Z",
          "iopub.status.idle": "2021-06-08T02:16:48.998243Z",
          "shell.execute_reply.started": "2021-06-08T02:16:48.992448Z",
          "shell.execute_reply": "2021-06-08T02:16:48.997288Z"
        },
        "trusted": true
      },
      "source": [
        "def split_al(word):\n",
        "    if word.startswith('ال'):\n",
        "        return word[:2], word[2:]\n",
        "    else: \n",
        "        return word\n",
        "\n",
        "def split_al_sent(sent):\n",
        "    ww = []\n",
        "    for word in sent.split():\n",
        "        out = split_al(word)\n",
        "        if type(out) is tuple:\n",
        "            for w in out:\n",
        "                ww.append(w)\n",
        "        else:\n",
        "            ww.append(word)\n",
        "    return ' '.join(w for w in ww)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGuKlu3mFIZc",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:50.345045Z",
          "iopub.execute_input": "2021-06-08T02:16:50.345366Z",
          "iopub.status.idle": "2021-06-08T02:16:51.796892Z",
          "shell.execute_reply.started": "2021-06-08T02:16:50.345330Z",
          "shell.execute_reply": "2021-06-08T02:16:51.795902Z"
        },
        "trusted": true
      },
      "source": [
        "df['ar'] = df.apply(lambda row: fix_ar(row.ar), axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be905b2c",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:51.799257Z",
          "iopub.execute_input": "2021-06-08T02:16:51.799840Z",
          "iopub.status.idle": "2021-06-08T02:16:54.618551Z",
          "shell.execute_reply.started": "2021-06-08T02:16:51.799800Z",
          "shell.execute_reply": "2021-06-08T02:16:54.617702Z"
        },
        "trusted": true
      },
      "source": [
        "text_pairs = []\n",
        "for idx, row in df.iterrows():\n",
        "  en, ar = row['en'], row['ar']\n",
        "  ar = \"[start] \" + ar + \" [end]\"\n",
        "  text_pairs.append((en, ar))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAxNAdK2H0eB",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:58.012634Z",
          "iopub.execute_input": "2021-06-08T02:16:58.013120Z",
          "iopub.status.idle": "2021-06-08T02:17:00.598315Z",
          "shell.execute_reply.started": "2021-06-08T02:16:58.013084Z",
          "shell.execute_reply": "2021-06-08T02:17:00.597570Z"
        },
        "trusted": true
      },
      "source": [
        "for idx, row in df.iterrows():\n",
        "  if len(row.ar.split()) < 1:\n",
        "    print(row.ar, '\\n*')\n",
        "    print(row.en)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "296e4594",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:00.600320Z",
          "iopub.execute_input": "2021-06-08T02:17:00.600692Z",
          "iopub.status.idle": "2021-06-08T02:17:00.605892Z",
          "shell.execute_reply.started": "2021-06-08T02:17:00.600644Z",
          "shell.execute_reply": "2021-06-08T02:17:00.605074Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43dfaace-2dba-4a9c-8b77-b07b72131c3c"
      },
      "source": [
        "for _ in range(2):\n",
        "    print(random.choice(text_pairs))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('as promised...?', '[start] {\\\\ 3cH5A1867 } كما وعدتك يازيرو ...؟ [end]')\n",
            "('Cheetah, come back!', '[start] شيتا ، يرجع ! [end]')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6kRoro9iBOe",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:00.607746Z",
          "iopub.execute_input": "2021-06-08T02:17:00.608167Z",
          "iopub.status.idle": "2021-06-08T02:17:00.620306Z",
          "shell.execute_reply.started": "2021-06-08T02:17:00.608132Z",
          "shell.execute_reply": "2021-06-08T02:17:00.619437Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075bb101-3e48-43bc-ad00-b6842871dd8b"
      },
      "source": [
        "len(text_pairs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35118"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1283f6f",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:01.800249Z",
          "iopub.execute_input": "2021-06-08T02:17:01.800560Z",
          "iopub.status.idle": "2021-06-08T02:17:01.847161Z",
          "shell.execute_reply.started": "2021-06-08T02:17:01.800532Z",
          "shell.execute_reply": "2021-06-08T02:17:01.846417Z"
        },
        "trusted": true
      },
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) -  num_val_samples\n",
        "train_pairs = text_pairs[: num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7a1f8cd",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:02.645892Z",
          "iopub.execute_input": "2021-06-08T02:17:02.646210Z",
          "iopub.status.idle": "2021-06-08T02:17:02.654338Z",
          "shell.execute_reply.started": "2021-06-08T02:17:02.646181Z",
          "shell.execute_reply": "2021-06-08T02:17:02.653626Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef443a4-1c7a-4fa1-a568-f3dc6436cadc"
      },
      "source": [
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35118 total pairs\n",
            "29851 training pairs\n",
            "5267 validation pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5f6d84e"
      },
      "source": [
        "#### Vectorizing the text data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487ed66f",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:03.954239Z",
          "iopub.execute_input": "2021-06-08T02:17:03.954544Z",
          "iopub.status.idle": "2021-06-08T02:17:04.856227Z",
          "shell.execute_reply.started": "2021-06-08T02:17:03.954516Z",
          "shell.execute_reply": "2021-06-08T02:17:04.854499Z"
        },
        "trusted": true
      },
      "source": [
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "vocab_size = 20000\n",
        "sequence_length = 50\n",
        "batch_size = 265\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    return tf.strings.regex_replace(input_string, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "            # max_tokens=vocab_size, \n",
        "            output_mode='int', \n",
        "            output_sequence_length=sequence_length)\n",
        "\n",
        "ar_vectorization = TextVectorization(\n",
        "    # max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size)\n",
        "\n",
        "eng_texts = [pair[0] for pair in text_pairs]\n",
        "ar_texts = [pair[1] for pair in text_pairs]\n",
        "eng_vectorization.adapt(eng_texts)\n",
        "ar_vectorization.adapt(ar_texts)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubW5QOJtLzMI",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:04.859040Z",
          "iopub.execute_input": "2021-06-08T02:17:04.859565Z",
          "iopub.status.idle": "2021-06-08T02:17:04.938344Z",
          "shell.execute_reply.started": "2021-06-08T02:17:04.859525Z",
          "shell.execute_reply": "2021-06-08T02:17:04.937688Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d0cfc9-b35a-4522-c9ce-55d2bc6f112b"
      },
      "source": [
        "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 13164)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wep-jkjyLWz6",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:04.939783Z",
          "iopub.execute_input": "2021-06-08T02:17:04.940155Z",
          "iopub.status.idle": "2021-06-08T02:17:05.011110Z",
          "shell.execute_reply.started": "2021-06-08T02:17:04.940116Z",
          "shell.execute_reply": "2021-06-08T02:17:05.010101Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4228ca-f91d-43dd-aff2-b4e02b079da6"
      },
      "source": [
        "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 13164)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5qiqjOWdVz1",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:05.013252Z",
          "iopub.execute_input": "2021-06-08T02:17:05.013639Z",
          "iopub.status.idle": "2021-06-08T02:17:05.021566Z",
          "shell.execute_reply.started": "2021-06-08T02:17:05.013599Z",
          "shell.execute_reply": "2021-06-08T02:17:05.020566Z"
        },
        "trusted": true
      },
      "source": [
        "def format_dataset(eng, ar):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ar = ar_vectorization(ar)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ar_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ar_texts = list(ar_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a3ed140",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:05.596886Z",
          "iopub.execute_input": "2021-06-08T02:17:05.597209Z",
          "iopub.status.idle": "2021-06-08T02:17:06.245999Z",
          "shell.execute_reply.started": "2021-06-08T02:17:05.597181Z",
          "shell.execute_reply": "2021-06-08T02:17:06.245218Z"
        },
        "trusted": true
      },
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3902789f",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:06.247956Z",
          "iopub.execute_input": "2021-06-08T02:17:06.248284Z",
          "iopub.status.idle": "2021-06-08T02:17:06.519611Z",
          "shell.execute_reply.started": "2021-06-08T02:17:06.248250Z",
          "shell.execute_reply": "2021-06-08T02:17:06.518592Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db94c6e4-848f-4e66-fb70-88469b8ca52c"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (265, 50)\n",
            "inputs[\"decoder_inputs\"].shape: (265, 50)\n",
            "targets.shape: (265, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fa0936c"
      },
      "source": [
        "### Building the Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3baf452",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:07.222610Z",
          "iopub.execute_input": "2021-06-08T02:17:07.222939Z",
          "iopub.status.idle": "2021-06-08T02:17:07.247405Z",
          "shell.execute_reply.started": "2021-06-08T02:17:07.222907Z",
          "shell.execute_reply": "2021-06-08T02:17:07.246264Z"
        },
        "trusted": true
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "          'embed_dim': self.embed_dim,\n",
        "          'dense_dim': self.dense_dim,\n",
        "          'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, pretrained=False, weights=False, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        if not pretrained:\n",
        "          self.token_embeddings = layers.Embedding(\n",
        "              input_dim=vocab_size, output_dim=embed_dim\n",
        "          )\n",
        "        else:\n",
        "          # pre-trained\n",
        "          self.token_embeddings = layers.Embedding(\n",
        "              input_dim=vocab_size, output_dim=embed_dim, weights=[weights]\n",
        "          ) \n",
        "\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "      \n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'sequence_length': self.sequence_length,\n",
        "      'vocab_size': self.vocab_size,\n",
        "      'embed_dim': self.embed_dim,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'embed_dim': self.embed_dim,\n",
        "      'latent_dim': self.latent_dim,\n",
        "      'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpXVu3eA_b9P",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:20.072791Z",
          "iopub.execute_input": "2021-06-08T02:17:20.073181Z",
          "iopub.status.idle": "2021-06-08T02:20:23.160034Z",
          "shell.execute_reply.started": "2021-06-08T02:17:20.073151Z",
          "shell.execute_reply": "2021-06-08T02:20:23.158925Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce04a0b-f292-4fd9-d995-d2e72fce868b"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-08 21:43:13--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-06-08 21:43:13--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-06-08 21:43:13--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 40s  \n",
            "\n",
            "2021-06-08 21:45:53 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-08T03:46:29.636942Z",
          "iopub.execute_input": "2021-06-08T03:46:29.637271Z",
          "iopub.status.idle": "2021-06-08T03:46:29.674663Z",
          "shell.execute_reply.started": "2021-06-08T03:46:29.637241Z",
          "shell.execute_reply": "2021-06-08T03:46:29.673702Z"
        },
        "trusted": true,
        "id": "LMeo9ow6hAL4"
      },
      "source": [
        "def get_weights(vectorizer, embeddings_path, is_gensim=False):\n",
        "    path_to_glove_file = embeddings_path\n",
        "    \n",
        "    if is_gensim:\n",
        "        embeddings_index = gensim.models.Word2Vec.load(embeddings_path)\n",
        "        \n",
        "    else:\n",
        "        embeddings_index = {}\n",
        "        with open(path_to_glove_file) as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "                embeddings_index[word] = coefs\n",
        "    \n",
        "    if not is_gensim:\n",
        "        print(\"Found %s word vectors.\" % len(dict(embeddings_index)))\n",
        "    else:\n",
        "        print(\"Found %s word vectors.\" % (embeddings_index.wv.vectors.shape[0]))\n",
        "\n",
        "    \n",
        "    \n",
        "    vocab = vectorizer.get_vocabulary()\n",
        "    word_index = dict(zip(vocab, range(len(vocab))))\n",
        "    num_tokens = len(vocab)\n",
        "    embedding_dim = 300\n",
        "    hits = 0\n",
        "    misses = 0\n",
        "\n",
        "    # Prepare embedding matrix\n",
        "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        if not is_gensim:\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "        else:\n",
        "            if word in embeddings_index.wv:\n",
        "                embedding_vector = embeddings_index.wv[word]\n",
        "            else:\n",
        "                embedding_vector = None\n",
        "        if embedding_vector is not None:\n",
        "            # Words not found in embedding index will be all-zeros.\n",
        "            # This includes the representation for \"padding\" and \"OOV\"\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            hits += 1\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.uniform(-.1, .1, size=(embedding_dim))\n",
        "            misses += 1\n",
        "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "    return embedding_matrix"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-08T03:47:05.607322Z",
          "iopub.execute_input": "2021-06-08T03:47:05.607709Z",
          "iopub.status.idle": "2021-06-08T03:47:35.559143Z",
          "shell.execute_reply.started": "2021-06-08T03:47:05.607653Z",
          "shell.execute_reply": "2021-06-08T03:47:35.557315Z"
        },
        "trusted": true,
        "id": "vm0fr1fDhAL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b542ee-8f42-40ab-cd21-4ffffd0f2f18"
      },
      "source": [
        "english_embeddings = get_weights(eng_vectorization, './glove.6B.300d.txt')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Converted 11896 words (1268 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-08T03:46:30.659781Z",
          "iopub.execute_input": "2021-06-08T03:46:30.660121Z",
          "iopub.status.idle": "2021-06-08T03:46:30.663804Z",
          "shell.execute_reply.started": "2021-06-08T03:46:30.660094Z",
          "shell.execute_reply": "2021-06-08T03:46:30.662582Z"
        },
        "trusted": true,
        "id": "7to0sCeihAL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0511c92-b281-4de2-adee-9941a7502703"
      },
      "source": [
        "! wget https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_cbow_300_wiki.zip\n",
        "! unzip -q full_grams_cbow_300_wiki.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-08 21:46:43--  https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_cbow_300_wiki.zip\n",
            "Resolving bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)... 108.61.0.122, 2001:19f0:0:22::100\n",
            "Connecting to bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)|108.61.0.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1491895880 (1.4G) [application/zip]\n",
            "Saving to: ‘full_grams_cbow_300_wiki.zip’\n",
            "\n",
            "full_grams_cbow_300 100%[===================>]   1.39G  39.1MB/s    in 38s     \n",
            "\n",
            "2021-06-08 21:47:21 (37.5 MB/s) - ‘full_grams_cbow_300_wiki.zip’ saved [1491895880/1491895880]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-08T03:46:30.842479Z",
          "iopub.execute_input": "2021-06-08T03:46:30.842754Z",
          "iopub.status.idle": "2021-06-08T03:46:53.711963Z",
          "shell.execute_reply.started": "2021-06-08T03:46:30.842728Z",
          "shell.execute_reply": "2021-06-08T03:46:53.710792Z"
        },
        "trusted": true,
        "id": "tHG1gRmEhAL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0405371-3ba5-4063-ac6e-460376bd4a5a"
      },
      "source": [
        "arabic_embeddings = get_weights(ar_vectorization, './full_grams_cbow_300_wiki.mdl', is_gensim=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 662109 word vectors.\n",
            "Converted 12646 words (7354 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0rA0C4s1OVW",
        "execution": {
          "iopub.status.busy": "2021-06-08T03:47:59.830643Z",
          "iopub.execute_input": "2021-06-08T03:47:59.830984Z",
          "iopub.status.idle": "2021-06-08T03:47:59.906741Z",
          "shell.execute_reply.started": "2021-06-08T03:47:59.830952Z",
          "shell.execute_reply": "2021-06-08T03:47:59.905831Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851bae6c-4fc6-4a1e-a838-c917d0e760c9"
      },
      "source": [
        "ar_vocab_size = len(ar_vectorization.get_vocabulary())\n",
        "en_vocab_size = len(eng_vectorization.get_vocabulary())\n",
        "en_vocab_size, ar_vocab_size"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13164, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c57e1e",
        "execution": {
          "iopub.status.busy": "2021-06-08T03:49:47.423352Z",
          "iopub.execute_input": "2021-06-08T03:49:47.423713Z",
          "iopub.status.idle": "2021-06-08T03:49:48.045217Z",
          "shell.execute_reply.started": "2021-06-08T03:49:47.423679Z",
          "shell.execute_reply": "2021-06-08T03:49:48.044246Z"
        },
        "trusted": true
      },
      "source": [
        "embed_dim = 300\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, en_vocab_size, embed_dim, pretrained=True, weights=english_embeddings)(encoder_inputs)\n",
        "# x = PositionalEmbedding(sequence_length, num_tokens, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim, pretrained=True, weights=arabic_embeddings)(decoder_inputs)\n",
        "# x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(ar_vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt9sp6Yrb4pE",
        "execution": {
          "iopub.status.busy": "2021-06-08T03:49:49.471793Z",
          "iopub.execute_input": "2021-06-08T03:49:49.472124Z",
          "iopub.status.idle": "2021-06-08T03:49:49.477189Z",
          "shell.execute_reply.started": "2021-06-08T03:49:49.472094Z",
          "shell.execute_reply": "2021-06-08T03:49:49.476150Z"
        },
        "trusted": true
      },
      "source": [
        "googledrive_path = '/content/drive/MyDrive/Transformers/final/'"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqmaEQ94kyB9",
        "execution": {
          "iopub.status.busy": "2021-06-08T03:49:54.155540Z",
          "iopub.execute_input": "2021-06-08T03:49:54.155939Z",
          "iopub.status.idle": "2021-06-08T03:49:54.164631Z",
          "shell.execute_reply.started": "2021-06-08T03:49:54.155904Z",
          "shell.execute_reply": "2021-06-08T03:49:54.163841Z"
        },
        "trusted": true
      },
      "source": [
        "from keras import callbacks\n",
        "early_stopping_cb = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=True)\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(googledrive_path+'/weights_adam.ckpt', monitor='val_accuracy', save_weights_only=True,verbose=True, save_best_only=True)\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=googledrive_path+\"/logs\")\n",
        "lr_schr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=True, factor=0.3, min_lr=0.0001)\n",
        "cbs = [early_stopping_cb, checkpoint_cb, tensorboard_callback, lr_schr]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-08T03:50:03.472687Z",
          "iopub.execute_input": "2021-06-08T03:50:03.473046Z",
          "iopub.status.idle": "2021-06-08T04:02:41.112268Z",
          "shell.execute_reply.started": "2021-06-08T03:50:03.473016Z",
          "shell.execute_reply": "2021-06-08T04:02:41.111344Z"
        },
        "trusted": true,
        "id": "oe_QsoxGhAL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6cd7cc-d139-4384-ab01-b651539b8c91"
      },
      "source": [
        "epochs = 100  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding (Positiona (None, None, 300)    3964200     encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder (Transforme (None, None, 300)    4119848     positional_embedding[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "model_1 (Functional)            (None, None, 20000)  19042948    decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 27,126,996\n",
            "Trainable params: 27,126,996\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "113/113 [==============================] - 89s 719ms/step - loss: 0.9249 - accuracy: 0.2856 - val_loss: 0.7911 - val_accuracy: 0.3460\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.34595, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 2/100\n",
            "113/113 [==============================] - 82s 730ms/step - loss: 0.7580 - accuracy: 0.3606 - val_loss: 0.7337 - val_accuracy: 0.3759\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.34595 to 0.37591, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 3/100\n",
            "113/113 [==============================] - 82s 728ms/step - loss: 0.6875 - accuracy: 0.3877 - val_loss: 0.7036 - val_accuracy: 0.3919\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.37591 to 0.39186, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 4/100\n",
            "113/113 [==============================] - 82s 729ms/step - loss: 0.6276 - accuracy: 0.4111 - val_loss: 0.6876 - val_accuracy: 0.4033\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.39186 to 0.40328, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 5/100\n",
            "113/113 [==============================] - 82s 729ms/step - loss: 0.5755 - accuracy: 0.4302 - val_loss: 0.6812 - val_accuracy: 0.4052\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.40328 to 0.40523, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 6/100\n",
            "113/113 [==============================] - 82s 730ms/step - loss: 0.5257 - accuracy: 0.4515 - val_loss: 0.6804 - val_accuracy: 0.4063\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.40523 to 0.40633, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 7/100\n",
            "113/113 [==============================] - 82s 730ms/step - loss: 0.4821 - accuracy: 0.4723 - val_loss: 0.6856 - val_accuracy: 0.4067\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.40633 to 0.40667, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 8/100\n",
            "113/113 [==============================] - 82s 730ms/step - loss: 0.4420 - accuracy: 0.4954 - val_loss: 0.6921 - val_accuracy: 0.4076\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.40667 to 0.40760, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 9/100\n",
            "113/113 [==============================] - 82s 729ms/step - loss: 0.4030 - accuracy: 0.5245 - val_loss: 0.7064 - val_accuracy: 0.4099\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.40760 to 0.40986, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 10/100\n",
            "113/113 [==============================] - 82s 728ms/step - loss: 0.3666 - accuracy: 0.5526 - val_loss: 0.7207 - val_accuracy: 0.4122\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.40986 to 0.41221, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 11/100\n",
            "113/113 [==============================] - 82s 728ms/step - loss: 0.3358 - accuracy: 0.5783 - val_loss: 0.7293 - val_accuracy: 0.4084\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.41221\n",
            "Epoch 12/100\n",
            "113/113 [==============================] - 82s 727ms/step - loss: 0.3081 - accuracy: 0.6032 - val_loss: 0.7541 - val_accuracy: 0.4124\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.41221 to 0.41244, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 13/100\n",
            "113/113 [==============================] - 82s 727ms/step - loss: 0.2857 - accuracy: 0.6235 - val_loss: 0.7618 - val_accuracy: 0.4061\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.41244\n",
            "Epoch 14/100\n",
            "113/113 [==============================] - 82s 728ms/step - loss: 0.2668 - accuracy: 0.6420 - val_loss: 0.7851 - val_accuracy: 0.4087\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.41244\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "Epoch 15/100\n",
            "113/113 [==============================] - 82s 730ms/step - loss: 0.2156 - accuracy: 0.7001 - val_loss: 0.7740 - val_accuracy: 0.4187\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.41244 to 0.41872, saving model to /content/drive/MyDrive/Transformers/final/weights_adam.ckpt\n",
            "Epoch 16/100\n",
            "113/113 [==============================] - 82s 727ms/step - loss: 0.1763 - accuracy: 0.7506 - val_loss: 0.7904 - val_accuracy: 0.4154\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.41872\n",
            "Epoch 17/100\n",
            "113/113 [==============================] - 82s 727ms/step - loss: 0.1551 - accuracy: 0.7790 - val_loss: 0.8111 - val_accuracy: 0.4138\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.41872\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 18/100\n",
            "113/113 [==============================] - 83s 730ms/step - loss: 0.1344 - accuracy: 0.8070 - val_loss: 0.8108 - val_accuracy: 0.4167\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.41872\n",
            "Epoch 19/100\n",
            "113/113 [==============================] - 82s 730ms/step - loss: 0.1246 - accuracy: 0.8210 - val_loss: 0.8189 - val_accuracy: 0.4148\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.41872\n",
            "Epoch 20/100\n",
            "113/113 [==============================] - 83s 730ms/step - loss: 0.1178 - accuracy: 0.8305 - val_loss: 0.8273 - val_accuracy: 0.4135\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.41872\n",
            "Epoch 00020: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fde87cc5f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Qvebl8FHsG",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:39:58.377008Z",
          "iopub.execute_input": "2021-06-08T02:39:58.377328Z",
          "iopub.status.idle": "2021-06-08T02:39:58.713799Z",
          "shell.execute_reply.started": "2021-06-08T02:39:58.377296Z",
          "shell.execute_reply": "2021-06-08T02:39:58.712839Z"
        },
        "trusted": true,
        "outputId": "409bb93b-6c20-4a05-d937-b01576015d44"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(googledrive_path)\n",
        "transformer.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa2c86f8210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8447490b",
        "execution": {
          "iopub.status.busy": "2021-06-08T04:04:11.583186Z",
          "iopub.execute_input": "2021-06-08T04:04:11.583522Z",
          "iopub.status.idle": "2021-06-08T04:04:17.842227Z",
          "shell.execute_reply.started": "2021-06-08T04:04:11.583490Z",
          "shell.execute_reply": "2021-06-08T04:04:17.841483Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa6355b-e09d-4c15-e46c-cf877dce6ce0"
      },
      "source": [
        "ar_vocab = ar_vectorization.get_vocabulary()\n",
        "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
        "max_decoded_sentence_length = sequence_length\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in val_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts[:30])\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, '\\n', translated)\n",
        "    print('*'*50)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I guess everything's okay because nothing comes up. \n",
            " [start] اعتقد ان كل شيء انتهي كل شيء انتهي [end]\n",
            "**************************************************\n",
            "I think once your friend, Mr. Caldwell, understands the situation, he'll fill in some missing pieces. \n",
            " [start] اعتقد ان ال وقت ال سيد كودويل سيتصل بك ال جحيم و ال نهاية [end]\n",
            "**************************************************\n",
            "What are you doing? \n",
            " [start] ماذا تفعل ؟ [end]\n",
            "**************************************************\n",
            "It's that woman. \n",
            " [start] انها تلك ال مراة [end]\n",
            "**************************************************\n",
            "The papers. What's the idea of throwing them out? \n",
            " [start] ال ذي ما ال فكرة ؟ [end]\n",
            "**************************************************\n",
            "That has not been part of my teaching. \n",
            " [start] هذا لن [UNK] من ال جزء من ال خاصة به ال خاصة به ال خاصة به ال خاصة [end]\n",
            "**************************************************\n",
            "He died before they reached port, not before the skipper had a description of the island and got an idea of where it lies. \n",
            " [start] لقد مات قبل ان تقلع منهم ال جزيره ، ال قبطان ال ذي وصل لاكثر من ال جزيرة ، [UNK] [end]\n",
            "**************************************************\n",
            "I guess everything's okay because nothing comes up. \n",
            " [start] اعتقد ان كل شيء انتهي كل شيء انتهي [end]\n",
            "**************************************************\n",
            "I think once your friend, Mr. Caldwell, understands the situation, he'll fill in some missing pieces. \n",
            " [start] اعتقد ان ال وقت ال سيد كودويل سيتصل بك ال جحيم و ال نهاية [end]\n",
            "**************************************************\n",
            "A popular destination for Presidents' Day. \n",
            " [start] رييس ال سفر ل 15 يوم [end]\n",
            "**************************************************\n",
            "Look on the back. \n",
            " [start] انظر الى ال ظهر [end]\n",
            "**************************************************\n",
            "Look on the back. \n",
            " [start] انظر الى ال ظهر [end]\n",
            "**************************************************\n",
            "Yes sir. I'm sorry. It was my car. \n",
            " [start] نعم ، سيد سيارة ال سيارة سيارة [end]\n",
            "**************************************************\n",
            "She's gone, Little Cheetah. \n",
            " [start] لقد ذهب قريبا شيتا [end]\n",
            "**************************************************\n",
            "I'll do whatever you want! \n",
            " [start] سوف افعل شييا [end]\n",
            "**************************************************\n",
            "U.S. Post Office. \n",
            " [start] مركز ال زبون [end]\n",
            "**************************************************\n",
            "I should be overwhelming her based on machine specs! \n",
            " [start] يجب ان تكون صحيحة اجل [UNK] [UNK] [end]\n",
            "**************************************************\n",
            "I think once your friend, Mr. Caldwell, understands the situation, he'll fill in some missing pieces. \n",
            " [start] اعتقد ان ال وقت ال سيد كودويل سيتصل بك ال جحيم و ال نهاية [end]\n",
            "**************************************************\n",
            "She's gone, Little Cheetah. \n",
            " [start] لقد ذهب قريبا شيتا [end]\n",
            "**************************************************\n",
            "U.S. Post Office. \n",
            " [start] مركز ال زبون [end]\n",
            "**************************************************\n",
            "Was I okay? \n",
            " [start] هل كنت بخير ؟ [end]\n",
            "**************************************************\n",
            "I'll do whatever you want! \n",
            " [start] سوف افعل شييا [end]\n",
            "**************************************************\n",
            "It's that woman. \n",
            " [start] انها تلك ال مراة [end]\n",
            "**************************************************\n",
            "! - Dead or alive! \n",
            " [start] حيا او ميتا [end]\n",
            "**************************************************\n",
            "Ma! \n",
            " [start] امي [end]\n",
            "**************************************************\n",
            "The CEO is... giving you something that I could never give. \n",
            " [start] لقد ال رييس [UNK] شييا [end]\n",
            "**************************************************\n",
            "A popular destination for Presidents' Day. \n",
            " [start] رييس ال سفر ل 15 يوم [end]\n",
            "**************************************************\n",
            "That has not been part of my teaching. \n",
            " [start] هذا لن [UNK] من ال جزء من ال خاصة به ال خاصة به ال خاصة به ال خاصة [end]\n",
            "**************************************************\n",
            "U.S. Post Office. \n",
            " [start] مركز ال زبون [end]\n",
            "**************************************************\n",
            "The papers. What's the idea of throwing them out? \n",
            " [start] ال ذي ما ال فكرة ؟ [end]\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2i4SbVoP_Yz",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:40:58.148840Z",
          "iopub.execute_input": "2021-06-08T02:40:58.149218Z",
          "iopub.status.idle": "2021-06-08T02:40:58.155254Z",
          "shell.execute_reply.started": "2021-06-08T02:40:58.149177Z",
          "shell.execute_reply": "2021-06-08T02:40:58.154292Z"
        },
        "trusted": true
      },
      "source": [
        "def get_bleu():\n",
        "  \n",
        "  preds, src = [], []\n",
        "\n",
        "  with tqdm(total=len(val_pairs), position=0, leave=True) as pbar:\n",
        "    for en_sent, ar_sent in tqdm(val_pairs, position=0, leave=True):\n",
        "      translated = decode_sequence(en_sent)\n",
        "      preds.append(translated)\n",
        "      src.append(ar_sent)\n",
        "      pbar.update()\n",
        "\n",
        "    return src, preds\n",
        "    # print_scores(src, preds)\n",
        "\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9uh1623AwUW",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:40:59.410199Z",
          "iopub.execute_input": "2021-06-08T02:40:59.410504Z",
          "iopub.status.idle": "2021-06-08T02:40:59.415944Z",
          "shell.execute_reply.started": "2021-06-08T02:40:59.410474Z",
          "shell.execute_reply": "2021-06-08T02:40:59.414990Z"
        },
        "trusted": true
      },
      "source": [
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "def print_scores(trgs, preds):\n",
        "    print('----- Bleu-n Scores -----')\n",
        "    print(\"1:\", corpus_bleu(trgs, preds, weights=[1.0/1.0])*100)\n",
        "    print(\"2:\", corpus_bleu(trgs, preds, weights=[1.0/2.0, 1.0/2.0])*100)\n",
        "    print(\"3:\", corpus_bleu(trgs, preds, weights=[1.0/3.0, 1.0/3.0, 1.0/3.0])*100)\n",
        "    print(\"4:\", corpus_bleu(trgs, preds)*100)\n",
        "    print('-'*25)\n",
        "    print('----- Rouge Scores -----')\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(preds, src, avg=True)\n",
        "    for key, item in scores.items():\n",
        "      print(key,':',item)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaXlIeHdRGqi",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:41:00.631949Z",
          "iopub.execute_input": "2021-06-08T02:41:00.632276Z",
          "iopub.status.idle": "2021-06-08T02:59:20.928004Z",
          "shell.execute_reply.started": "2021-06-08T02:41:00.632247Z",
          "shell.execute_reply": "2021-06-08T02:59:20.925932Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcbd6d86-eb50-4dd6-f778-cc6e2aa12ee9"
      },
      "source": [
        "src, preds = get_bleu()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5267/5267 [20:57<00:00,  4.19it/s]\n",
            "100%|██████████| 5267/5267 [20:57<00:00,  4.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-08T02:59:20.936277Z",
          "iopub.execute_input": "2021-06-08T02:59:20.936757Z",
          "iopub.status.idle": "2021-06-08T03:00:10.585162Z",
          "shell.execute_reply.started": "2021-06-08T02:59:20.936723Z",
          "shell.execute_reply": "2021-06-08T03:00:10.583379Z"
        },
        "trusted": true,
        "id": "-QGhrnethAL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "812c243e-30f2-40ab-e1f1-7a3f0f7a72c4"
      },
      "source": [
        "print_scores(src, preds)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 42.716420044320536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2: 65.35779987447599\n",
            "3: 75.31213326921176\n",
            "4: 80.84417101713393\n",
            "-------------------------\n",
            "----- Rouge Scores -----\n",
            "rouge-1 : {'f': 0.4937869320226216, 'p': 0.5281938079866801, 'r': 0.48853570740613916}\n",
            "rouge-2 : {'f': 0.16773662900926523, 'p': 0.17706054602896212, 'r': 0.1660911175213441}\n",
            "rouge-l : {'f': 0.5046094857087982, 'p': 0.5483246760158327, 'r': 0.4873552657643637}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}