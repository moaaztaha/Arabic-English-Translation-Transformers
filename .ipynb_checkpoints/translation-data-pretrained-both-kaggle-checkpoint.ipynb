{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T01:34:16.628366Z",
     "iopub.status.busy": "2021-06-08T01:34:16.627960Z",
     "iopub.status.idle": "2021-06-08T01:34:26.014739Z",
     "shell.execute_reply": "2021-06-08T01:34:26.013775Z",
     "shell.execute_reply.started": "2021-06-08T01:34:16.628281Z"
    },
    "id": "0R53kjrYGpSc",
    "outputId": "25e29470-6a5b-4cb8-8fba-6943fd0ca93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarabic in /opt/conda/lib/python3.7/site-packages (0.6.10)\n",
      "Cloning into 'Arabic-English-Translation-Transformers'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
      "remote: Total 39 (delta 13), reused 34 (delta 8), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarabic\n",
    "!git clone https://github.com/moaaztaha/Arabic-English-Translation-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:47.415253Z",
     "iopub.status.busy": "2021-06-08T02:16:47.414915Z",
     "iopub.status.idle": "2021-06-08T02:16:47.421088Z",
     "shell.execute_reply": "2021-06-08T02:16:47.419702Z",
     "shell.execute_reply.started": "2021-06-08T02:16:47.415225Z"
    },
    "id": "e9e97fa1"
   },
   "outputs": [],
   "source": [
    "# modules\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1710411f"
   },
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:47.955686Z",
     "iopub.status.busy": "2021-06-08T02:16:47.955355Z",
     "iopub.status.idle": "2021-06-08T02:16:48.136995Z",
     "shell.execute_reply": "2021-06-08T02:16:48.136060Z",
     "shell.execute_reply.started": "2021-06-08T02:16:47.955644Z"
    },
    "id": "1RNDMX9cizGW",
    "outputId": "e7f0bb9a-8761-44a8-c1cc-3f0331ca3fe3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n"
     ]
    }
   ],
   "source": [
    "ar = pd.read_table('../input/ar-en-translation-small/ArabicNewData.txt', delimiter='\\\\n', names=['ar'])\n",
    "en = pd.read_table('../input/ar-en-translation-small/EnglishNewData.txt', delimiter='\\\\n', names=['en'])\n",
    "\n",
    "en['ar'] = ar['ar']\n",
    "df = en.copy()\n",
    "df = df.iloc[:35118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:48.455428Z",
     "iopub.status.busy": "2021-06-08T02:16:48.455116Z",
     "iopub.status.idle": "2021-06-08T02:16:48.461638Z",
     "shell.execute_reply": "2021-06-08T02:16:48.460738Z",
     "shell.execute_reply.started": "2021-06-08T02:16:48.455402Z"
    },
    "id": "T2YTqqOTGl5c"
   },
   "outputs": [],
   "source": [
    "morphs = [strip_tashkeel, strip_tatweel]\n",
    "\n",
    "def fix_ar(sent):\n",
    "  sent = split_al_sent(sent)\n",
    "  tokens = araby.tokenize(sent, morphs=morphs)\n",
    "  sent = araby.normalize_hamza(' '.join(tokens), method='tasheel')\n",
    "  return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:48.992476Z",
     "iopub.status.busy": "2021-06-08T02:16:48.992161Z",
     "iopub.status.idle": "2021-06-08T02:16:48.998243Z",
     "shell.execute_reply": "2021-06-08T02:16:48.997288Z",
     "shell.execute_reply.started": "2021-06-08T02:16:48.992448Z"
    },
    "id": "7WhmXTv1KRkd"
   },
   "outputs": [],
   "source": [
    "def split_al(word):\n",
    "    if word.startswith('ال'):\n",
    "        return word[:2], word[2:]\n",
    "    else: \n",
    "        return word\n",
    "\n",
    "def split_al_sent(sent):\n",
    "    ww = []\n",
    "    for word in sent.split():\n",
    "        out = split_al(word)\n",
    "        if type(out) is tuple:\n",
    "            for w in out:\n",
    "                ww.append(w)\n",
    "        else:\n",
    "            ww.append(word)\n",
    "    return ' '.join(w for w in ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:50.345366Z",
     "iopub.status.busy": "2021-06-08T02:16:50.345045Z",
     "iopub.status.idle": "2021-06-08T02:16:51.796892Z",
     "shell.execute_reply": "2021-06-08T02:16:51.795902Z",
     "shell.execute_reply.started": "2021-06-08T02:16:50.345330Z"
    },
    "id": "QGuKlu3mFIZc"
   },
   "outputs": [],
   "source": [
    "df['ar'] = df.apply(lambda row: fix_ar(row.ar), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:51.799840Z",
     "iopub.status.busy": "2021-06-08T02:16:51.799257Z",
     "iopub.status.idle": "2021-06-08T02:16:54.618551Z",
     "shell.execute_reply": "2021-06-08T02:16:54.617702Z",
     "shell.execute_reply.started": "2021-06-08T02:16:51.799800Z"
    },
    "id": "be905b2c"
   },
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "for idx, row in df.iterrows():\n",
    "  en, ar = row['en'], row['ar']\n",
    "  ar = \"[start] \" + ar + \" [end]\"\n",
    "  text_pairs.append((en, ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:58.013120Z",
     "iopub.status.busy": "2021-06-08T02:16:58.012634Z",
     "iopub.status.idle": "2021-06-08T02:17:00.598315Z",
     "shell.execute_reply": "2021-06-08T02:17:00.597570Z",
     "shell.execute_reply.started": "2021-06-08T02:16:58.013084Z"
    },
    "id": "fAxNAdK2H0eB"
   },
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "  if len(row.ar.split()) < 1:\n",
    "    print(row.ar, '\\n*')\n",
    "    print(row.en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:00.600692Z",
     "iopub.status.busy": "2021-06-08T02:17:00.600320Z",
     "iopub.status.idle": "2021-06-08T02:17:00.605892Z",
     "shell.execute_reply": "2021-06-08T02:17:00.605074Z",
     "shell.execute_reply.started": "2021-06-08T02:17:00.600644Z"
    },
    "id": "296e4594",
    "outputId": "cf5ca9a6-e5ae-45ff-9b35-1dac88910f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Darling, we're going to forget all about these dreams and think about something cheerful, aren't we?\", '[start] عزيزتي , سننسى كل هذه ال احلام و نفكر في شىء مرح , اليس كذلك ؟ [end]')\n",
      "(\"But you can't murder me just like that!\", '[start] لكن لو تقتلني ! فستكون تلك جريمة قتل [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:00.608167Z",
     "iopub.status.busy": "2021-06-08T02:17:00.607746Z",
     "iopub.status.idle": "2021-06-08T02:17:00.620306Z",
     "shell.execute_reply": "2021-06-08T02:17:00.619437Z",
     "shell.execute_reply.started": "2021-06-08T02:17:00.608132Z"
    },
    "id": "o6kRoro9iBOe",
    "outputId": "1ec02783-32ee-45c0-bef7-e64dfb28b8c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35118"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:01.800560Z",
     "iopub.status.busy": "2021-06-08T02:17:01.800249Z",
     "iopub.status.idle": "2021-06-08T02:17:01.847161Z",
     "shell.execute_reply": "2021-06-08T02:17:01.846417Z",
     "shell.execute_reply.started": "2021-06-08T02:17:01.800532Z"
    },
    "id": "c1283f6f"
   },
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) -  num_val_samples\n",
    "train_pairs = text_pairs[: num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:02.646210Z",
     "iopub.status.busy": "2021-06-08T02:17:02.645892Z",
     "iopub.status.idle": "2021-06-08T02:17:02.654338Z",
     "shell.execute_reply": "2021-06-08T02:17:02.653626Z",
     "shell.execute_reply.started": "2021-06-08T02:17:02.646181Z"
    },
    "id": "a7a1f8cd",
    "outputId": "2ff87303-7ba6-43a8-f9f1-fdc7db9a7f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35118 total pairs\n",
      "29851 training pairs\n",
      "5267 validation pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5f6d84e"
   },
   "source": [
    "#### Vectorizing the text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:03.954544Z",
     "iopub.status.busy": "2021-06-08T02:17:03.954239Z",
     "iopub.status.idle": "2021-06-08T02:17:04.856227Z",
     "shell.execute_reply": "2021-06-08T02:17:04.854499Z",
     "shell.execute_reply.started": "2021-06-08T02:17:03.954516Z"
    },
    "id": "487ed66f"
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "vocab_size = 20000\n",
    "sequence_length = 50\n",
    "batch_size = 265\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    return tf.strings.regex_replace(input_string, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "            # max_tokens=vocab_size, \n",
    "            output_mode='int', \n",
    "            output_sequence_length=sequence_length)\n",
    "\n",
    "ar_vectorization = TextVectorization(\n",
    "    # max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size)\n",
    "\n",
    "eng_texts = [pair[0] for pair in text_pairs]\n",
    "ar_texts = [pair[1] for pair in text_pairs]\n",
    "eng_vectorization.adapt(eng_texts)\n",
    "ar_vectorization.adapt(ar_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:04.859565Z",
     "iopub.status.busy": "2021-06-08T02:17:04.859040Z",
     "iopub.status.idle": "2021-06-08T02:17:04.938344Z",
     "shell.execute_reply": "2021-06-08T02:17:04.937688Z",
     "shell.execute_reply.started": "2021-06-08T02:17:04.859525Z"
    },
    "id": "ubW5QOJtLzMI",
    "outputId": "255cd7fe-f248-4210-8572-21cafce65778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 13164)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:04.940155Z",
     "iopub.status.busy": "2021-06-08T02:17:04.939783Z",
     "iopub.status.idle": "2021-06-08T02:17:05.011110Z",
     "shell.execute_reply": "2021-06-08T02:17:05.010101Z",
     "shell.execute_reply.started": "2021-06-08T02:17:04.940116Z"
    },
    "id": "wep-jkjyLWz6",
    "outputId": "fa3d480c-3075-4d24-84c8-25324524b0fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 13164)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:05.013639Z",
     "iopub.status.busy": "2021-06-08T02:17:05.013252Z",
     "iopub.status.idle": "2021-06-08T02:17:05.021566Z",
     "shell.execute_reply": "2021-06-08T02:17:05.020566Z",
     "shell.execute_reply.started": "2021-06-08T02:17:05.013599Z"
    },
    "id": "V5qiqjOWdVz1"
   },
   "outputs": [],
   "source": [
    "def format_dataset(eng, ar):\n",
    "    eng = eng_vectorization(eng)\n",
    "    ar = ar_vectorization(ar)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, ar_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    ar_texts = list(ar_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:05.597209Z",
     "iopub.status.busy": "2021-06-08T02:17:05.596886Z",
     "iopub.status.idle": "2021-06-08T02:17:06.245999Z",
     "shell.execute_reply": "2021-06-08T02:17:06.245218Z",
     "shell.execute_reply.started": "2021-06-08T02:17:05.597181Z"
    },
    "id": "7a3ed140"
   },
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:06.248284Z",
     "iopub.status.busy": "2021-06-08T02:17:06.247956Z",
     "iopub.status.idle": "2021-06-08T02:17:06.519611Z",
     "shell.execute_reply": "2021-06-08T02:17:06.518592Z",
     "shell.execute_reply.started": "2021-06-08T02:17:06.248250Z"
    },
    "id": "3902789f",
    "outputId": "b85367db-eb0f-4bb3-9cf7-f2cf54ee12e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (265, 50)\n",
      "inputs[\"decoder_inputs\"].shape: (265, 50)\n",
      "targets.shape: (265, 50)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fa0936c"
   },
   "source": [
    "### Building the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:07.222939Z",
     "iopub.status.busy": "2021-06-08T02:17:07.222610Z",
     "iopub.status.idle": "2021-06-08T02:17:07.247405Z",
     "shell.execute_reply": "2021-06-08T02:17:07.246264Z",
     "shell.execute_reply.started": "2021-06-08T02:17:07.222907Z"
    },
    "id": "b3baf452"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "          'embed_dim': self.embed_dim,\n",
    "          'dense_dim': self.dense_dim,\n",
    "          'num_heads': self.num_heads,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, pretrained=False, weights=False, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        if not pretrained:\n",
    "          self.token_embeddings = layers.Embedding(\n",
    "              input_dim=vocab_size, output_dim=embed_dim\n",
    "          )\n",
    "        else:\n",
    "          # pre-trained\n",
    "          self.token_embeddings = layers.Embedding(\n",
    "              input_dim=vocab_size, output_dim=embed_dim, weights=[weights]\n",
    "          ) \n",
    "\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "      \n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "      'sequence_length': self.sequence_length,\n",
    "      'vocab_size': self.vocab_size,\n",
    "      'embed_dim': self.embed_dim,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "      'embed_dim': self.embed_dim,\n",
    "      'latent_dim': self.latent_dim,\n",
    "      'num_heads': self.num_heads,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:20.073181Z",
     "iopub.status.busy": "2021-06-08T02:17:20.072791Z",
     "iopub.status.idle": "2021-06-08T02:20:23.160034Z",
     "shell.execute_reply": "2021-06-08T02:20:23.158925Z",
     "shell.execute_reply.started": "2021-06-08T02:17:20.073151Z"
    },
    "id": "fpXVu3eA_b9P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-08 02:17:20--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-06-08 02:17:20--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-06-08 02:17:21--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.26MB/s    in 2m 40s  \n",
      "\n",
      "2021-06-08 02:20:01 (5.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:21:45.995027Z",
     "iopub.status.busy": "2021-06-08T02:21:45.994677Z",
     "iopub.status.idle": "2021-06-08T02:22:15.646535Z",
     "shell.execute_reply": "2021-06-08T02:22:15.644971Z",
     "shell.execute_reply.started": "2021-06-08T02:21:45.994991Z"
    },
    "id": "afjYy3g6BM2K",
    "outputId": "e32988e0-c411-403e-c96b-f847eb6a7677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), f\"{os.getcwd()}/glove.6B.300d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:22:19.678697Z",
     "iopub.status.busy": "2021-06-08T02:22:19.678371Z",
     "iopub.status.idle": "2021-06-08T02:22:19.715575Z",
     "shell.execute_reply": "2021-06-08T02:22:19.714615Z",
     "shell.execute_reply.started": "2021-06-08T02:22:19.678659Z"
    },
    "id": "XRNRUzOCtccc"
   },
   "outputs": [],
   "source": [
    "vocab = eng_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:15:46.077892Z",
     "iopub.status.busy": "2021-06-08T03:15:46.077526Z",
     "iopub.status.idle": "2021-06-08T03:15:46.207948Z",
     "shell.execute_reply": "2021-06-08T03:15:46.207172Z",
     "shell.execute_reply.started": "2021-06-08T03:15:46.077854Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:46:29.637271Z",
     "iopub.status.busy": "2021-06-08T03:46:29.636942Z",
     "iopub.status.idle": "2021-06-08T03:46:29.674663Z",
     "shell.execute_reply": "2021-06-08T03:46:29.673702Z",
     "shell.execute_reply.started": "2021-06-08T03:46:29.637241Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_weights(vectorizer, embeddings_path, is_gensim=False):\n",
    "    path_to_glove_file = embeddings_path\n",
    "    \n",
    "    if is_gensim:\n",
    "        embeddings_index = gensim.models.Word2Vec.load(embeddings_path)\n",
    "        \n",
    "    else:\n",
    "        embeddings_index = {}\n",
    "        with open(path_to_glove_file) as f:\n",
    "            for line in f:\n",
    "                word, coefs = line.split(maxsplit=1)\n",
    "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                embeddings_index[word] = coefs\n",
    "    \n",
    "    if not is_gensim:\n",
    "        print(\"Found %s word vectors.\" % len(dict(embeddings_index)))\n",
    "    else:\n",
    "        print(\"Found %s word vectors.\" % (embeddings_index.wv.vectors.shape[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    vocab = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "    num_tokens = len(vocab)\n",
    "    embedding_dim = 300\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if not is_gensim:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "        else:\n",
    "            if word in embeddings_index.wv:\n",
    "                embedding_vector = embeddings_index.wv[word]\n",
    "            else:\n",
    "                embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.uniform(-.1, .1, size=(embedding_dim))\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:47:05.607709Z",
     "iopub.status.busy": "2021-06-08T03:47:05.607322Z",
     "iopub.status.idle": "2021-06-08T03:47:35.559143Z",
     "shell.execute_reply": "2021-06-08T03:47:35.557315Z",
     "shell.execute_reply.started": "2021-06-08T03:47:05.607653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Converted 11896 words (1268 misses)\n"
     ]
    }
   ],
   "source": [
    "english_embeddings = get_weights(eng_vectorization, './glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:46:30.660121Z",
     "iopub.status.busy": "2021-06-08T03:46:30.659781Z",
     "iopub.status.idle": "2021-06-08T03:46:30.663804Z",
     "shell.execute_reply": "2021-06-08T03:46:30.662582Z",
     "shell.execute_reply.started": "2021-06-08T03:46:30.660094Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! wget https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_cbow_300_wiki.zip\n",
    "# ! unzip -q full_grams_cbow_300_wiki.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:46:30.842754Z",
     "iopub.status.busy": "2021-06-08T03:46:30.842479Z",
     "iopub.status.idle": "2021-06-08T03:46:53.711963Z",
     "shell.execute_reply": "2021-06-08T03:46:53.710792Z",
     "shell.execute_reply.started": "2021-06-08T03:46:30.842728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 662109 word vectors.\n",
      "Converted 12646 words (7354 misses)\n"
     ]
    }
   ],
   "source": [
    "arabic_embeddings = get_weights(ar_vectorization, './full_grams_cbow_300_wiki.mdl', is_gensim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:47:59.830984Z",
     "iopub.status.busy": "2021-06-08T03:47:59.830643Z",
     "iopub.status.idle": "2021-06-08T03:47:59.906741Z",
     "shell.execute_reply": "2021-06-08T03:47:59.905831Z",
     "shell.execute_reply.started": "2021-06-08T03:47:59.830952Z"
    },
    "id": "U0rA0C4s1OVW",
    "outputId": "1d2f5bc1-65d4-4e4e-cd22-8fad12f6cc0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13164, 20000)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_vocab_size = len(ar_vectorization.get_vocabulary())\n",
    "en_vocab_size = len(eng_vectorization.get_vocabulary())\n",
    "en_vocab_size, ar_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:49:47.423713Z",
     "iopub.status.busy": "2021-06-08T03:49:47.423352Z",
     "iopub.status.idle": "2021-06-08T03:49:48.045217Z",
     "shell.execute_reply": "2021-06-08T03:49:48.044246Z",
     "shell.execute_reply.started": "2021-06-08T03:49:47.423679Z"
    },
    "id": "04c57e1e"
   },
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, en_vocab_size, embed_dim, pretrained=True, weights=english_embeddings)(encoder_inputs)\n",
    "# x = PositionalEmbedding(sequence_length, num_tokens, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim, pretrained=True, weights=arabic_embeddings)(decoder_inputs)\n",
    "# x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(ar_vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:49:49.472124Z",
     "iopub.status.busy": "2021-06-08T03:49:49.471793Z",
     "iopub.status.idle": "2021-06-08T03:49:49.477189Z",
     "shell.execute_reply": "2021-06-08T03:49:49.476150Z",
     "shell.execute_reply.started": "2021-06-08T03:49:49.472094Z"
    },
    "id": "vt9sp6Yrb4pE"
   },
   "outputs": [],
   "source": [
    "googledrive_path = './pretrained_both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:49:54.155939Z",
     "iopub.status.busy": "2021-06-08T03:49:54.155540Z",
     "iopub.status.idle": "2021-06-08T03:49:54.164631Z",
     "shell.execute_reply": "2021-06-08T03:49:54.163841Z",
     "shell.execute_reply.started": "2021-06-08T03:49:54.155904Z"
    },
    "id": "wqmaEQ94kyB9"
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "early_stopping_cb = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=True)\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(googledrive_path+'/weights_adam.ckpt', monitor='val_accuracy', save_weights_only=True,verbose=True, save_best_only=True)\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=googledrive_path+\"/logs\")\n",
    "lr_schr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=True, factor=0.3, min_lr=0.0001)\n",
    "cbs = [early_stopping_cb, checkpoint_cb, tensorboard_callback, lr_schr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:50:03.473046Z",
     "iopub.status.busy": "2021-06-08T03:50:03.472687Z",
     "iopub.status.idle": "2021-06-08T04:02:41.112268Z",
     "shell.execute_reply": "2021-06-08T04:02:41.111344Z",
     "shell.execute_reply.started": "2021-06-08T03:50:03.473016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding_8 (Positio (None, None, 300)    3964200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_4 (Transfor (None, None, 300)    4119848     positional_embedding_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Functional)            (None, None, 20000)  19042948    decoder_inputs[0][0]             \n",
      "                                                                 transformer_encoder_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 27,126,996\n",
      "Trainable params: 27,126,996\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 57s 483ms/step - loss: 1.0848 - accuracy: 0.2297 - val_loss: 0.8051 - val_accuracy: 0.3303\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.33027, saving model to ./pretrained_both/weights_adam.ckpt\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 54s 474ms/step - loss: 0.7797 - accuracy: 0.3474 - val_loss: 0.7445 - val_accuracy: 0.3720\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.33027 to 0.37196, saving model to ./pretrained_both/weights_adam.ckpt\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 54s 476ms/step - loss: 0.7033 - accuracy: 0.3805 - val_loss: 0.7138 - val_accuracy: 0.3864\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.37196 to 0.38642, saving model to ./pretrained_both/weights_adam.ckpt\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 54s 476ms/step - loss: 0.6402 - accuracy: 0.4048 - val_loss: 0.6999 - val_accuracy: 0.3941\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.38642 to 0.39413, saving model to ./pretrained_both/weights_adam.ckpt\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 53s 473ms/step - loss: 0.5850 - accuracy: 0.4269 - val_loss: 0.6870 - val_accuracy: 0.4045\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.39413 to 0.40451, saving model to ./pretrained_both/weights_adam.ckpt\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 53s 473ms/step - loss: 0.5348 - accuracy: 0.4466 - val_loss: 0.6839 - val_accuracy: 0.4059\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.40451 to 0.40587, saving model to ./pretrained_both/weights_adam.ckpt\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 53s 472ms/step - loss: 0.4889 - accuracy: 0.4683 - val_loss: 0.6898 - val_accuracy: 0.4023\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.40587\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 53s 473ms/step - loss: 0.4481 - accuracy: 0.4914 - val_loss: 0.7031 - val_accuracy: 0.4022\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.40587\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 54s 474ms/step - loss: 0.3952 - accuracy: 0.5317 - val_loss: 0.6861 - val_accuracy: 0.4190\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.40587 to 0.41904, saving model to ./pretrained_both/weights_adam.ckpt\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 53s 471ms/step - loss: 0.3452 - accuracy: 0.5789 - val_loss: 0.6964 - val_accuracy: 0.4181\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.41904\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 53s 473ms/step - loss: 0.3128 - accuracy: 0.6122 - val_loss: 0.7087 - val_accuracy: 0.4164\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.41904\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 54s 474ms/step - loss: 0.2836 - accuracy: 0.6418 - val_loss: 0.7113 - val_accuracy: 0.4189\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.41904\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 53s 473ms/step - loss: 0.2682 - accuracy: 0.6599 - val_loss: 0.7179 - val_accuracy: 0.4181\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.41904\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 53s 473ms/step - loss: 0.2554 - accuracy: 0.6746 - val_loss: 0.7245 - val_accuracy: 0.4180\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.41904\n",
      "Epoch 00014: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e6d1137d0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:23:20.904743Z",
     "iopub.status.busy": "2021-06-08T02:23:20.904352Z",
     "iopub.status.idle": "2021-06-08T02:39:48.287839Z",
     "shell.execute_reply": "2021-06-08T02:39:48.287102Z",
     "shell.execute_reply.started": "2021-06-08T02:23:20.904704Z"
    },
    "id": "H50DkdfyLyqt",
    "outputId": "14b02f52-c376-4b58-9dab-288665275e71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding_4 (Positio (None, None, 300)    3964200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_2 (Transfor (None, None, 300)    4119848     positional_embedding_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "model_5 (Functional)            (None, None, 20000)  19042948    decoder_inputs[0][0]             \n",
      "                                                                 transformer_encoder_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 27,126,996\n",
      "Trainable params: 27,126,996\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 59s 498ms/step - loss: 1.0497 - accuracy: 0.2615 - val_loss: 0.8023 - val_accuracy: 0.3430\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.34302, saving model to ./pretrained_en/weights_adam.ckpt\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 54s 479ms/step - loss: 0.7741 - accuracy: 0.3561 - val_loss: 0.7480 - val_accuracy: 0.3737\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.34302 to 0.37374, saving model to ./pretrained_en/weights_adam.ckpt\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.6870 - accuracy: 0.3973 - val_loss: 0.7232 - val_accuracy: 0.3868\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.37374 to 0.38684, saving model to ./pretrained_en/weights_adam.ckpt\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 54s 481ms/step - loss: 0.6143 - accuracy: 0.4313 - val_loss: 0.7174 - val_accuracy: 0.3919\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.38684 to 0.39191, saving model to ./pretrained_en/weights_adam.ckpt\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.5502 - accuracy: 0.4588 - val_loss: 0.7178 - val_accuracy: 0.3896\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.39191\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 54s 479ms/step - loss: 0.4938 - accuracy: 0.4889 - val_loss: 0.7225 - val_accuracy: 0.4016\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.39191 to 0.40165, saving model to ./pretrained_en/weights_adam.ckpt\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.4389 - accuracy: 0.5204 - val_loss: 0.7294 - val_accuracy: 0.4006\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.40165\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.3882 - accuracy: 0.5550 - val_loss: 0.7288 - val_accuracy: 0.3982\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.40165\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.3292 - accuracy: 0.6020 - val_loss: 0.7174 - val_accuracy: 0.4111\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.40165 to 0.41113, saving model to ./pretrained_en/weights_adam.ckpt\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.2778 - accuracy: 0.6564 - val_loss: 0.7329 - val_accuracy: 0.4113\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.41113 to 0.41133, saving model to ./pretrained_en/weights_adam.ckpt\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 54s 482ms/step - loss: 0.2468 - accuracy: 0.6915 - val_loss: 0.7502 - val_accuracy: 0.4082\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.41133\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 54s 481ms/step - loss: 0.2200 - accuracy: 0.7223 - val_loss: 0.7668 - val_accuracy: 0.4089\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.41133\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.1972 - accuracy: 0.7470 - val_loss: 0.7729 - val_accuracy: 0.4126\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.41133 to 0.41265, saving model to ./pretrained_en/weights_adam.ckpt\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 54s 479ms/step - loss: 0.1835 - accuracy: 0.7640 - val_loss: 0.7813 - val_accuracy: 0.4117\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.41265\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.1723 - accuracy: 0.7791 - val_loss: 0.7897 - val_accuracy: 0.4097\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.41265\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 54s 479ms/step - loss: 0.1636 - accuracy: 0.7895 - val_loss: 0.7982 - val_accuracy: 0.4074\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.41265\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.1551 - accuracy: 0.7997 - val_loss: 0.8071 - val_accuracy: 0.4075\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.41265\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 54s 480ms/step - loss: 0.1479 - accuracy: 0.8098 - val_loss: 0.8159 - val_accuracy: 0.4061\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.41265\n",
      "Epoch 00018: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9eab9aad10>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:39:58.377328Z",
     "iopub.status.busy": "2021-06-08T02:39:58.377008Z",
     "iopub.status.idle": "2021-06-08T02:39:58.713799Z",
     "shell.execute_reply": "2021-06-08T02:39:58.712839Z",
     "shell.execute_reply.started": "2021-06-08T02:39:58.377296Z"
    },
    "id": "j5Qvebl8FHsG",
    "outputId": "409bb93b-6c20-4a05-d937-b01576015d44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa2c86f8210>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(googledrive_path)\n",
    "transformer.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T04:04:11.583522Z",
     "iopub.status.busy": "2021-06-08T04:04:11.583186Z",
     "iopub.status.idle": "2021-06-08T04:04:17.842227Z",
     "shell.execute_reply": "2021-06-08T04:04:17.841483Z",
     "shell.execute_reply.started": "2021-06-08T04:04:11.583490Z"
    },
    "id": "8447490b",
    "outputId": "0579b30f-8902-4242-8ac0-574cfd8c53da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was originally just a water container. \n",
      " [start] هذه كانت ال حجرة سفينة فقط [UNK] [end]\n",
      "**************************************************\n",
      "Room 1 4. Next to the solarium. \n",
      " [start] ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة [end]\n",
      "**************************************************\n",
      "Maybe not as thick as the ones that Joshua blew down with his trumpet. \n",
      " [start] ربما لا بين ال ثلج كما [UNK] [end]\n",
      "**************************************************\n",
      "Room 1 4. Next to the solarium. \n",
      " [start] ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة [end]\n",
      "**************************************************\n",
      "You're not Han Na, are you? \n",
      " [start] انت لست هان نا ، اليس كذلك ؟ [end]\n",
      "**************************************************\n",
      "I Won't Be Here,And- \n",
      " [start] لن اكون ساكون [end]\n",
      "**************************************************\n",
      "You're not Han Na, are you? \n",
      " [start] انت لست هان نا ، اليس كذلك ؟ [end]\n",
      "**************************************************\n",
      "I'll keep my foot here, and you hit it whenever you can. \n",
      " [start] [UNK] على ال ان تذهب و [UNK] [end]\n",
      "**************************************************\n",
      "So that is indeed what they're planning. \n",
      " [start] لذا ، هذا ال حقيقة انهم يقولوا ماذا ؟ [end]\n",
      "**************************************************\n",
      "I Won't Be Here,And- \n",
      " [start] لن اكون ساكون [end]\n",
      "**************************************************\n",
      "That's large-format barrels, neutral oak. \n",
      " [start] ذلك سحر ال [UNK] ال سرو [end]\n",
      "**************************************************\n",
      "Father, come on. \n",
      " [start] ابي هيا [end]\n",
      "**************************************************\n",
      "You're definitely not the Tony Stark I once knew. \n",
      " [start] انت متاكد انك لا توني ، ان [end]\n",
      "**************************************************\n",
      "This was an office for ghosts? \n",
      " [start] لقد كنت هنا بسبب شبح [end]\n",
      "**************************************************\n",
      "This was an office for ghosts? \n",
      " [start] لقد كنت هنا بسبب شبح [end]\n",
      "**************************************************\n",
      "Yeah,Mr. Meade Meant A Lot To Me, \n",
      " [start] كلير ميد انا ، انت ، انا [end]\n",
      "**************************************************\n",
      "I'll keep my foot here, and you hit it whenever you can. \n",
      " [start] [UNK] على ال ان تذهب و [UNK] [end]\n",
      "**************************************************\n",
      "Find out what she's into, and then also be into it. \n",
      " [start] يبحث عنها و هي و هي ايضا [end]\n",
      "**************************************************\n",
      "Kang Woo, are you having a hard time? \n",
      " [start] كانغ وو ، هل انت ايضا ؟ [end]\n",
      "**************************************************\n",
      "Tarzan, don't. \n",
      " [start] ترازان لا [end]\n",
      "**************************************************\n",
      "I purposely didn't given it back to him because that's what I wanted. \n",
      " [start] لقد [UNK] ال ذي لم افعل ما هو [end]\n",
      "**************************************************\n",
      "- Later, perhaps. \n",
      " [start] ربما ربما [end]\n",
      "**************************************************\n",
      "Thank you! \n",
      " [start] شكرا لك [end]\n",
      "**************************************************\n",
      "Honey! \n",
      " [start] عزيزتي [end]\n",
      "**************************************************\n",
      "-Let him have it! \n",
      " [start] [UNK] [end]\n",
      "**************************************************\n",
      "This was an office for ghosts? \n",
      " [start] لقد كنت هنا بسبب شبح [end]\n",
      "**************************************************\n",
      "- Being confident without begging... \n",
      " [start] احيانا [end]\n",
      "**************************************************\n",
      "Find out what she's into, and then also be into it. \n",
      " [start] يبحث عنها و هي و هي ايضا [end]\n",
      "**************************************************\n",
      "- Bed 26, sir. \n",
      " [start] سرير رقم [UNK] [end]\n",
      "**************************************************\n",
      "That's large-format barrels, neutral oak. \n",
      " [start] ذلك سحر ال [UNK] ال سرو [end]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "ar_vocab = ar_vectorization.get_vocabulary()\n",
    "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
    "max_decoded_sentence_length = sequence_length\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = ar_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in val_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts[:30])\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence, '\\n', translated)\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:40:58.149218Z",
     "iopub.status.busy": "2021-06-08T02:40:58.148840Z",
     "iopub.status.idle": "2021-06-08T02:40:58.155254Z",
     "shell.execute_reply": "2021-06-08T02:40:58.154292Z",
     "shell.execute_reply.started": "2021-06-08T02:40:58.149177Z"
    },
    "id": "p2i4SbVoP_Yz"
   },
   "outputs": [],
   "source": [
    "def get_bleu():\n",
    "  \n",
    "  preds, src = [], []\n",
    "\n",
    "  with tqdm(total=len(val_pairs), position=0, leave=True) as pbar:\n",
    "    for en_sent, ar_sent in tqdm(val_pairs, position=0, leave=True):\n",
    "      translated = decode_sequence(en_sent)\n",
    "      preds.append(translated)\n",
    "      src.append(ar_sent)\n",
    "      pbar.update()\n",
    "\n",
    "    return src, preds\n",
    "    # print_scores(src, preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:40:59.410504Z",
     "iopub.status.busy": "2021-06-08T02:40:59.410199Z",
     "iopub.status.idle": "2021-06-08T02:40:59.415944Z",
     "shell.execute_reply": "2021-06-08T02:40:59.414990Z",
     "shell.execute_reply.started": "2021-06-08T02:40:59.410474Z"
    },
    "id": "A9uh1623AwUW"
   },
   "outputs": [],
   "source": [
    "def print_scores(trgs, preds):\n",
    "    print('----- Bleu-n Scores -----')\n",
    "    print(\"1:\", corpus_bleu(trgs, preds, weights=[1.0/1.0])*100)\n",
    "    print(\"2:\", corpus_bleu(trgs, preds, weights=[1.0/2.0, 1.0/2.0])*100)\n",
    "    print(\"3:\", corpus_bleu(trgs, preds, weights=[1.0/3.0, 1.0/3.0, 1.0/3.0])*100)\n",
    "    print(\"4:\", corpus_bleu(trgs, preds)*100)\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:41:00.632276Z",
     "iopub.status.busy": "2021-06-08T02:41:00.631949Z",
     "iopub.status.idle": "2021-06-08T02:59:20.928004Z",
     "shell.execute_reply": "2021-06-08T02:59:20.925932Z",
     "shell.execute_reply.started": "2021-06-08T02:41:00.632247Z"
    },
    "id": "YaXlIeHdRGqi",
    "outputId": "ee8a6fbf-2d72-4bc6-b65c-ce54f944723d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5267/5267 [18:20<00:00,  4.79it/s]\n",
      "100%|██████████| 5267/5267 [18:20<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "src, preds = get_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:59:20.930082Z",
     "iopub.status.busy": "2021-06-08T02:59:20.929740Z",
     "iopub.status.idle": "2021-06-08T02:59:20.934886Z",
     "shell.execute_reply": "2021-06-08T02:59:20.933814Z",
     "shell.execute_reply.started": "2021-06-08T02:59:20.930048Z"
    },
    "id": "tSinG65pSGkC"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:59:20.936757Z",
     "iopub.status.busy": "2021-06-08T02:59:20.936277Z",
     "iopub.status.idle": "2021-06-08T03:00:10.585162Z",
     "shell.execute_reply": "2021-06-08T03:00:10.583379Z",
     "shell.execute_reply.started": "2021-06-08T02:59:20.936723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 40.14637653331428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 63.3611683393814\n",
      "3: 73.77039652394956\n",
      "4: 79.59972885593355\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_scores(preds, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:00:17.962374Z",
     "iopub.status.busy": "2021-06-08T03:00:17.962062Z",
     "iopub.status.idle": "2021-06-08T03:00:17.973887Z",
     "shell.execute_reply": "2021-06-08T03:00:17.971402Z",
     "shell.execute_reply.started": "2021-06-08T03:00:17.962344Z"
    },
    "id": "mBYxHDaET9ya",
    "outputId": "b43e79a4-0bf6-4ad8-9b39-51d353de5dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  اذا كان [UNK] على ال يدي\n",
      "source:  لو وق بين يدى\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  ماذا قلت ؟\n",
      "source:  - ماذا قلت ؟\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  يبدو انك [UNK]\n",
      "source:  . واو .... رييس انت تكسب ال كثير من ال اموال\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  لا ، لا ، لا\n",
      "source:  لا يوجد رد\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  هذه هي ال احذية لكن ، اعتقد انه في سريرك\n",
      "source:  هذه امتعته ، لكن اعتقد هو في حجرته .\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  لانى [UNK]\n",
      "source:  #، هذا جزيي ال مفضل لانكم #\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  اكره ال ناس ال اكره ال طلاب\n",
      "source:  اكره متاعب ال فتيات\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  الى ال كونت هناك واجب لابتهاج ال خبز ال خبز و تنفيس\n",
      "source:  بالسجن ال ابدي ان تاكلي خبز ال حزن و ان تشربي ماء ال عذاب\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  كل ال خروج من ال طريق\n",
      "source:  حسنا ايها ال ناس ابتعدو من هنا\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  لا افهم\n",
      "source:  لست افهم\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "  i = random.randint(0, 500)\n",
    "  print(\"prediction:\", preds[i][7:-6])\n",
    "  print(\"source:\", src[i][7:-6])\n",
    "  print('_'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T04:06:15.256470Z",
     "iopub.status.busy": "2021-06-08T04:06:15.256141Z",
     "iopub.status.idle": "2021-06-08T04:06:31.744554Z",
     "shell.execute_reply": "2021-06-08T04:06:31.743387Z",
     "shell.execute_reply.started": "2021-06-08T04:06:15.256439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: pretrained_both/weights_adam.ckpt.data-00000-of-00001 (deflated 9%)\n",
      "  adding: pretrained_en/weights_adam.ckpt.index (deflated 76%)\n"
     ]
    }
   ],
   "source": [
    "!zip translation_pre_both.zip ./pretrained_both/weights_adam.ckpt.data-00000-of-00001 ./pretrained_en/weights_adam.ckpt.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T04:06:37.795508Z",
     "iopub.status.busy": "2021-06-08T04:06:37.795168Z",
     "iopub.status.idle": "2021-06-08T04:06:37.803246Z",
     "shell.execute_reply": "2021-06-08T04:06:37.802112Z",
     "shell.execute_reply.started": "2021-06-08T04:06:37.795475Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='translation_pre_both.zip' target='_blank'>translation_pre_both.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/translation_pre_both.zip"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'translation_pre_both.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
