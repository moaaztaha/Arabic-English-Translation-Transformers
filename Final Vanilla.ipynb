{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Final",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R53kjrYGpSc",
        "execution": {
          "iopub.status.busy": "2021-06-08T01:34:16.627960Z",
          "iopub.execute_input": "2021-06-08T01:34:16.628366Z",
          "iopub.status.idle": "2021-06-08T01:34:26.014739Z",
          "shell.execute_reply.started": "2021-06-08T01:34:16.628281Z",
          "shell.execute_reply": "2021-06-08T01:34:26.013775Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20e483c4-61d6-4b7c-a658-3f332e3bd401"
      },
      "source": [
        "!pip install pyarabic\n",
        "!git clone https://github.com/moaaztaha/Arabic-English-Translation-Transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyarabic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
            "\r\u001b[K     |███                             | 10kB 23.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 16.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 40kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 51kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 71kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 92kB 8.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 7.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
            "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp37-none-any.whl size=113324 sha256=90a1550a50d3227ed57ed1c99ad8b6d502012f0ca85a5737116cc67f95fadf31\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
            "Successfully built pyarabic\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.10\n",
            "Cloning into 'Arabic-English-Translation-Transformers'...\n",
            "remote: Enumerating objects: 81, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 81 (delta 30), reused 69 (delta 18), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (81/81), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9e97fa1",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:47.414915Z",
          "iopub.execute_input": "2021-06-08T02:16:47.415253Z",
          "iopub.status.idle": "2021-06-08T02:16:47.421088Z",
          "shell.execute_reply.started": "2021-06-08T02:16:47.415225Z",
          "shell.execute_reply": "2021-06-08T02:16:47.419702Z"
        },
        "trusted": true
      },
      "source": [
        "# modules\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import callbacks\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gensim\n",
        "import numpy as np\n",
        "\n",
        "import pyarabic.araby as araby\n",
        "from pyarabic.araby import strip_tashkeel, strip_tatweel"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1710411f"
      },
      "source": [
        "### Data Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RNDMX9cizGW",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:47.955355Z",
          "iopub.execute_input": "2021-06-08T02:16:47.955686Z",
          "iopub.status.idle": "2021-06-08T02:16:48.136995Z",
          "shell.execute_reply.started": "2021-06-08T02:16:47.955644Z",
          "shell.execute_reply": "2021-06-08T02:16:48.136060Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc39df46-9482-4b26-b0ce-2281a249a3af"
      },
      "source": [
        "ar = pd.read_table('/content/Arabic-English-Translation-Transformers/ArabicNewData.txt', delimiter='\\\\n', names=['ar'])\n",
        "en = pd.read_table('/content/Arabic-English-Translation-Transformers/EnglishNewData.txt', delimiter='\\\\n', names=['en'])\n",
        "\n",
        "en['ar'] = ar['ar']\n",
        "df = en.copy()\n",
        "df = df.iloc[:35118]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return read_csv(**locals())\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yRyviFk6Ayg"
      },
      "source": [
        "### Arabic Preprocessing\n",
        "- removing tashkeel\n",
        "- removing tatweel\n",
        "- normalize hamza \n",
        "- split 'ال'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2YTqqOTGl5c",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:48.455116Z",
          "iopub.execute_input": "2021-06-08T02:16:48.455428Z",
          "iopub.status.idle": "2021-06-08T02:16:48.461638Z",
          "shell.execute_reply.started": "2021-06-08T02:16:48.455402Z",
          "shell.execute_reply": "2021-06-08T02:16:48.460738Z"
        },
        "trusted": true
      },
      "source": [
        "morphs = [strip_tashkeel, strip_tatweel]\n",
        "\n",
        "def fix_ar(sent):\n",
        "  sent = split_al_sent(sent)\n",
        "  tokens = araby.tokenize(sent, morphs=morphs)\n",
        "  sent = araby.normalize_hamza(' '.join(tokens), method='tasheel')\n",
        "  return sent"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WhmXTv1KRkd",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:48.992161Z",
          "iopub.execute_input": "2021-06-08T02:16:48.992476Z",
          "iopub.status.idle": "2021-06-08T02:16:48.998243Z",
          "shell.execute_reply.started": "2021-06-08T02:16:48.992448Z",
          "shell.execute_reply": "2021-06-08T02:16:48.997288Z"
        },
        "trusted": true
      },
      "source": [
        "def split_al(word):\n",
        "    if word.startswith('ال'):\n",
        "        return word[:2], word[2:]\n",
        "    else: \n",
        "        return word\n",
        "\n",
        "def split_al_sent(sent):\n",
        "    ww = []\n",
        "    for word in sent.split():\n",
        "        out = split_al(word)\n",
        "        if type(out) is tuple:\n",
        "            for w in out:\n",
        "                ww.append(w)\n",
        "        else:\n",
        "            ww.append(word)\n",
        "    return ' '.join(w for w in ww)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGuKlu3mFIZc",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:50.345045Z",
          "iopub.execute_input": "2021-06-08T02:16:50.345366Z",
          "iopub.status.idle": "2021-06-08T02:16:51.796892Z",
          "shell.execute_reply.started": "2021-06-08T02:16:50.345330Z",
          "shell.execute_reply": "2021-06-08T02:16:51.795902Z"
        },
        "trusted": true
      },
      "source": [
        "df['ar'] = df.apply(lambda row: fix_ar(row.ar), axis=1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be905b2c",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:51.799257Z",
          "iopub.execute_input": "2021-06-08T02:16:51.799840Z",
          "iopub.status.idle": "2021-06-08T02:16:54.618551Z",
          "shell.execute_reply.started": "2021-06-08T02:16:51.799800Z",
          "shell.execute_reply": "2021-06-08T02:16:54.617702Z"
        },
        "trusted": true
      },
      "source": [
        "# getting text pairs\n",
        "text_pairs = []\n",
        "for idx, row in df.iterrows():\n",
        "  en, ar = row['en'], row['ar']\n",
        "  ar = \"[start] \" + ar + \" [end]\"\n",
        "  text_pairs.append((en, ar))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAxNAdK2H0eB",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:16:58.012634Z",
          "iopub.execute_input": "2021-06-08T02:16:58.013120Z",
          "iopub.status.idle": "2021-06-08T02:17:00.598315Z",
          "shell.execute_reply.started": "2021-06-08T02:16:58.013084Z",
          "shell.execute_reply": "2021-06-08T02:17:00.597570Z"
        },
        "trusted": true
      },
      "source": [
        "for idx, row in df.iterrows():\n",
        "  if len(row.ar.split()) < 1:\n",
        "    print(row.ar, '\\n*')\n",
        "    print(row.en)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "296e4594",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:00.600320Z",
          "iopub.execute_input": "2021-06-08T02:17:00.600692Z",
          "iopub.status.idle": "2021-06-08T02:17:00.605892Z",
          "shell.execute_reply.started": "2021-06-08T02:17:00.600644Z",
          "shell.execute_reply": "2021-06-08T02:17:00.605074Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261fba89-a9a7-4487-a1dc-bc33ad9434b5"
      },
      "source": [
        "for _ in range(2):\n",
        "    print(random.choice(text_pairs))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(\"Chief Secretary Kim hasn't given me any files or information on the architect yet.\", '[start] انا لم احصل علي ال ملفات من . ال سكرتير كيم بعد [end]')\n",
            "('Excellent.', '[start] ممتاز [end]')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6kRoro9iBOe",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:00.607746Z",
          "iopub.execute_input": "2021-06-08T02:17:00.608167Z",
          "iopub.status.idle": "2021-06-08T02:17:00.620306Z",
          "shell.execute_reply.started": "2021-06-08T02:17:00.608132Z",
          "shell.execute_reply": "2021-06-08T02:17:00.619437Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7e4d17-dcc3-40d4-a09d-7f540ce6ae55"
      },
      "source": [
        "len(text_pairs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35118"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1283f6f",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:01.800249Z",
          "iopub.execute_input": "2021-06-08T02:17:01.800560Z",
          "iopub.status.idle": "2021-06-08T02:17:01.847161Z",
          "shell.execute_reply.started": "2021-06-08T02:17:01.800532Z",
          "shell.execute_reply": "2021-06-08T02:17:01.846417Z"
        },
        "trusted": true
      },
      "source": [
        "# spliting data into train and validate\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) -  num_val_samples\n",
        "train_pairs = text_pairs[: num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7a1f8cd",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:02.645892Z",
          "iopub.execute_input": "2021-06-08T02:17:02.646210Z",
          "iopub.status.idle": "2021-06-08T02:17:02.654338Z",
          "shell.execute_reply.started": "2021-06-08T02:17:02.646181Z",
          "shell.execute_reply": "2021-06-08T02:17:02.653626Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ef7d0d-383e-4f95-f0fa-0e89c2f3e7f2"
      },
      "source": [
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35118 total pairs\n",
            "29851 training pairs\n",
            "5267 validation pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5f6d84e"
      },
      "source": [
        "#### Vectorizing the text data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487ed66f",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:03.954239Z",
          "iopub.execute_input": "2021-06-08T02:17:03.954544Z",
          "iopub.status.idle": "2021-06-08T02:17:04.856227Z",
          "shell.execute_reply.started": "2021-06-08T02:17:03.954516Z",
          "shell.execute_reply": "2021-06-08T02:17:04.854499Z"
        },
        "trusted": true
      },
      "source": [
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "vocab_size = 20000\n",
        "sequence_length = 50\n",
        "batch_size = 265\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    return tf.strings.regex_replace(input_string, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "            # max_tokens=vocab_size, \n",
        "            output_mode='int', \n",
        "            output_sequence_length=sequence_length)\n",
        "\n",
        "ar_vectorization = TextVectorization(\n",
        "    # max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size)\n",
        "\n",
        "eng_texts = [pair[0] for pair in text_pairs]\n",
        "ar_texts = [pair[1] for pair in text_pairs]\n",
        "eng_vectorization.adapt(eng_texts)\n",
        "ar_vectorization.adapt(ar_texts)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubW5QOJtLzMI",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:04.859040Z",
          "iopub.execute_input": "2021-06-08T02:17:04.859565Z",
          "iopub.status.idle": "2021-06-08T02:17:04.938344Z",
          "shell.execute_reply.started": "2021-06-08T02:17:04.859525Z",
          "shell.execute_reply": "2021-06-08T02:17:04.937688Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9400e78-aa43-4a0a-beb3-25bdf381153a"
      },
      "source": [
        "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 13164)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wep-jkjyLWz6",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:04.939783Z",
          "iopub.execute_input": "2021-06-08T02:17:04.940155Z",
          "iopub.status.idle": "2021-06-08T02:17:05.011110Z",
          "shell.execute_reply.started": "2021-06-08T02:17:04.940116Z",
          "shell.execute_reply": "2021-06-08T02:17:05.010101Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "370a11d3-e826-4eb1-fe15-80eed0246646"
      },
      "source": [
        "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 13164)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5qiqjOWdVz1",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:05.013252Z",
          "iopub.execute_input": "2021-06-08T02:17:05.013639Z",
          "iopub.status.idle": "2021-06-08T02:17:05.021566Z",
          "shell.execute_reply.started": "2021-06-08T02:17:05.013599Z",
          "shell.execute_reply": "2021-06-08T02:17:05.020566Z"
        },
        "trusted": true
      },
      "source": [
        "# making the dataset\n",
        "def format_dataset(eng, ar):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ar = ar_vectorization(ar)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ar_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ar_texts = list(ar_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a3ed140",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:05.596886Z",
          "iopub.execute_input": "2021-06-08T02:17:05.597209Z",
          "iopub.status.idle": "2021-06-08T02:17:06.245999Z",
          "shell.execute_reply.started": "2021-06-08T02:17:05.597181Z",
          "shell.execute_reply": "2021-06-08T02:17:06.245218Z"
        },
        "trusted": true
      },
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3902789f",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:06.247956Z",
          "iopub.execute_input": "2021-06-08T02:17:06.248284Z",
          "iopub.status.idle": "2021-06-08T02:17:06.519611Z",
          "shell.execute_reply.started": "2021-06-08T02:17:06.248250Z",
          "shell.execute_reply": "2021-06-08T02:17:06.518592Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202eaf86-769f-4c73-874a-5135f2a2427a"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (265, 50)\n",
            "inputs[\"decoder_inputs\"].shape: (265, 50)\n",
            "targets.shape: (265, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fa0936c"
      },
      "source": [
        "### Building the Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3baf452",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:17:07.222610Z",
          "iopub.execute_input": "2021-06-08T02:17:07.222939Z",
          "iopub.status.idle": "2021-06-08T02:17:07.247405Z",
          "shell.execute_reply.started": "2021-06-08T02:17:07.222907Z",
          "shell.execute_reply": "2021-06-08T02:17:07.246264Z"
        },
        "trusted": true
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "          'embed_dim': self.embed_dim,\n",
        "          'dense_dim': self.dense_dim,\n",
        "          'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, pretrained=False, weights=False, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        if not pretrained:\n",
        "          self.token_embeddings = layers.Embedding(\n",
        "              input_dim=vocab_size, output_dim=embed_dim\n",
        "          )\n",
        "        else:\n",
        "          # pre-trained\n",
        "          self.token_embeddings = layers.Embedding(\n",
        "              input_dim=vocab_size, output_dim=embed_dim, weights=[weights]\n",
        "          ) \n",
        "\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "      \n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'sequence_length': self.sequence_length,\n",
        "      'vocab_size': self.vocab_size,\n",
        "      'embed_dim': self.embed_dim,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'embed_dim': self.embed_dim,\n",
        "      'latent_dim': self.latent_dim,\n",
        "      'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0rA0C4s1OVW",
        "execution": {
          "iopub.status.busy": "2021-06-08T03:47:59.830643Z",
          "iopub.execute_input": "2021-06-08T03:47:59.830984Z",
          "iopub.status.idle": "2021-06-08T03:47:59.906741Z",
          "shell.execute_reply.started": "2021-06-08T03:47:59.830952Z",
          "shell.execute_reply": "2021-06-08T03:47:59.905831Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720110c7-24dc-4107-af69-81fa079827bc"
      },
      "source": [
        "ar_vocab_size = len(ar_vectorization.get_vocabulary())\n",
        "en_vocab_size = len(eng_vectorization.get_vocabulary())\n",
        "en_vocab_size, ar_vocab_size"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13164, 20000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c57e1e",
        "execution": {
          "iopub.status.busy": "2021-06-08T03:49:47.423352Z",
          "iopub.execute_input": "2021-06-08T03:49:47.423713Z",
          "iopub.status.idle": "2021-06-08T03:49:48.045217Z",
          "shell.execute_reply.started": "2021-06-08T03:49:47.423679Z",
          "shell.execute_reply": "2021-06-08T03:49:48.044246Z"
        },
        "trusted": true
      },
      "source": [
        "# build the model\n",
        "embed_dim = 300\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "#x = PositionalEmbedding(sequence_length, en_vocab_size, embed_dim, pretrained=True, weights=english_embeddings)(encoder_inputs)\n",
        "x = PositionalEmbedding(sequence_length, en_vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "#x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim, pretrained=True, weights=arabic_embeddings)(decoder_inputs)\n",
        "x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(ar_vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt9sp6Yrb4pE",
        "execution": {
          "iopub.status.busy": "2021-06-08T03:49:49.471793Z",
          "iopub.execute_input": "2021-06-08T03:49:49.472124Z",
          "iopub.status.idle": "2021-06-08T03:49:49.477189Z",
          "shell.execute_reply.started": "2021-06-08T03:49:49.472094Z",
          "shell.execute_reply": "2021-06-08T03:49:49.476150Z"
        },
        "trusted": true
      },
      "source": [
        "googledrive_path = '/content/drive/MyDrive/Transformers/final_vanilla/'"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhuZU27y7DRl"
      },
      "source": [
        "### Callbacks\n",
        "- Early Stopping \n",
        "- Saving weights \n",
        "- Learning Rate Schedular \n",
        "- Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqmaEQ94kyB9",
        "execution": {
          "iopub.status.busy": "2021-06-08T03:49:54.155540Z",
          "iopub.execute_input": "2021-06-08T03:49:54.155939Z",
          "iopub.status.idle": "2021-06-08T03:49:54.164631Z",
          "shell.execute_reply.started": "2021-06-08T03:49:54.155904Z",
          "shell.execute_reply": "2021-06-08T03:49:54.163841Z"
        },
        "trusted": true
      },
      "source": [
        "early_stopping_cb = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=True)\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(googledrive_path+'/weights_adam.ckpt', monitor='val_accuracy', save_weights_only=True,verbose=True, save_best_only=True)\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=googledrive_path+\"/logs\")\n",
        "lr_schr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=True, factor=0.3, min_lr=0.0001)\n",
        "cbs = [early_stopping_cb, checkpoint_cb, tensorboard_callback, lr_schr]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-08T03:50:03.472687Z",
          "iopub.execute_input": "2021-06-08T03:50:03.473046Z",
          "iopub.status.idle": "2021-06-08T04:02:41.112268Z",
          "shell.execute_reply.started": "2021-06-08T03:50:03.473016Z",
          "shell.execute_reply": "2021-06-08T04:02:41.111344Z"
        },
        "trusted": true,
        "id": "oe_QsoxGhAL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c763947d-5406-4779-b1ea-272ec8de17ff"
      },
      "source": [
        "epochs = 100  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding_2 (Positio (None, None, 300)    3964200     encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_1 (Transfor (None, None, 300)    4119848     positional_embedding_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "model_3 (Functional)            (None, None, 20000)  19042948    decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder_1[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 27,126,996\n",
            "Trainable params: 27,126,996\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "113/113 [==============================] - 84s 717ms/step - loss: 0.9298 - accuracy: 0.2944 - val_loss: 0.8011 - val_accuracy: 0.3461\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.34611, saving model to /content/drive/MyDrive/Transformers/final_vanilla/weights_adam.ckpt\n",
            "Epoch 2/100\n",
            "113/113 [==============================] - 79s 703ms/step - loss: 0.7596 - accuracy: 0.3697 - val_loss: 0.7486 - val_accuracy: 0.3740\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.34611 to 0.37396, saving model to /content/drive/MyDrive/Transformers/final_vanilla/weights_adam.ckpt\n",
            "Epoch 3/100\n",
            "113/113 [==============================] - 80s 706ms/step - loss: 0.6769 - accuracy: 0.4080 - val_loss: 0.7306 - val_accuracy: 0.3869\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.37396 to 0.38690, saving model to /content/drive/MyDrive/Transformers/final_vanilla/weights_adam.ckpt\n",
            "Epoch 4/100\n",
            "113/113 [==============================] - 79s 703ms/step - loss: 0.6083 - accuracy: 0.4380 - val_loss: 0.7306 - val_accuracy: 0.3923\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.38690 to 0.39233, saving model to /content/drive/MyDrive/Transformers/final_vanilla/weights_adam.ckpt\n",
            "Epoch 5/100\n",
            "113/113 [==============================] - 80s 706ms/step - loss: 0.5474 - accuracy: 0.4646 - val_loss: 0.7270 - val_accuracy: 0.3873\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.39233\n",
            "Epoch 6/100\n",
            "113/113 [==============================] - 79s 703ms/step - loss: 0.4916 - accuracy: 0.4942 - val_loss: 0.7329 - val_accuracy: 0.3972\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.39233 to 0.39718, saving model to /content/drive/MyDrive/Transformers/final_vanilla/weights_adam.ckpt\n",
            "Epoch 7/100\n",
            "113/113 [==============================] - 79s 703ms/step - loss: 0.4374 - accuracy: 0.5259 - val_loss: 0.7393 - val_accuracy: 0.4010\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.39718 to 0.40102, saving model to /content/drive/MyDrive/Transformers/final_vanilla/weights_adam.ckpt\n",
            "Epoch 8/100\n",
            "113/113 [==============================] - 80s 705ms/step - loss: 0.3896 - accuracy: 0.5559 - val_loss: 0.7499 - val_accuracy: 0.3919\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.40102\n",
            "Epoch 9/100\n",
            "113/113 [==============================] - 80s 704ms/step - loss: 0.3450 - accuracy: 0.5896 - val_loss: 0.7619 - val_accuracy: 0.3940\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.40102\n",
            "\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "Epoch 10/100\n",
            "113/113 [==============================] - 79s 703ms/step - loss: 0.2676 - accuracy: 0.6666 - val_loss: 0.7596 - val_accuracy: 0.4099\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.40102 to 0.40988, saving model to /content/drive/MyDrive/Transformers/final_vanilla/weights_adam.ckpt\n",
            "Epoch 11/100\n",
            "113/113 [==============================] - 79s 703ms/step - loss: 0.2225 - accuracy: 0.7195 - val_loss: 0.7777 - val_accuracy: 0.4075\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.40988\n",
            "Epoch 12/100\n",
            "113/113 [==============================] - 80s 704ms/step - loss: 0.1932 - accuracy: 0.7548 - val_loss: 0.7962 - val_accuracy: 0.4060\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.40988\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 13/100\n",
            "113/113 [==============================] - 79s 703ms/step - loss: 0.1673 - accuracy: 0.7860 - val_loss: 0.8032 - val_accuracy: 0.4051\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.40988\n",
            "Epoch 14/100\n",
            "113/113 [==============================] - 80s 706ms/step - loss: 0.1556 - accuracy: 0.8012 - val_loss: 0.8126 - val_accuracy: 0.4044\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.40988\n",
            "Epoch 15/100\n",
            "113/113 [==============================] - 79s 704ms/step - loss: 0.1467 - accuracy: 0.8127 - val_loss: 0.8220 - val_accuracy: 0.4030\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.40988\n",
            "Epoch 00015: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc32390f8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Qvebl8FHsG",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:39:58.377008Z",
          "iopub.execute_input": "2021-06-08T02:39:58.377328Z",
          "iopub.status.idle": "2021-06-08T02:39:58.713799Z",
          "shell.execute_reply.started": "2021-06-08T02:39:58.377296Z",
          "shell.execute_reply": "2021-06-08T02:39:58.712839Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270e04b3-8353-47b8-c090-e419db9886d3"
      },
      "source": [
        "# Loading the latest checkpoint\n",
        "latest = tf.train.latest_checkpoint(googledrive_path)\n",
        "transformer.load_weights(latest)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc33e1d80d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8447490b",
        "execution": {
          "iopub.status.busy": "2021-06-08T04:04:11.583186Z",
          "iopub.execute_input": "2021-06-08T04:04:11.583522Z",
          "iopub.status.idle": "2021-06-08T04:04:17.842227Z",
          "shell.execute_reply.started": "2021-06-08T04:04:11.583490Z",
          "shell.execute_reply": "2021-06-08T04:04:17.841483Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb50c969-3b72-44cd-9770-beb878e3c031"
      },
      "source": [
        "ar_vocab = ar_vectorization.get_vocabulary()\n",
        "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
        "max_decoded_sentence_length = sequence_length\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in val_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts[:30])\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, '\\n', translated)\n",
        "    print('*'*50)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Who called them? \n",
            " [start] من هم ؟ [end]\n",
            "**************************************************\n",
            "You can't run into the rainy season when you're trying to make an outdoor picture. \n",
            " [start] انت لا تستطيع ان [UNK] [UNK] في موسم انت [UNK] الى ال مجرم [end]\n",
            "**************************************************\n",
            "The village of Aceitunilla is in one of the poorest valleys. \n",
            " [start] ان ال جمهور والتصفيق كله لى فى احد [end]\n",
            "**************************************************\n",
            "You can't run into the rainy season when you're trying to make an outdoor picture. \n",
            " [start] انت لا تستطيع ان [UNK] [UNK] في موسم انت [UNK] الى ال مجرم [end]\n",
            "**************************************************\n",
            "Who's the person that used to deliver milk to that house? \n",
            " [start] من ال شخص ال ذي كان يهتم به ؟ [end]\n",
            "**************************************************\n",
            "Though not that one. \n",
            " [start] هذا ليس بالامر [end]\n",
            "**************************************************\n",
            "- Southwest? Well, there is nothing for thousands of miles. \n",
            " [start] قوة من ال رجال لا يوجد خمسة دولارات على ال عديد من ال ايام [end]\n",
            "**************************************************\n",
            "-Now take him out of here. -Come on. \n",
            " [start] هيا تحركوا [end]\n",
            "**************************************************\n",
            "This monster you've seen, you think your bombs will stop him? \n",
            " [start] هذا ال ذي [UNK] فيه ال كفاية هو ال قنابل [end]\n",
            "**************************************************\n",
            "Is this far enough? \n",
            " [start] اهذا صحيح ؟ [end]\n",
            "**************************************************\n",
            "Rick, please. \n",
            " [start] Rick please [end]\n",
            "**************************************************\n",
            "- Southwest? Well, there is nothing for thousands of miles. \n",
            " [start] قوة من ال رجال لا يوجد خمسة دولارات على ال عديد من ال ايام [end]\n",
            "**************************************************\n",
            "I Don't Care. \n",
            " [start] لا اهتم [end]\n",
            "**************************************************\n",
            "with your kind attention and permission.. \n",
            " [start] بانتباهك ال رحيم ورخصة [end]\n",
            "**************************************************\n",
            "Coffee. \n",
            " [start] قهوة ؟ [end]\n",
            "**************************************************\n",
            "Though not that one. \n",
            " [start] هذا ليس بالامر [end]\n",
            "**************************************************\n",
            "Chief Secretary Kim is asking me for some time off... that's the first time in 15 years you've made that request. \n",
            " [start] ، ايها ال سكرتير كيم [UNK] ، هل هذا ال وقت من ال ذي جعلني انت [UNK] فيه ، صحيح ؟ [end]\n",
            "**************************************************\n",
            "Who's the person that used to deliver milk to that house? \n",
            " [start] من ال شخص ال ذي كان يهتم به ؟ [end]\n",
            "**************************************************\n",
            "And fortunately, your life didn't end then... you were able to find a way to continue on with your life the way you're meant to. \n",
            " [start] و [UNK] [UNK] لي الا يمكنك ان ال مطاف اعيش بها بسبب ال طريق الى جانبه الى ال اخر [end]\n",
            "**************************************************\n",
            "The village of Aceitunilla is in one of the poorest valleys. \n",
            " [start] ان ال جمهور والتصفيق كله لى فى احد [end]\n",
            "**************************************************\n",
            "Can I stay here tonight? \n",
            " [start] هل استطيع ال مكوث هنا ؟ [end]\n",
            "**************************************************\n",
            "I'm telling you, she is with your father ― ! \n",
            " [start] انا اخبرتك بمن جونج شيل [end]\n",
            "**************************************************\n",
            "Yes, but keep that thought in your heart, Jacques. \n",
            " [start] نعم لكن ضعي هذا في قلب ال قلب ال قلب [end]\n",
            "**************************************************\n",
            "- Southwest? Well, there is nothing for thousands of miles. \n",
            " [start] قوة من ال رجال لا يوجد خمسة دولارات على ال عديد من ال ايام [end]\n",
            "**************************************************\n",
            "Try it. - I don't want to try it! \n",
            " [start] اريد ان افعل هذا [end]\n",
            "**************************************************\n",
            "Now, which of these beds do you prefer? \n",
            " [start] ال ان ، ال ذي تعني ب [UNK] ما تفكر به ؟ [end]\n",
            "**************************************************\n",
            "Can I stay here tonight? \n",
            " [start] هل استطيع ال مكوث هنا ؟ [end]\n",
            "**************************************************\n",
            "Chief Secretary Kim is asking me for some time off... that's the first time in 15 years you've made that request. \n",
            " [start] ، ايها ال سكرتير كيم [UNK] ، هل هذا ال وقت من ال ذي جعلني انت [UNK] فيه ، صحيح ؟ [end]\n",
            "**************************************************\n",
            "The boy. You don't dig him, do you? \n",
            " [start] ال فتى ال ى ال صغير هل تعتقد انه [UNK] ؟ [end]\n",
            "**************************************************\n",
            "The Western and Atlantic Flyer speeding into Marrietta, Ga., in the Spring 1861. \n",
            " [start] ال جزيرة استوايية و ال اطلسي في ال معيشة في ال هايل من ال سماء [end]\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2i4SbVoP_Yz",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:40:58.148840Z",
          "iopub.execute_input": "2021-06-08T02:40:58.149218Z",
          "iopub.status.idle": "2021-06-08T02:40:58.155254Z",
          "shell.execute_reply.started": "2021-06-08T02:40:58.149177Z",
          "shell.execute_reply": "2021-06-08T02:40:58.154292Z"
        },
        "trusted": true
      },
      "source": [
        "def get_bleu():\n",
        "  \n",
        "  preds, src = [], []\n",
        "\n",
        "  with tqdm(total=len(val_pairs), position=0, leave=True) as pbar:\n",
        "    for en_sent, ar_sent in tqdm(val_pairs, position=0, leave=True):\n",
        "      translated = decode_sequence(en_sent)\n",
        "      preds.append(translated.split())\n",
        "      src.append(ar_sent.split())\n",
        "      pbar.update()\n",
        "\n",
        "    return src, preds\n",
        "    # print_scores(src, preds)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9uh1623AwUW",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:40:59.410199Z",
          "iopub.execute_input": "2021-06-08T02:40:59.410504Z",
          "iopub.status.idle": "2021-06-08T02:40:59.415944Z",
          "shell.execute_reply.started": "2021-06-08T02:40:59.410474Z",
          "shell.execute_reply": "2021-06-08T02:40:59.414990Z"
        },
        "trusted": true
      },
      "source": [
        "# !!pip install Rouge\n",
        "from rouge import Rouge\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "def print_scores(trgs, preds):\n",
        "    print('----- Bleu-n Scores -----')\n",
        "    print(\"1:\", corpus_bleu(trgs, preds, weights=[1.0/1.0])*100)\n",
        "    print(\"2:\", corpus_bleu(trgs, preds, weights=[1.0/2.0, 1.0/2.0])*100)\n",
        "    print(\"3:\", corpus_bleu(trgs, preds, weights=[1.0/3.0, 1.0/3.0, 1.0/3.0])*100)\n",
        "    print(\"4:\", corpus_bleu(trgs, preds)*100)\n",
        "    print('-'*25)\n",
        "    print('----- Rouge Scores -----')\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores([\" \".join(i) for i in preds ], [\" \".join(i) for i in src ], avg=True)\n",
        "    for key, item in scores.items():\n",
        "      print(key,':',item)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaXlIeHdRGqi",
        "execution": {
          "iopub.status.busy": "2021-06-08T02:41:00.631949Z",
          "iopub.execute_input": "2021-06-08T02:41:00.632276Z",
          "iopub.status.idle": "2021-06-08T02:59:20.928004Z",
          "shell.execute_reply.started": "2021-06-08T02:41:00.632247Z",
          "shell.execute_reply": "2021-06-08T02:59:20.925932Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d29abeb4-e571-427e-f790-00d03f9d5012"
      },
      "source": [
        "src, preds = get_bleu()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5267/5267 [19:34<00:00,  4.48it/s]\n",
            "100%|██████████| 5267/5267 [19:34<00:00,  4.48it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-08T02:59:20.936277Z",
          "iopub.execute_input": "2021-06-08T02:59:20.936757Z",
          "iopub.status.idle": "2021-06-08T03:00:10.585162Z",
          "shell.execute_reply.started": "2021-06-08T02:59:20.936723Z",
          "shell.execute_reply": "2021-06-08T03:00:10.583379Z"
        },
        "trusted": true,
        "id": "-QGhrnethAL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33599c5e-e2e6-4a9e-b708-3d75bdf2afbd"
      },
      "source": [
        "print_scores(src, preds)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- Bleu-n Scores -----\n",
            "1: 2.788899627226287\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2: 16.699998883911\n",
            "3: 30.325708872140204\n",
            "4: 40.86563211784568\n",
            "-------------------------\n",
            "----- Rouge Scores -----\n",
            "rouge-1 : {'f': 0.48335406725543123, 'p': 0.5094369421223762, 'r': 0.4848706965425694}\n",
            "rouge-2 : {'f': 0.15959647282114406, 'p': 0.1671309138689211, 'r': 0.15888596579843062}\n",
            "rouge-l : {'f': 0.4879406998746469, 'p': 0.5158916953942765, 'r': 0.4849411907950849}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}