{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0R53kjrYGpSc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0R53kjrYGpSc",
    "outputId": "25e29470-6a5b-4cb8-8fba-6943fd0ca93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarabic\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
      "\r",
      "\u001b[K     |███                             | 10kB 20.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 20kB 27.4MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 30kB 20.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 40kB 24.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 51kB 23.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 61kB 25.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 71kB 21.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 81kB 22.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 92kB 24.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 102kB 23.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 112kB 23.0MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
      "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp37-none-any.whl size=113324 sha256=515995db1a871f38f3902fd74098ba9ca3c0810f522c69b3f2599799d4351f73\n",
      "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
      "Successfully built pyarabic\n",
      "Installing collected packages: pyarabic\n",
      "Successfully installed pyarabic-0.6.10\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarabic\n",
    "# !git clone https://github.com/moaaztaha/Arabic-English-Translation-Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e9e97fa1",
   "metadata": {
    "id": "e9e97fa1"
   },
   "outputs": [],
   "source": [
    "# modules\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1710411f",
   "metadata": {
    "id": "1710411f"
   },
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "1RNDMX9cizGW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RNDMX9cizGW",
    "outputId": "e7f0bb9a-8761-44a8-c1cc-3f0331ca3fe3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return read_csv(**locals())\n"
     ]
    }
   ],
   "source": [
    "ar = pd.read_table('/content/drive/MyDrive/ArabicNewData.txt', delimiter='\\\\n', names=['ar'])\n",
    "en = pd.read_table('/content/drive/MyDrive/EnglishNewData.txt', delimiter='\\\\n', names=['en'])\n",
    "\n",
    "en['ar'] = ar['ar']\n",
    "df = en.copy()\n",
    "df = df.iloc[:35118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "T2YTqqOTGl5c",
   "metadata": {
    "id": "T2YTqqOTGl5c"
   },
   "outputs": [],
   "source": [
    "morphs = [strip_tashkeel, strip_tatweel]\n",
    "\n",
    "def fix_ar(sent):\n",
    "  sent = split_al_sent(sent)\n",
    "  tokens = araby.tokenize(sent, morphs=morphs)\n",
    "  sent = araby.normalize_hamza(' '.join(tokens), method='tasheel')\n",
    "  return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "7WhmXTv1KRkd",
   "metadata": {
    "id": "7WhmXTv1KRkd"
   },
   "outputs": [],
   "source": [
    "def split_al(word):\n",
    "    if word.startswith('ال'):\n",
    "        return word[:2], word[2:]\n",
    "    else: \n",
    "        return word\n",
    "\n",
    "def split_al_sent(sent):\n",
    "    ww = []\n",
    "    for word in sent.split():\n",
    "        out = split_al(word)\n",
    "        if type(out) is tuple:\n",
    "            for w in out:\n",
    "                ww.append(w)\n",
    "        else:\n",
    "            ww.append(word)\n",
    "    return ' '.join(w for w in ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "QGuKlu3mFIZc",
   "metadata": {
    "id": "QGuKlu3mFIZc"
   },
   "outputs": [],
   "source": [
    "df['ar'] = df.apply(lambda row: fix_ar(row.ar), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "be905b2c",
   "metadata": {
    "id": "be905b2c"
   },
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "for idx, row in df.iterrows():\n",
    "  en, ar = row['en'], row['ar']\n",
    "  ar = \"[start] \" + ar + \" [end]\"\n",
    "  text_pairs.append((en, ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fAxNAdK2H0eB",
   "metadata": {
    "id": "fAxNAdK2H0eB"
   },
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "  if len(row.ar.split()) < 1:\n",
    "    print(row.ar, '\\n*')\n",
    "    print(row.en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "296e4594",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "296e4594",
    "outputId": "cf5ca9a6-e5ae-45ff-9b35-1dac88910f32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I understand that you were stuck in a jam.', '[start] افهم انه يفترض ان تبدي حزينة [end]')\n",
      "(\"S-so, that's it? We're...\", '[start] اذن ، هذا كل شيء ، نحن ... [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "o6kRoro9iBOe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6kRoro9iBOe",
    "outputId": "1ec02783-32ee-45c0-bef7-e64dfb28b8c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35118"
      ]
     },
     "execution_count": 293,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "c1283f6f",
   "metadata": {
    "id": "c1283f6f"
   },
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) -  num_val_samples\n",
    "train_pairs = text_pairs[: num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a7a1f8cd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7a1f8cd",
    "outputId": "2ff87303-7ba6-43a8-f9f1-fdc7db9a7f56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35118 total pairs\n",
      "29851 training pairs\n",
      "5267 validation pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6d84e",
   "metadata": {
    "id": "d5f6d84e"
   },
   "source": [
    "#### Vectorizing the text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "487ed66f",
   "metadata": {
    "id": "487ed66f"
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "vocab_size = 20000\n",
    "sequence_length = 50\n",
    "batch_size = 265\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    return tf.strings.regex_replace(input_string, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "            # max_tokens=vocab_size, \n",
    "            output_mode='int', \n",
    "            output_sequence_length=sequence_length)\n",
    "\n",
    "ar_vectorization = TextVectorization(\n",
    "    # max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size)\n",
    "\n",
    "eng_texts = [pair[0] for pair in text_pairs]\n",
    "ar_texts = [pair[1] for pair in text_pairs]\n",
    "eng_vectorization.adapt(eng_texts)\n",
    "ar_vectorization.adapt(ar_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ubW5QOJtLzMI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubW5QOJtLzMI",
    "outputId": "255cd7fe-f248-4210-8572-21cafce65778"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 13164)"
      ]
     },
     "execution_count": 301,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "wep-jkjyLWz6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wep-jkjyLWz6",
    "outputId": "fa3d480c-3075-4d24-84c8-25324524b0fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 13164)"
      ]
     },
     "execution_count": 302,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "V5qiqjOWdVz1",
   "metadata": {
    "id": "V5qiqjOWdVz1"
   },
   "outputs": [],
   "source": [
    "def format_dataset(eng, ar):\n",
    "    eng = eng_vectorization(eng)\n",
    "    ar = ar_vectorization(ar)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, ar_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    ar_texts = list(ar_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7a3ed140",
   "metadata": {
    "id": "7a3ed140"
   },
   "outputs": [],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "3902789f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3902789f",
    "outputId": "b85367db-eb0f-4bb3-9cf7-f2cf54ee12e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (265, 50)\n",
      "inputs[\"decoder_inputs\"].shape: (265, 50)\n",
      "targets.shape: (265, 50)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0936c",
   "metadata": {
    "id": "4fa0936c"
   },
   "source": [
    "### Building the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "b3baf452",
   "metadata": {
    "id": "b3baf452"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "          'embed_dim': self.embed_dim,\n",
    "          'dense_dim': self.dense_dim,\n",
    "          'num_heads': self.num_heads,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, pretrained=False, weights=False, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        if not pretrained:\n",
    "          self.token_embeddings = layers.Embedding(\n",
    "              input_dim=vocab_size, output_dim=embed_dim\n",
    "          )\n",
    "        else:\n",
    "          # pre-trained\n",
    "          self.token_embeddings = layers.Embedding(\n",
    "              input_dim=vocab_size, output_dim=embed_dim, weights=[weights]\n",
    "          ) \n",
    "\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "      \n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "      'sequence_length': self.sequence_length,\n",
    "      'vocab_size': self.vocab_size,\n",
    "      'embed_dim': self.embed_dim,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "      'embed_dim': self.embed_dim,\n",
    "      'latent_dim': self.latent_dim,\n",
    "      'num_heads': self.num_heads,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fpXVu3eA_b9P",
   "metadata": {
    "id": "fpXVu3eA_b9P"
   },
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afjYy3g6BM2K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afjYy3g6BM2K",
    "outputId": "e32988e0-c411-403e-c96b-f847eb6a7677"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \"/content/glove.6B.300d.txt\"\n",
    ")\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "XRNRUzOCtccc",
   "metadata": {
    "id": "XRNRUzOCtccc"
   },
   "outputs": [],
   "source": [
    "vocab = eng_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocab, range(len(vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "o_rOhjwPuqkN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_rOhjwPuqkN",
    "outputId": "faf29a9f-e1c0-4ece-a1a3-9b779d31151b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 11896 words (1268 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(vocab)\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.uniform(-.1, .1, size=(embedding_dim))\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "XWvsoAzszKiS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWvsoAzszKiS",
    "outputId": "76a8d628-1e0f-4240-a60f-c3f3e970ba2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13164"
      ]
     },
     "execution_count": 316,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "FcFB7h_o0JZC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcFB7h_o0JZC",
    "outputId": "7a8cbbad-1e5e-4e7e-cf50-b00a647a2ae9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13164"
      ]
     },
     "execution_count": 317,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "U0rA0C4s1OVW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0rA0C4s1OVW",
    "outputId": "1d2f5bc1-65d4-4e4e-cd22-8fad12f6cc0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 318,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_vocab_size = len(ar_vectorization.get_vocabulary())\n",
    "ar_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "04c57e1e",
   "metadata": {
    "id": "04c57e1e"
   },
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "# x = PositionalEmbedding(sequence_length, num_tokens, embed_dim, pretrained=True, weights=embedding_matrix)(encoder_inputs)\n",
    "x = PositionalEmbedding(sequence_length, num_tokens, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(ar_vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "vt9sp6Yrb4pE",
   "metadata": {
    "id": "vt9sp6Yrb4pE"
   },
   "outputs": [],
   "source": [
    "googledrive_path = '/content/drive/MyDrive/Transformers/translatoin_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "wqmaEQ94kyB9",
   "metadata": {
    "id": "wqmaEQ94kyB9"
   },
   "outputs": [],
   "source": [
    "from keras import callbacks\n",
    "early_stopping_cb = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=True)\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(googledrive_path+'/weights_adam.ckpt', monitor='val_accuracy', save_weights_only=True,verbose=True, save_best_only=True)\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=googledrive_path+\"/logs\")\n",
    "lr_schr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=True, factor=0.3, min_lr=0.0001)\n",
    "cbs = [early_stopping_cb, checkpoint_cb, tensorboard_callback, lr_schr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H50DkdfyLyqt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H50DkdfyLyqt",
    "outputId": "14b02f52-c376-4b58-9dab-288665275e71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding_12 (Positi (None, None, 300)    3964200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_6 (Transfor (None, None, 300)    4119848     positional_embedding_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "model_13 (Functional)           (None, None, 20000)  19042948    decoder_inputs[0][0]             \n",
      "                                                                 transformer_encoder_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 27,126,996\n",
      "Trainable params: 27,126,996\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "113/113 [==============================] - 84s 718ms/step - loss: 0.9279 - accuracy: 0.2975 - val_loss: 0.8085 - val_accuracy: 0.3481\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.34809, saving model to /content/drive/MyDrive/Transformers/translatoin_data/weights_adam.ckpt\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 80s 705ms/step - loss: 0.7555 - accuracy: 0.3720 - val_loss: 0.7600 - val_accuracy: 0.3744\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.34809 to 0.37443, saving model to /content/drive/MyDrive/Transformers/translatoin_data/weights_adam.ckpt\n",
      "Epoch 3/100\n",
      " 65/113 [================>.............] - ETA: 31s - loss: 0.6860 - accuracy: 0.4034"
     ]
    }
   ],
   "source": [
    "epochs = 100  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "j5Qvebl8FHsG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5Qvebl8FHsG",
    "outputId": "409bb93b-6c20-4a05-d937-b01576015d44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7efc93e21a10>"
      ]
     },
     "execution_count": 127,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(googledrive_path)\n",
    "transformer.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8447490b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8447490b",
    "outputId": "0579b30f-8902-4242-8ac0-574cfd8c53da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1 \n",
      " [start] المادة 1 [end]\n",
      "**************************************************\n",
      "Having regard to the proposal from the Commission; \n",
      " [start] مع الأخذ في الاعتبار الاقتراح الصادر عن المفوضية الأوروبية، [end]\n",
      "**************************************************\n",
      "Before publication as provided for in paragraphs 2 and 4 and registration as provided for in paragraph 3, the Commission may request the opinion of the Committee provided for in Article 15. \n",
      " [start] قبل المادة 9 [end]\n",
      "**************************************************\n",
      "The representative of the Commission shall submit to the Committee a draft of the measures to be taken \n",
      " [start] 3 التكيف مع التقدم التقني في أساليب التحليل الكمي المنصوص عليها في الملحق II يجب أن تتلاءم مع الإجراءات المحددة في المادة 6 [end]\n",
      "**************************************************\n",
      "2. The measures referred to in the second indent of paragraph 1 shall apply until the substance is listed in Annex I or until a decision not to list it has been taken in accordance with the procedure laid down in Article 29. \n",
      " [start] 2 يجب اتخاذ التدابير المشار إليها في الفقرة الفرعية الثانية من المادة 1 في المادة 2 [end]\n",
      "**************************************************\n",
      "Article 2 \n",
      " [start] المادة 2 [end]\n",
      "**************************************************\n",
      "3. The measures decided upon by the Commission may be referred to the Council by any Member State within three working days following the day on which they were communicated. The Council shall meet without delay. It may by a qualified majority amend or repeal the measures in question. \n",
      " [start] 3 لا يجب اتخاذ التدابير المناسبة حتى نهاية فترة قرار المفوضية الأوروبية أو لم يكن هؤلاء الأشخاص المهنيين المشار إليها في ما إذا رأت الهيئة على الفور من قبل الأغلبية المؤهلة [end]\n",
      "**************************************************\n",
      "Article 7 \n",
      " [start] المادة 7 [end]\n",
      "**************************************************\n",
      "2. The measures referred to in the second indent of paragraph 1 shall apply until the substance is listed in Annex I or until a decision not to list it has been taken in accordance with the procedure laid down in Article 29. \n",
      " [start] 2 يجب اتخاذ التدابير المشار إليها في الفقرة الفرعية الثانية من المادة 1 في المادة 2 [end]\n",
      "**************************************************\n",
      "- security features. \n",
      " [start] يتخذ جميع [end]\n",
      "**************************************************\n",
      "Member States shall ensure that employees' representatives, when carrying out their functions, enjoy adequate protection and guarantees to enable them to perform properly the duties which have been assigned to them. \n",
      " [start] يجب أن تضمن الدول الأعضاء أن يتمتع الأشخاص الذين يقررون أهلية الحصول على الإذن لبدء الأعمال [end]\n",
      "**************************************************\n",
      ">TABLE> \n",
      " [start] الجدول [end]\n",
      "**************************************************\n",
      "Having regard to the opinion of the Economic and Social Committee (2), \n",
      " [start] مع الأخذ في الاعتبار الرأي الصادر عن اللجنة الاقتصادية والاجتماعية 2؛ [end]\n",
      "**************************************************\n",
      "Article 5 \n",
      " [start] المادة 5 [end]\n",
      "**************************************************\n",
      "3. To qualify under the tariff quota covered by serial No 09.0003, the following must be presented: \n",
      " [start] 3 بخصوص الموافقة على نوع الآلية عندما تُلاحظ أي فروقات بين خصائصها والتفاصيل الواردة في شهادة الموافقة على النوع وأو وثيقة المعلومات، وعندما لا تكون الدولة العضو، التي منحت الموافقة على نوع الآلية، قد شرعت هذه الفروقات بموجب المادة 6 2 أو 3 ولا تعتبر الآلية مخالفة للنوع المعتمد عندما\n",
      "**************************************************\n",
      "2. The measures referred to in the second indent of paragraph 1 shall apply until the substance is listed in Annex I or until a decision not to list it has been taken in accordance with the procedure laid down in Article 29. \n",
      " [start] 2 يجب اتخاذ التدابير المشار إليها في الفقرة الفرعية الثانية من المادة 1 في المادة 2 [end]\n",
      "**************************************************\n",
      "THE COMMISSION OF THE EUROPEAN COMMUNITIES, \n",
      " [start] مفوضية المجموعات الأوروبية، [end]\n",
      "**************************************************\n",
      "Before publication as provided for in paragraphs 2 and 4 and registration as provided for in paragraph 3, the Commission may request the opinion of the Committee provided for in Article 15. \n",
      " [start] قبل المادة 9 [end]\n",
      "**************************************************\n",
      "- instruments referred to in Article 16(3) of Directive 73/239/EEC, \n",
      " [start] الأدوات المشار إليها في المادة 18، و 2؛ [end]\n",
      "**************************************************\n",
      "Having regard to the Opinion of the European Parliament; \n",
      " [start] مع الأخذ في الاعتبار الرأي الصادر عن البرلمان الأوروبي 2، [end]\n",
      "**************************************************\n",
      ">TABLE> \n",
      " [start] الجدول [end]\n",
      "**************************************************\n",
      "The representative of the Commission shall submit to the Committee a draft of the measures to be taken \n",
      " [start] 3 التكيف مع التقدم التقني في أساليب التحليل الكمي المنصوص عليها في الملحق II يجب أن تتلاءم مع الإجراءات المحددة في المادة 6 [end]\n",
      "**************************************************\n",
      "- the degree of overfishing, \n",
      " [start] شهادة الدكتوراه في الصيدلة التي تمنحها الجامعات؛ [end]\n",
      "**************************************************\n",
      "Article 1 \n",
      " [start] المادة 1 [end]\n",
      "**************************************************\n",
      "Article 2 \n",
      " [start] المادة 2 [end]\n",
      "**************************************************\n",
      "Having regard to the Treaty establishing the European Economic Community, and in particular Article 99 thereof, \n",
      " [start] مع الأخذ في الاعتبار المعاهدة التي أنشئت بموجبها الجماعة الاقتصادية الأوروبية وبالتحديد المادة 100 منها؛ [end]\n",
      "**************************************************\n",
      "Article 7 \n",
      " [start] المادة 7 [end]\n",
      "**************************************************\n",
      "(c) the technical action plan for the following year; \n",
      " [start] ج الظروف الخاصة بصندوق الحصص في إطار عمل عام [end]\n",
      "**************************************************\n",
      "The representative of the Commission shall submit to the Committee a draft of the measures to be taken \n",
      " [start] 3 التكيف مع التقدم التقني في أساليب التحليل الكمي المنصوص عليها في الملحق II يجب أن تتلاءم مع الإجراءات المحددة في المادة 6 [end]\n",
      "**************************************************\n",
      "Before publication as provided for in paragraphs 2 and 4 and registration as provided for in paragraph 3, the Commission may request the opinion of the Committee provided for in Article 15. \n",
      " [start] قبل المادة 9 [end]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "ar_vocab = ar_vectorization.get_vocabulary()\n",
    "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
    "max_decoded_sentence_length = sequence_length\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = ar_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in val_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts[:30])\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence, '\\n', translated)\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "p2i4SbVoP_Yz",
   "metadata": {
    "id": "p2i4SbVoP_Yz"
   },
   "outputs": [],
   "source": [
    "def get_bleu():\n",
    "  \n",
    "  preds, src = [], []\n",
    "\n",
    "  with tqdm(total=len(val_pairs), position=0, leave=True) as pbar:\n",
    "    for en_sent, ar_sent in tqdm(val_pairs, position=0, leave=True):\n",
    "      translated = decode_sequence(en_sent)\n",
    "      preds.append(translated)\n",
    "      src.append(ar_sent)\n",
    "      pbar.update()\n",
    "\n",
    "    return src, preds\n",
    "    # print_scores(src, preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "A9uh1623AwUW",
   "metadata": {
    "id": "A9uh1623AwUW"
   },
   "outputs": [],
   "source": [
    "def print_scores(trgs, preds):\n",
    "    print('----- Bleu-n Scores -----')\n",
    "    print(\"1:\", corpus_bleu(trgs, preds, weights=[1.0/1.0])*100)\n",
    "    print(\"2:\", corpus_bleu(trgs, preds, weights=[1.0/2.0, 1.0/2.0])*100)\n",
    "    print(\"3:\", corpus_bleu(trgs, preds, weights=[1.0/3.0, 1.0/3.0, 1.0/3.0])*100)\n",
    "    print(\"4:\", corpus_bleu(trgs, preds)*100)\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "YaXlIeHdRGqi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YaXlIeHdRGqi",
    "outputId": "ee8a6fbf-2d72-4bc6-b65c-ce54f944723d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1113/1113 [10:59<00:00,  1.69it/s]\n",
      "100%|██████████| 1113/1113 [10:59<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "src, preds = get_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "tSinG65pSGkC",
   "metadata": {
    "id": "tSinG65pSGkC"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "hEfbrEOtBKSk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hEfbrEOtBKSk",
    "outputId": "8f89b268-946b-4cf4-e58d-38b60bebcbef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 19.884332149891485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 44.59185144159355\n",
      "3: 58.3673984448545\n",
      "4: 66.77713039775935\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_scores(preds, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "mBYxHDaET9ya",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBYxHDaET9ya",
    "outputId": "b43e79a4-0bf6-4ad8-9b39-51d353de5dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  المادة 1\n",
      "source:  المادة 1\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  ب يجوز أن تستخدم المنتجات الوحيدة التي تتألف من قبل السلطة المختصة في إطار برنامج معتمد لذلك من تلك البيانات وقابليتها للمقارنة؛\n",
      "source:  (ب) يشغلها شخص مستعد لمسك محاسبة الحيازة الزراعية وقادر على القيام بها، ومستعد لوضع بيانات المحاسبة لحيازته الزراعية في تصرف المفوضية الأوروبية؛\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  مع الأخذ في الاعتبار الرأي الصادر عن اللجنة الاقتصادية والاجتماعية 2؛\n",
      "source:  مع الأخذ في الاعتبار الرأي الصادر عن اللجنة الاقتصادية والاجتماعية (2)،\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  الفصل الخامس\n",
      "source:  الفصل الخامس\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  يجب أن تمتثل للمتطلبات المنصوص عليها في الملحق 5 ما يلي\n",
      "source:  بموجب الشروط المنصوص عليها في الملحق، تتحمل المفوضية تكلفة ما يلي:\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  2 حيث ووفقاً للشروط التي تُطبق الفقرة 1 في حالة عدم التقيد بالفقرة 1 ب وعند الضرورة، يتم فيها قبول أعضاء هذه الحالة، يجوز للهيئة تقديم المساهمات لهذا النظام خلال شخص يعمل باسمه الخاص بهذا النظام خلال فترة تعيينه في دولة عضو\n",
      "source:  2. حيث عملا بالفقرة 1، وباستمرار العمل بمساهمات نظام التقاعد التكميلي في إحدى الدول الأعضاء المساهمات، يعفى صاحب العمل والعامل، عند الاقتضاء، من أي التزام بالمساهمة في نظام التقاعد التكميلي في أي دولة عضو أخرى.\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  مع الأخذ في الاعتبار الرأي الصادر عن اللجنة الاقتصادية والاجتماعية 2؛\n",
      "source:  مع الأخذ في الاعتبار الرأي الصادر عن اللجنة الاقتصادية والاجتماعية (2)،\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  توجيه المجلس رقم 9277 المؤرخ في 14 تشرين الأول 1993 حول تثبيت المستويات القصوى المؤقتة للمتبقيات من ألياف النسيج الثنائي 4 و5 و6، غير كافية، قد تؤثر ذه المسألة على النحو المعدل أخيراً بواسطة التوجيه ‎90642EEC\n",
      "source:  توجيه المجلس رقم 93/41/EEC بتاريخ 14 يونيو/حزيران 1993 الذي أبطل التوجيه رقم 87/22/EEC بشأن تنسيق التدابير الوطنية المتعلقة بطرح منتجات طبية ذات التكنولوجيا العالية في السوق، وبخاصة تلك المشتقة من التكنولوجيا الحيوية\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  في حالة عدم الإخلال بأحكام الفقرة 3، لا يوجد بها من الدول الأعضاء المنتجات المخصصة للاستهلاك البشري وفقًا للمادة 4 من قِبل الإدارات الفرنسية ما يتعلق بتراخيص تسويق المنتجات المسحوبة من قِبل منظمة المنتجين وتحديد\n",
      "source:  في الملخص الإحصائي للمخزون المنصوص عليه في المادة 4، يتعين حساب المنتجات المُجهّزة وفقًا للحمولة الفعلية بالطن؛ ويتعين حساب النفط الخام والمنتجات الوسيطة:\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  1 التي تتم إضافة إلى حد تغييرها لعقد\n",
      "source:  1. يجب حماية الأسماء المسجلة من:\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "  i = random.randint(0, 500)\n",
    "  print(\"prediction:\", preds[i][7:-6])\n",
    "  print(\"source:\", src[i][7:-6])\n",
    "  print('_'*100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "translation data",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
