{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:47.415253Z",
     "iopub.status.busy": "2021-06-08T02:16:47.414915Z",
     "iopub.status.idle": "2021-06-08T02:16:47.421088Z",
     "shell.execute_reply": "2021-06-08T02:16:47.419702Z",
     "shell.execute_reply.started": "2021-06-08T02:16:47.415225Z"
    },
    "id": "e9e97fa1"
   },
   "outputs": [],
   "source": [
    "# modules\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import pyarabic.araby as araby\n",
    "from pyarabic.araby import strip_tashkeel, strip_tatweel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1710411f"
   },
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:47.955686Z",
     "iopub.status.busy": "2021-06-08T02:16:47.955355Z",
     "iopub.status.idle": "2021-06-08T02:16:48.136995Z",
     "shell.execute_reply": "2021-06-08T02:16:48.136060Z",
     "shell.execute_reply.started": "2021-06-08T02:16:47.955644Z"
    },
    "id": "1RNDMX9cizGW",
    "outputId": "d8167ea6-e4ad-4614-e635-3c3938556fd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kelwa/anaconda3/envs/colab/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/kelwa/anaconda3/envs/colab/lib/python3.7/site-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "ar = pd.read_table('ArabicNewData.txt', delimiter='\\\\n', names=['ar'])\n",
    "en = pd.read_table('EnglishNewData.txt', delimiter='\\\\n', names=['en'])\n",
    "\n",
    "en['ar'] = ar['ar']\n",
    "df = en.copy()\n",
    "df = df.iloc[:35118]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:48.455428Z",
     "iopub.status.busy": "2021-06-08T02:16:48.455116Z",
     "iopub.status.idle": "2021-06-08T02:16:48.461638Z",
     "shell.execute_reply": "2021-06-08T02:16:48.460738Z",
     "shell.execute_reply.started": "2021-06-08T02:16:48.455402Z"
    },
    "id": "T2YTqqOTGl5c"
   },
   "outputs": [],
   "source": [
    "morphs = [strip_tashkeel, strip_tatweel]\n",
    "\n",
    "def fix_ar(sent):\n",
    "  sent = split_al_sent(sent)\n",
    "  tokens = araby.tokenize(sent, morphs=morphs)\n",
    "  sent = araby.normalize_hamza(' '.join(tokens), method='tasheel')\n",
    "  return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:48.992476Z",
     "iopub.status.busy": "2021-06-08T02:16:48.992161Z",
     "iopub.status.idle": "2021-06-08T02:16:48.998243Z",
     "shell.execute_reply": "2021-06-08T02:16:48.997288Z",
     "shell.execute_reply.started": "2021-06-08T02:16:48.992448Z"
    },
    "id": "7WhmXTv1KRkd"
   },
   "outputs": [],
   "source": [
    "def split_al(word):\n",
    "    if word.startswith('ال'):\n",
    "        return word[:2], word[2:]\n",
    "    else: \n",
    "        return word\n",
    "\n",
    "def split_al_sent(sent):\n",
    "    ww = []\n",
    "    for word in sent.split():\n",
    "        out = split_al(word)\n",
    "        if type(out) is tuple:\n",
    "            for w in out:\n",
    "                ww.append(w)\n",
    "        else:\n",
    "            ww.append(word)\n",
    "    return ' '.join(w for w in ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:50.345366Z",
     "iopub.status.busy": "2021-06-08T02:16:50.345045Z",
     "iopub.status.idle": "2021-06-08T02:16:51.796892Z",
     "shell.execute_reply": "2021-06-08T02:16:51.795902Z",
     "shell.execute_reply.started": "2021-06-08T02:16:50.345330Z"
    },
    "id": "QGuKlu3mFIZc"
   },
   "outputs": [],
   "source": [
    "df['ar'] = df.apply(lambda row: fix_ar(row.ar), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:51.799840Z",
     "iopub.status.busy": "2021-06-08T02:16:51.799257Z",
     "iopub.status.idle": "2021-06-08T02:16:54.618551Z",
     "shell.execute_reply": "2021-06-08T02:16:54.617702Z",
     "shell.execute_reply.started": "2021-06-08T02:16:51.799800Z"
    },
    "id": "be905b2c"
   },
   "outputs": [],
   "source": [
    "text_pairs = []\n",
    "for idx, row in df.iterrows():\n",
    "  en, ar = row['en'], row['ar']\n",
    "  ar = \"[start] \" + ar + \" [end]\"\n",
    "  text_pairs.append((en, ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:16:58.013120Z",
     "iopub.status.busy": "2021-06-08T02:16:58.012634Z",
     "iopub.status.idle": "2021-06-08T02:17:00.598315Z",
     "shell.execute_reply": "2021-06-08T02:17:00.597570Z",
     "shell.execute_reply.started": "2021-06-08T02:16:58.013084Z"
    },
    "id": "fAxNAdK2H0eB"
   },
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "  if len(row.ar.split()) < 1:\n",
    "    print(row.ar, '\\n*')\n",
    "    print(row.en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:00.600692Z",
     "iopub.status.busy": "2021-06-08T02:17:00.600320Z",
     "iopub.status.idle": "2021-06-08T02:17:00.605892Z",
     "shell.execute_reply": "2021-06-08T02:17:00.605074Z",
     "shell.execute_reply.started": "2021-06-08T02:17:00.600644Z"
    },
    "id": "296e4594",
    "outputId": "43dfaace-2dba-4a9c-8b77-b07b72131c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mr Dances With Wolves.', '[start] Mr Dances With Wolves . [end]')\n",
      "('At ease.', '[start] بهدوء [end]')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:00.608167Z",
     "iopub.status.busy": "2021-06-08T02:17:00.607746Z",
     "iopub.status.idle": "2021-06-08T02:17:00.620306Z",
     "shell.execute_reply": "2021-06-08T02:17:00.619437Z",
     "shell.execute_reply.started": "2021-06-08T02:17:00.608132Z"
    },
    "id": "o6kRoro9iBOe",
    "outputId": "075bb101-3e48-43bc-ad00-b6842871dd8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35118"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:01.800560Z",
     "iopub.status.busy": "2021-06-08T02:17:01.800249Z",
     "iopub.status.idle": "2021-06-08T02:17:01.847161Z",
     "shell.execute_reply": "2021-06-08T02:17:01.846417Z",
     "shell.execute_reply.started": "2021-06-08T02:17:01.800532Z"
    },
    "id": "c1283f6f"
   },
   "outputs": [],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) -  num_val_samples\n",
    "train_pairs = text_pairs[: num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:02.646210Z",
     "iopub.status.busy": "2021-06-08T02:17:02.645892Z",
     "iopub.status.idle": "2021-06-08T02:17:02.654338Z",
     "shell.execute_reply": "2021-06-08T02:17:02.653626Z",
     "shell.execute_reply.started": "2021-06-08T02:17:02.646181Z"
    },
    "id": "a7a1f8cd",
    "outputId": "1ef443a4-1c7a-4fa1-a568-f3dc6436cadc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35118 total pairs\n",
      "29851 training pairs\n",
      "5267 validation pairs\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5f6d84e"
   },
   "source": [
    "#### Vectorizing the text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:03.954544Z",
     "iopub.status.busy": "2021-06-08T02:17:03.954239Z",
     "iopub.status.idle": "2021-06-08T02:17:04.856227Z",
     "shell.execute_reply": "2021-06-08T02:17:04.854499Z",
     "shell.execute_reply.started": "2021-06-08T02:17:03.954516Z"
    },
    "id": "487ed66f"
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "vocab_size = 20000\n",
    "sequence_length = 50\n",
    "batch_size = 265\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    return tf.strings.regex_replace(input_string, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "            # max_tokens=vocab_size, \n",
    "            output_mode='int', \n",
    "            output_sequence_length=sequence_length)\n",
    "\n",
    "ar_vectorization = TextVectorization(\n",
    "    # max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size)\n",
    "\n",
    "eng_texts = [pair[0] for pair in text_pairs]\n",
    "ar_texts = [pair[1] for pair in text_pairs]\n",
    "eng_vectorization.adapt(eng_texts)\n",
    "ar_vectorization.adapt(ar_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:04.859565Z",
     "iopub.status.busy": "2021-06-08T02:17:04.859040Z",
     "iopub.status.idle": "2021-06-08T02:17:04.938344Z",
     "shell.execute_reply": "2021-06-08T02:17:04.937688Z",
     "shell.execute_reply.started": "2021-06-08T02:17:04.859525Z"
    },
    "id": "ubW5QOJtLzMI",
    "outputId": "d0d0cfc9-b35a-4522-c9ce-55d2bc6f112b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 13164)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ar_vectorization.get_vocabulary()), len(eng_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:05.013639Z",
     "iopub.status.busy": "2021-06-08T02:17:05.013252Z",
     "iopub.status.idle": "2021-06-08T02:17:05.021566Z",
     "shell.execute_reply": "2021-06-08T02:17:05.020566Z",
     "shell.execute_reply.started": "2021-06-08T02:17:05.013599Z"
    },
    "id": "V5qiqjOWdVz1"
   },
   "outputs": [],
   "source": [
    "def format_dataset(eng, ar):\n",
    "    eng = eng_vectorization(eng)\n",
    "    ar = ar_vectorization(ar)\n",
    "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, ar_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    ar_texts = list(ar_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:05.597209Z",
     "iopub.status.busy": "2021-06-08T02:17:05.596886Z",
     "iopub.status.idle": "2021-06-08T02:17:06.245999Z",
     "shell.execute_reply": "2021-06-08T02:17:06.245218Z",
     "shell.execute_reply.started": "2021-06-08T02:17:05.597181Z"
    },
    "id": "7a3ed140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function format_dataset at 0x7f3759ba7d90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function format_dataset at 0x7f3759ba7d90> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:06.248284Z",
     "iopub.status.busy": "2021-06-08T02:17:06.247956Z",
     "iopub.status.idle": "2021-06-08T02:17:06.519611Z",
     "shell.execute_reply": "2021-06-08T02:17:06.518592Z",
     "shell.execute_reply.started": "2021-06-08T02:17:06.248250Z"
    },
    "id": "3902789f",
    "outputId": "db94c6e4-848f-4e66-fb70-88469b8ca52c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (265, 50)\n",
      "inputs[\"decoder_inputs\"].shape: (265, 50)\n",
      "targets.shape: (265, 50)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fa0936c"
   },
   "source": [
    "### Building the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:17:07.222939Z",
     "iopub.status.busy": "2021-06-08T02:17:07.222610Z",
     "iopub.status.idle": "2021-06-08T02:17:07.247405Z",
     "shell.execute_reply": "2021-06-08T02:17:07.246264Z",
     "shell.execute_reply.started": "2021-06-08T02:17:07.222907Z"
    },
    "id": "b3baf452"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "          'embed_dim': self.embed_dim,\n",
    "          'dense_dim': self.dense_dim,\n",
    "          'num_heads': self.num_heads,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, pretrained=False, weights=False, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        if not pretrained:\n",
    "          self.token_embeddings = layers.Embedding(\n",
    "              input_dim=vocab_size, output_dim=embed_dim\n",
    "          )\n",
    "        else:\n",
    "          # pre-trained\n",
    "          self.token_embeddings = layers.Embedding(\n",
    "              input_dim=vocab_size, output_dim=embed_dim, weights=[weights]\n",
    "          ) \n",
    "\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "      \n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "      'sequence_length': self.sequence_length,\n",
    "      'vocab_size': self.vocab_size,\n",
    "      'embed_dim': self.embed_dim,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "\n",
    "      config = super().get_config().copy()\n",
    "      config.update({\n",
    "      'embed_dim': self.embed_dim,\n",
    "      'latent_dim': self.latent_dim,\n",
    "      'num_heads': self.num_heads,\n",
    "      })\n",
    "      return config\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:46:29.637271Z",
     "iopub.status.busy": "2021-06-08T03:46:29.636942Z",
     "iopub.status.idle": "2021-06-08T03:46:29.674663Z",
     "shell.execute_reply": "2021-06-08T03:46:29.673702Z",
     "shell.execute_reply.started": "2021-06-08T03:46:29.637241Z"
    },
    "id": "LMeo9ow6hAL4"
   },
   "outputs": [],
   "source": [
    "def get_weights(vectorizer, embeddings_path, is_gensim=False, is_local=False):\n",
    "    path_to_glove_file = embeddings_path\n",
    "    \n",
    "    if is_gensim:\n",
    "        embeddings_index = gensim.models.Word2Vec.load(embeddings_path)\n",
    "\n",
    "    elif is_local:\n",
    "        with open(embeddings_path, \"rb\") as f:\n",
    "            embeddings_index = pickle.load(f)\n",
    "    else:\n",
    "        embeddings_index = {}\n",
    "        with open(path_to_glove_file) as f:\n",
    "            for line in f:\n",
    "                word, coefs = line.split(maxsplit=1)\n",
    "                coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                embeddings_index[word] = coefs\n",
    "    \n",
    "    if not is_gensim:\n",
    "        print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "    else:\n",
    "        print(\"Found %s word vectors.\" % (embeddings_index.wv.vectors.shape[0]))\n",
    "\n",
    "    \n",
    "    \n",
    "    vocab = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(vocab, range(len(vocab))))\n",
    "    num_tokens = len(vocab)\n",
    "    embedding_dim = 300\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if not is_gensim:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "        else:\n",
    "            if word in embeddings_index.wv:\n",
    "                embedding_vector = embeddings_index.wv[word]\n",
    "            else:\n",
    "                embedding_vector = None\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.uniform(-.1, .1, size=(embedding_dim))\n",
    "            misses += 1\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T03:47:05.607709Z",
     "iopub.status.busy": "2021-06-08T03:47:05.607322Z",
     "iopub.status.idle": "2021-06-08T03:47:35.559143Z",
     "shell.execute_reply": "2021-06-08T03:47:35.557315Z",
     "shell.execute_reply.started": "2021-06-08T03:47:05.607653Z"
    },
    "id": "vm0fr1fDhAL5",
    "outputId": "a3b542ee-8f42-40ab-cd21-4ffffd0f2f18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10002 word vectors.\n",
      "Converted 5625 words (7539 misses)\n"
     ]
    }
   ],
   "source": [
    "english_embeddings = get_weights(eng_vectorization, './pretrained_en.pkl', is_local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T03:46:30.842754Z",
     "iopub.status.busy": "2021-06-08T03:46:30.842479Z",
     "iopub.status.idle": "2021-06-08T03:46:53.711963Z",
     "shell.execute_reply": "2021-06-08T03:46:53.710792Z",
     "shell.execute_reply.started": "2021-06-08T03:46:30.842728Z"
    },
    "id": "tHG1gRmEhAL5",
    "outputId": "a0405371-3ba5-4063-ac6e-460376bd4a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10002 word vectors.\n",
      "Converted 4285 words (15715 misses)\n"
     ]
    }
   ],
   "source": [
    "arabic_embeddings = get_weights(ar_vectorization, './pretrained_ar.pkl', is_local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T03:47:59.830984Z",
     "iopub.status.busy": "2021-06-08T03:47:59.830643Z",
     "iopub.status.idle": "2021-06-08T03:47:59.906741Z",
     "shell.execute_reply": "2021-06-08T03:47:59.905831Z",
     "shell.execute_reply.started": "2021-06-08T03:47:59.830952Z"
    },
    "id": "U0rA0C4s1OVW",
    "outputId": "851bae6c-4fc6-4a1e-a838-c917d0e760c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13164, 20000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_vocab_size = len(ar_vectorization.get_vocabulary())\n",
    "en_vocab_size = len(eng_vectorization.get_vocabulary())\n",
    "en_vocab_size, ar_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:49:47.423713Z",
     "iopub.status.busy": "2021-06-08T03:49:47.423352Z",
     "iopub.status.idle": "2021-06-08T03:49:48.045217Z",
     "shell.execute_reply": "2021-06-08T03:49:48.044246Z",
     "shell.execute_reply.started": "2021-06-08T03:49:47.423679Z"
    },
    "id": "04c57e1e"
   },
   "outputs": [],
   "source": [
    "embed_dim = 300\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, en_vocab_size, embed_dim, pretrained=True, weights=english_embeddings)(encoder_inputs)\n",
    "# x = PositionalEmbedding(sequence_length, num_tokens, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim, pretrained=True, weights=arabic_embeddings)(decoder_inputs)\n",
    "# x = PositionalEmbedding(sequence_length, ar_vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(ar_vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:49:49.472124Z",
     "iopub.status.busy": "2021-06-08T03:49:49.471793Z",
     "iopub.status.idle": "2021-06-08T03:49:49.477189Z",
     "shell.execute_reply": "2021-06-08T03:49:49.476150Z",
     "shell.execute_reply.started": "2021-06-08T03:49:49.472094Z"
    },
    "id": "vt9sp6Yrb4pE"
   },
   "outputs": [],
   "source": [
    "googledrive_path = './our_pretrained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:49:54.155939Z",
     "iopub.status.busy": "2021-06-08T03:49:54.155540Z",
     "iopub.status.idle": "2021-06-08T03:49:54.164631Z",
     "shell.execute_reply": "2021-06-08T03:49:54.163841Z",
     "shell.execute_reply.started": "2021-06-08T03:49:54.155904Z"
    },
    "id": "wqmaEQ94kyB9"
   },
   "outputs": [],
   "source": [
    "early_stopping_cb = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=True)\n",
    "checkpoint_cb = callbacks.ModelCheckpoint(googledrive_path+'/weights_adam.ckpt', monitor='val_accuracy', save_weights_only=True,verbose=True, save_best_only=True)\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=googledrive_path+\"/logs\")\n",
    "lr_schr = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=2, verbose=True, factor=0.3, min_lr=0.0001)\n",
    "cbs = [early_stopping_cb, checkpoint_cb, tensorboard_callback, lr_schr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  9 00:34:45 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 465.31       Driver Version: 465.31       CUDA Version: 11.3     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| N/A   53C    P8     6W /  N/A |     47MiB /  6078MiB |     31%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     15093      G   /usr/lib/Xorg                      45MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-06-08T03:50:03.473046Z",
     "iopub.status.busy": "2021-06-08T03:50:03.472687Z",
     "iopub.status.idle": "2021-06-08T04:02:41.112268Z",
     "shell.execute_reply": "2021-06-08T04:02:41.111344Z",
     "shell.execute_reply.started": "2021-06-08T03:50:03.473016Z"
    },
    "id": "oe_QsoxGhAL7",
    "outputId": "4f6cd7cc-d139-4384-ab01-b651539b8c91",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding_2 (Positio (None, None, 300)    3964200     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_1 (Transfor (None, None, 300)    4119848     positional_embedding_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Functional)            (None, None, 20000)  19042948    decoder_inputs[0][0]             \n",
      "                                                                 transformer_encoder_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 27,126,996\n",
      "Trainable params: 27,126,996\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-2361f164f99d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/colab/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/colab/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/colab/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/colab/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/colab/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/colab/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/colab/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:39:58.377328Z",
     "iopub.status.busy": "2021-06-08T02:39:58.377008Z",
     "iopub.status.idle": "2021-06-08T02:39:58.713799Z",
     "shell.execute_reply": "2021-06-08T02:39:58.712839Z",
     "shell.execute_reply.started": "2021-06-08T02:39:58.377296Z"
    },
    "id": "j5Qvebl8FHsG",
    "outputId": "409bb93b-6c20-4a05-d937-b01576015d44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fa2c86f8210>"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(googledrive_path)\n",
    "transformer.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T04:04:11.583522Z",
     "iopub.status.busy": "2021-06-08T04:04:11.583186Z",
     "iopub.status.idle": "2021-06-08T04:04:17.842227Z",
     "shell.execute_reply": "2021-06-08T04:04:17.841483Z",
     "shell.execute_reply.started": "2021-06-08T04:04:11.583490Z"
    },
    "id": "8447490b",
    "outputId": "0579b30f-8902-4242-8ac0-574cfd8c53da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was originally just a water container. \n",
      " [start] هذه كانت ال حجرة سفينة فقط [UNK] [end]\n",
      "**************************************************\n",
      "Room 1 4. Next to the solarium. \n",
      " [start] ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة [end]\n",
      "**************************************************\n",
      "Maybe not as thick as the ones that Joshua blew down with his trumpet. \n",
      " [start] ربما لا بين ال ثلج كما [UNK] [end]\n",
      "**************************************************\n",
      "Room 1 4. Next to the solarium. \n",
      " [start] ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة ال حلقة [end]\n",
      "**************************************************\n",
      "You're not Han Na, are you? \n",
      " [start] انت لست هان نا ، اليس كذلك ؟ [end]\n",
      "**************************************************\n",
      "I Won't Be Here,And- \n",
      " [start] لن اكون ساكون [end]\n",
      "**************************************************\n",
      "You're not Han Na, are you? \n",
      " [start] انت لست هان نا ، اليس كذلك ؟ [end]\n",
      "**************************************************\n",
      "I'll keep my foot here, and you hit it whenever you can. \n",
      " [start] [UNK] على ال ان تذهب و [UNK] [end]\n",
      "**************************************************\n",
      "So that is indeed what they're planning. \n",
      " [start] لذا ، هذا ال حقيقة انهم يقولوا ماذا ؟ [end]\n",
      "**************************************************\n",
      "I Won't Be Here,And- \n",
      " [start] لن اكون ساكون [end]\n",
      "**************************************************\n",
      "That's large-format barrels, neutral oak. \n",
      " [start] ذلك سحر ال [UNK] ال سرو [end]\n",
      "**************************************************\n",
      "Father, come on. \n",
      " [start] ابي هيا [end]\n",
      "**************************************************\n",
      "You're definitely not the Tony Stark I once knew. \n",
      " [start] انت متاكد انك لا توني ، ان [end]\n",
      "**************************************************\n",
      "This was an office for ghosts? \n",
      " [start] لقد كنت هنا بسبب شبح [end]\n",
      "**************************************************\n",
      "This was an office for ghosts? \n",
      " [start] لقد كنت هنا بسبب شبح [end]\n",
      "**************************************************\n",
      "Yeah,Mr. Meade Meant A Lot To Me, \n",
      " [start] كلير ميد انا ، انت ، انا [end]\n",
      "**************************************************\n",
      "I'll keep my foot here, and you hit it whenever you can. \n",
      " [start] [UNK] على ال ان تذهب و [UNK] [end]\n",
      "**************************************************\n",
      "Find out what she's into, and then also be into it. \n",
      " [start] يبحث عنها و هي و هي ايضا [end]\n",
      "**************************************************\n",
      "Kang Woo, are you having a hard time? \n",
      " [start] كانغ وو ، هل انت ايضا ؟ [end]\n",
      "**************************************************\n",
      "Tarzan, don't. \n",
      " [start] ترازان لا [end]\n",
      "**************************************************\n",
      "I purposely didn't given it back to him because that's what I wanted. \n",
      " [start] لقد [UNK] ال ذي لم افعل ما هو [end]\n",
      "**************************************************\n",
      "- Later, perhaps. \n",
      " [start] ربما ربما [end]\n",
      "**************************************************\n",
      "Thank you! \n",
      " [start] شكرا لك [end]\n",
      "**************************************************\n",
      "Honey! \n",
      " [start] عزيزتي [end]\n",
      "**************************************************\n",
      "-Let him have it! \n",
      " [start] [UNK] [end]\n",
      "**************************************************\n",
      "This was an office for ghosts? \n",
      " [start] لقد كنت هنا بسبب شبح [end]\n",
      "**************************************************\n",
      "- Being confident without begging... \n",
      " [start] احيانا [end]\n",
      "**************************************************\n",
      "Find out what she's into, and then also be into it. \n",
      " [start] يبحث عنها و هي و هي ايضا [end]\n",
      "**************************************************\n",
      "- Bed 26, sir. \n",
      " [start] سرير رقم [UNK] [end]\n",
      "**************************************************\n",
      "That's large-format barrels, neutral oak. \n",
      " [start] ذلك سحر ال [UNK] ال سرو [end]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "ar_vocab = ar_vectorization.get_vocabulary()\n",
    "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
    "max_decoded_sentence_length = sequence_length\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = ar_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in val_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_eng_texts[:30])\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(input_sentence, '\\n', translated)\n",
    "    print('*'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:40:58.149218Z",
     "iopub.status.busy": "2021-06-08T02:40:58.148840Z",
     "iopub.status.idle": "2021-06-08T02:40:58.155254Z",
     "shell.execute_reply": "2021-06-08T02:40:58.154292Z",
     "shell.execute_reply.started": "2021-06-08T02:40:58.149177Z"
    },
    "id": "p2i4SbVoP_Yz"
   },
   "outputs": [],
   "source": [
    "def get_bleu():\n",
    "  \n",
    "  preds, src = [], []\n",
    "\n",
    "  with tqdm(total=len(val_pairs), position=0, leave=True) as pbar:\n",
    "    for en_sent, ar_sent in tqdm(val_pairs, position=0, leave=True):\n",
    "      translated = decode_sequence(en_sent)\n",
    "      preds.append(translated)\n",
    "      src.append(ar_sent)\n",
    "      pbar.update()\n",
    "\n",
    "    return src, preds\n",
    "    # print_scores(src, preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:40:59.410504Z",
     "iopub.status.busy": "2021-06-08T02:40:59.410199Z",
     "iopub.status.idle": "2021-06-08T02:40:59.415944Z",
     "shell.execute_reply": "2021-06-08T02:40:59.414990Z",
     "shell.execute_reply.started": "2021-06-08T02:40:59.410474Z"
    },
    "id": "A9uh1623AwUW"
   },
   "outputs": [],
   "source": [
    "def print_scores(trgs, preds):\n",
    "    print('----- Bleu-n Scores -----')\n",
    "    print(\"1:\", corpus_bleu(trgs, preds, weights=[1.0/1.0])*100)\n",
    "    print(\"2:\", corpus_bleu(trgs, preds, weights=[1.0/2.0, 1.0/2.0])*100)\n",
    "    print(\"3:\", corpus_bleu(trgs, preds, weights=[1.0/3.0, 1.0/3.0, 1.0/3.0])*100)\n",
    "    print(\"4:\", corpus_bleu(trgs, preds)*100)\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:41:00.632276Z",
     "iopub.status.busy": "2021-06-08T02:41:00.631949Z",
     "iopub.status.idle": "2021-06-08T02:59:20.928004Z",
     "shell.execute_reply": "2021-06-08T02:59:20.925932Z",
     "shell.execute_reply.started": "2021-06-08T02:41:00.632247Z"
    },
    "id": "YaXlIeHdRGqi",
    "outputId": "ee8a6fbf-2d72-4bc6-b65c-ce54f944723d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5267/5267 [18:20<00:00,  4.79it/s]\n",
      "100%|██████████| 5267/5267 [18:20<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "src, preds = get_bleu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:59:20.930082Z",
     "iopub.status.busy": "2021-06-08T02:59:20.929740Z",
     "iopub.status.idle": "2021-06-08T02:59:20.934886Z",
     "shell.execute_reply": "2021-06-08T02:59:20.933814Z",
     "shell.execute_reply.started": "2021-06-08T02:59:20.930048Z"
    },
    "id": "tSinG65pSGkC"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T02:59:20.936757Z",
     "iopub.status.busy": "2021-06-08T02:59:20.936277Z",
     "iopub.status.idle": "2021-06-08T03:00:10.585162Z",
     "shell.execute_reply": "2021-06-08T03:00:10.583379Z",
     "shell.execute_reply.started": "2021-06-08T02:59:20.936723Z"
    },
    "id": "-QGhrnethAL9",
    "outputId": "536e52ef-76ae-4be2-8cfc-c190417c36ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Bleu-n Scores -----\n",
      "1: 40.14637653331428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 63.3611683393814\n",
      "3: 73.77039652394956\n",
      "4: 79.59972885593355\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print_scores(preds, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-08T03:00:17.962374Z",
     "iopub.status.busy": "2021-06-08T03:00:17.962062Z",
     "iopub.status.idle": "2021-06-08T03:00:17.973887Z",
     "shell.execute_reply": "2021-06-08T03:00:17.971402Z",
     "shell.execute_reply.started": "2021-06-08T03:00:17.962344Z"
    },
    "id": "mBYxHDaET9ya",
    "outputId": "b43e79a4-0bf6-4ad8-9b39-51d353de5dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction:  اذا كان [UNK] على ال يدي\n",
      "source:  لو وق بين يدى\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  ماذا قلت ؟\n",
      "source:  - ماذا قلت ؟\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  يبدو انك [UNK]\n",
      "source:  . واو .... رييس انت تكسب ال كثير من ال اموال\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  لا ، لا ، لا\n",
      "source:  لا يوجد رد\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  هذه هي ال احذية لكن ، اعتقد انه في سريرك\n",
      "source:  هذه امتعته ، لكن اعتقد هو في حجرته .\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  لانى [UNK]\n",
      "source:  #، هذا جزيي ال مفضل لانكم #\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  اكره ال ناس ال اكره ال طلاب\n",
      "source:  اكره متاعب ال فتيات\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  الى ال كونت هناك واجب لابتهاج ال خبز ال خبز و تنفيس\n",
      "source:  بالسجن ال ابدي ان تاكلي خبز ال حزن و ان تشربي ماء ال عذاب\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  كل ال خروج من ال طريق\n",
      "source:  حسنا ايها ال ناس ابتعدو من هنا\n",
      "____________________________________________________________________________________________________\n",
      "prediction:  لا افهم\n",
      "source:  لست افهم\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "  i = random.randint(0, 500)\n",
    "  print(\"prediction:\", preds[i][7:-6])\n",
    "  print(\"source:\", src[i][7:-6])\n",
    "  print('_'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIBpHq8fhAL-"
   },
   "outputs": [],
   "source": [
    "from PyRouge.pyrouge import Rouge\n",
    "\n",
    "r = Rouge()\n",
    "\n",
    "system_generated_summary = \"The Kyrgyz President pushed through the law requiring the use of ink during the upcoming Parliamentary and Presidential elections In an effort to live up to its reputation in the 1990s as an island of democracy. The use of ink is one part of a general effort to show commitment towards more open elections. improper use of this type of ink can cause additional problems as the elections in Afghanistan showed. The use of ink and readers by itself is not a panacea for election ills.\"\n",
    "\n",
    "manual_summmary = \"The use of invisible ink and ultraviolet readers in the elections of the Kyrgyz Republic which is a small, mountainous state of the former Soviet republic, causing both worries and guarded optimism among different sectors of the population. Though the actual technology behind the ink is not complicated, the presence of ultraviolet light (of the kind used to verify money) causes the ink to glow with a neon yellow light. But, this use of the new technology has caused a lot of problems. \"\n",
    "\n",
    "[precision, recall, f_score] = r.rouge_l([system_generated_summary], [manual_summmary])\n",
    "\n",
    "print(\"Precision is :\"+str(precision)+\"\\nRecall is :\"+str(recall)+\"\\nF Score is :\"+str(f_score))\n",
    "\n",
    "#output\n",
    "\"\"\"\n",
    "Precision is :0.446058091286\n",
    "Recall is :0.439672801636\n",
    "F Score is :0.442843380487"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
