{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "simple transformer_other_dataset_adam_15k_50words_2datasets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R53kjrYGpSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfbae9b3-14d3-4ee0-89bf-aa0b104a2916"
      },
      "source": [
        "!git clone https://github.com/moaaztaha/Arabic-English-Translation-Transformers"
      ],
      "id": "0R53kjrYGpSc",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Arabic-English-Translation-Transformers'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 24 (delta 0), reused 24 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9e97fa1"
      },
      "source": [
        "# modules\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "id": "e9e97fa1",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1710411f"
      },
      "source": [
        "### Data Preprocessing "
      ],
      "id": "1710411f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RNDMX9cizGW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "c0ce067d-0a8d-413b-a36a-8530c3449f8d"
      },
      "source": [
        "en = pd.read_table('/content/Arabic-English-Translation-Transformers/data/eng/ac-test.en', delimiter='\\\\n', names=['en'])\n",
        "ar = pd.read_table('/content/Arabic-English-Translation-Transformers/data/ara/test.en_ref.ar', delimiter='\\\\n', names=['ar'])\n",
        "en['ar'] = ar['ar']\n",
        "df = en.copy()\n",
        "df.head()"
      ],
      "id": "1RNDMX9cizGW",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return read_csv(**locals())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>ar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>THE COUNCIL OF THE EUROPEAN ECONOMIC COMMUNITY,</td>\n",
              "      <td>مجلس الجماعة الاقتصادية الأوروبية</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Whereas the adoption of a common transport pol...</td>\n",
              "      <td>حيث أن اعتماد سياسة نقل مشتركة تنطوي من بين أم...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Article 1</td>\n",
              "      <td>المادة 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3. The types of carriage listed in Annex II sh...</td>\n",
              "      <td>3. لا تخضع أنواع النقل المدرجة في الملحق الثان...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Member States shall inform the Commission of t...</td>\n",
              "      <td>تبلغ الدول الأعضاء المفوضية الأوروبية بالتدابي...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  en                                                 ar\n",
              "0    THE COUNCIL OF THE EUROPEAN ECONOMIC COMMUNITY,                  مجلس الجماعة الاقتصادية الأوروبية\n",
              "1  Whereas the adoption of a common transport pol...  حيث أن اعتماد سياسة نقل مشتركة تنطوي من بين أم...\n",
              "2                                          Article 1                                           المادة 1\n",
              "3  3. The types of carriage listed in Annex II sh...  3. لا تخضع أنواع النقل المدرجة في الملحق الثان...\n",
              "4  Member States shall inform the Commission of t...  تبلغ الدول الأعضاء المفوضية الأوروبية بالتدابي..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be905b2c"
      },
      "source": [
        "text_pairs = []\n",
        "for idx, row in df.iterrows():\n",
        "    # split sentences\n",
        "    if '.' in row['en'] and '.' in row['ar'] and len(row['en'].split()) > 100:\n",
        "        en_sents = row['en'].split('.')\n",
        "        ar_sents = row['ar'].split('.')\n",
        "    \n",
        "        for en_sent, ar_sent in zip(en_sents, ar_sents):\n",
        "            ar_sent = \"[start] \" + ar_sent + \" [end]\"\n",
        "            text_pairs.append((en_sent, ar))\n",
        "    else:\n",
        "        en, ar = row['en'], row['ar']\n",
        "        ar = \"[start] \" + ar + \" [end]\"\n",
        "        text_pairs.append((en, ar))"
      ],
      "id": "be905b2c",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "296e4594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4a1f96-3efe-4716-d59f-1bcd014df0ad"
      },
      "source": [
        "for _ in range(2):\n",
        "    print(random.choice(text_pairs))"
      ],
      "id": "296e4594",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('With respect to the notifications and information submitted in conformity with Articles 7 (1) and (2), 8 (1), (2) and (3), industrial and commercial secrecy shall not apply to:', '[start] في ما يتعلق بالإشعارات والمعلومات المقدمة وفقاً للمواد 7(1) و (2)، و8 (1) و(2) و(3)، لا يجب تطبيق الأسرار الصناعية والتجارية على: [end]')\n",
            "(\"- the concentration specified at point 6 of Annex I (Table VI) to this Directive where the substance or substances under consideration do not appear in Annex I to Directive 67/548/EEC or appear in it without concentration limits;',\", '[start] - التركيز المحدد في النقطة 6 من الملحق I (الجدول VI) من هذا التوجيه حيث تكون المادة أو المواد التي هي قيد البحث غير مدرجة في الملحق I من التوجيه رقم 67/548/EEC أو مدرجة فيه من دون حدود متعقة بالتركيز؛‘ [end]')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6kRoro9iBOe",
        "outputId": "c13db85c-5ffd-40b4-fcdb-2e87e34ae9cc"
      },
      "source": [
        "len(text_pairs)"
      ],
      "id": "o6kRoro9iBOe",
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "c9H3kM4OiDWv",
        "outputId": "b0cb9a72-23d3-4730-801d-e51ad4c56128"
      },
      "source": [
        "en = pd.read_table('/content/Arabic-English-Translation-Transformers/data/eng/ac-dev.en', delimiter='\\\\n', names=['en'])\n",
        "ar = pd.read_table('/content/Arabic-English-Translation-Transformers/data/ara/tune.en_ref.ar', delimiter='\\\\n', names=['ar'])\n",
        "#2725 to 3742\n",
        "ar.drop(ar.loc[2725:3742].index,inplace=True)\n",
        "#2720 to 3707\n",
        "en.drop(en.loc[2725:3742].index,inplace=True)\n",
        "en['ar'] = ar['ar']\n",
        "df = en.copy()\n",
        "df.head()"
      ],
      "id": "c9H3kM4OiDWv",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return read_csv(**locals())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>en</th>\n",
              "      <th>ar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Having regard to the Treaty establishing the E...</td>\n",
              "      <td>مع الأخذ في الاعتبار المعاهدة التي أنشئت بموجب...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Whereas the progressive establishment of the c...</td>\n",
              "      <td>وحيث أنه لا يجب أن تواجه عملية الإنشاء التدريج...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1. Each Member State shall, by the end of 1962...</td>\n",
              "      <td>1. يتعيّن على كلّ دولة عضو بحلول نهاية العام 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4. The two Annexes to this Directive shall for...</td>\n",
              "      <td>4 يشكّل الملحقان المرفقان بهذا التوجيه جزءاً ل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Article 3</td>\n",
              "      <td>المادة 3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  en                                                 ar\n",
              "0  Having regard to the Treaty establishing the E...  مع الأخذ في الاعتبار المعاهدة التي أنشئت بموجب...\n",
              "1  Whereas the progressive establishment of the c...  وحيث أنه لا يجب أن تواجه عملية الإنشاء التدريج...\n",
              "2  1. Each Member State shall, by the end of 1962...  1. يتعيّن على كلّ دولة عضو بحلول نهاية العام 1...\n",
              "3  4. The two Annexes to this Directive shall for...  4 يشكّل الملحقان المرفقان بهذا التوجيه جزءاً ل...\n",
              "4                                          Article 3                                           المادة 3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEsz9VaiiNfw"
      },
      "source": [
        "for idx, row in df.iterrows():\n",
        "    # split sentences\n",
        "    if '.' in row['en'] and '.' in row['ar'] and len(row['en'].split()) > 100:\n",
        "        en_sents = row['en'].split('.')\n",
        "        ar_sents = row['ar'].split('.')\n",
        "    \n",
        "        for en_sent, ar_sent in zip(en_sents, ar_sents):\n",
        "            ar_sent = \"[start] \" + ar_sent + \" [end]\"\n",
        "            text_pairs.append((en_sent, ar))\n",
        "    else:\n",
        "        en, ar = row['en'], row['ar']\n",
        "        ar = \"[start] \" + ar + \" [end]\"\n",
        "        text_pairs.append((en, ar))"
      ],
      "id": "yEsz9VaiiNfw",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1283f6f"
      },
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) -  num_val_samples\n",
        "train_pairs = text_pairs[: num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples: num_train_samples + num_val_samples]"
      ],
      "id": "c1283f6f",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7a1f8cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6bced4a-693a-4534-d242-3fa282c6a3d4"
      },
      "source": [
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ],
      "id": "a7a1f8cd",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7421 total pairs\n",
            "6308 training pairs\n",
            "1113 validation pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5f6d84e"
      },
      "source": [
        "#### Vectorizing the text data "
      ],
      "id": "d5f6d84e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "487ed66f"
      },
      "source": [
        "strip_chars = string.punctuation\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 50\n",
        "batch_size = 265\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    return tf.strings.regex_replace(input_string, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "            max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\n",
        "\n",
        "ar_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization)\n",
        "\n",
        "eng_texts = [pair[0] for pair in text_pairs]\n",
        "ar_texts = [pair[1] for pair in text_pairs]\n",
        "eng_vectorization.adapt(eng_texts)\n",
        "ar_vectorization.adapt(ar_texts)"
      ],
      "id": "487ed66f",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5qiqjOWdVz1"
      },
      "source": [
        "def format_dataset(eng, ar):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ar = ar_vectorization(ar)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ar[:, :-1],}, ar[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, ar_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    ar_texts = list(ar_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, ar_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()"
      ],
      "id": "V5qiqjOWdVz1",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a3ed140"
      },
      "source": [
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "id": "7a3ed140",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3902789f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515e6428-a8a0-4f09-d3bf-a160f8b52955"
      },
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "id": "3902789f",
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (265, 50)\n",
            "inputs[\"decoder_inputs\"].shape: (265, 50)\n",
            "targets.shape: (265, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fa0936c"
      },
      "source": [
        "### Building the Model "
      ],
      "id": "4fa0936c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3baf452"
      },
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "          'embed_dim': self.embed_dim,\n",
        "          'dense_dim': self.dense_dim,\n",
        "          'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "      \n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'sequence_length': self.sequence_length,\n",
        "      'vocab_size': self.vocab_size,\n",
        "      'embed_dim': self.embed_dim,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "      config = super().get_config().copy()\n",
        "      config.update({\n",
        "      'embed_dim': self.embed_dim,\n",
        "      'latent_dim': self.latent_dim,\n",
        "      'num_heads': self.num_heads,\n",
        "      })\n",
        "      return config\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)"
      ],
      "id": "b3baf452",
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c57e1e"
      },
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "id": "04c57e1e",
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt9sp6Yrb4pE"
      },
      "source": [
        "googledrive_path = '/content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/'"
      ],
      "id": "vt9sp6Yrb4pE",
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqmaEQ94kyB9"
      },
      "source": [
        "from keras import callbacks\n",
        "early_stopping_cb = callbacks.EarlyStopping(monitor='val_accuracy', patience=4, verbose=True)\n",
        "checkpoint_cb = callbacks.ModelCheckpoint(googledrive_path+'weights_adam.ckpt', monitor='val_accuracy', save_weights_only=True,verbose=True, save_best_only=True)\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=googledrive_path+\"logs\")\n",
        "cbs = [early_stopping_cb, checkpoint_cb, tensorboard_callback]"
      ],
      "id": "wqmaEQ94kyB9",
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50DkdfyLyqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d68da68f-e3f4-4feb-fd2e-79db28716881"
      },
      "source": [
        "epochs = 30  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=cbs)"
      ],
      "id": "H50DkdfyLyqt",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_embedding_8 (Positio (None, None, 256)    3852800     encoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_4 (Transfor (None, None, 256)    3155456     positional_embedding_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "model_9 (Functional)            (None, None, 15000)  12967320    decoder_inputs[0][0]             \n",
            "                                                                 transformer_encoder_4[0][0]      \n",
            "==================================================================================================\n",
            "Total params: 19,975,576\n",
            "Trainable params: 19,975,576\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "24/24 [==============================] - 18s 630ms/step - loss: 3.3773 - accuracy: 0.0914 - val_loss: 3.0068 - val_accuracy: 0.1390\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.13896, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 2/30\n",
            "24/24 [==============================] - 14s 573ms/step - loss: 2.7734 - accuracy: 0.1698 - val_loss: 2.6972 - val_accuracy: 0.2104\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.13896 to 0.21036, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 3/30\n",
            "24/24 [==============================] - 13s 540ms/step - loss: 2.4147 - accuracy: 0.2487 - val_loss: 2.4506 - val_accuracy: 0.2580\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.21036 to 0.25804, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 4/30\n",
            "24/24 [==============================] - 13s 539ms/step - loss: 2.1224 - accuracy: 0.3033 - val_loss: 2.2997 - val_accuracy: 0.2844\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.25804 to 0.28436, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 5/30\n",
            "24/24 [==============================] - 13s 560ms/step - loss: 1.8995 - accuracy: 0.3459 - val_loss: 2.2123 - val_accuracy: 0.3062\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.28436 to 0.30620, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 6/30\n",
            "24/24 [==============================] - 13s 564ms/step - loss: 1.7210 - accuracy: 0.3836 - val_loss: 2.1542 - val_accuracy: 0.3260\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.30620 to 0.32600, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 7/30\n",
            "24/24 [==============================] - 13s 551ms/step - loss: 1.5685 - accuracy: 0.4187 - val_loss: 2.1228 - val_accuracy: 0.3297\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.32600 to 0.32969, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 8/30\n",
            "24/24 [==============================] - 13s 536ms/step - loss: 1.4288 - accuracy: 0.4517 - val_loss: 2.0927 - val_accuracy: 0.3418\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.32969 to 0.34185, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 9/30\n",
            "24/24 [==============================] - 13s 542ms/step - loss: 1.2978 - accuracy: 0.4831 - val_loss: 2.0831 - val_accuracy: 0.3460\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.34185 to 0.34599, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 10/30\n",
            "24/24 [==============================] - 14s 563ms/step - loss: 1.1776 - accuracy: 0.5138 - val_loss: 2.0825 - val_accuracy: 0.3511\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.34599 to 0.35109, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 11/30\n",
            "24/24 [==============================] - 13s 554ms/step - loss: 1.0678 - accuracy: 0.5419 - val_loss: 2.0981 - val_accuracy: 0.3544\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.35109 to 0.35437, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 12/30\n",
            "24/24 [==============================] - 13s 550ms/step - loss: 0.9716 - accuracy: 0.5680 - val_loss: 2.1198 - val_accuracy: 0.3487\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.35437\n",
            "Epoch 13/30\n",
            "24/24 [==============================] - 13s 536ms/step - loss: 0.8790 - accuracy: 0.5985 - val_loss: 2.1477 - val_accuracy: 0.3521\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.35437\n",
            "Epoch 14/30\n",
            "24/24 [==============================] - 13s 547ms/step - loss: 0.7887 - accuracy: 0.6341 - val_loss: 2.1759 - val_accuracy: 0.3555\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.35437 to 0.35548, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 15/30\n",
            "24/24 [==============================] - 13s 547ms/step - loss: 0.6974 - accuracy: 0.6699 - val_loss: 2.1685 - val_accuracy: 0.3603\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.35548 to 0.36033, saving model to /content/drive/MyDrive/Transformers/new_ds_20k_adam_50words2ds/weights_adam.ckpt\n",
            "Epoch 16/30\n",
            "24/24 [==============================] - 13s 562ms/step - loss: 0.6072 - accuracy: 0.7083 - val_loss: 2.2041 - val_accuracy: 0.3600\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.36033\n",
            "Epoch 17/30\n",
            "24/24 [==============================] - 13s 544ms/step - loss: 0.5267 - accuracy: 0.7473 - val_loss: 2.2731 - val_accuracy: 0.3596\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.36033\n",
            "Epoch 18/30\n",
            "24/24 [==============================] - 13s 549ms/step - loss: 0.4582 - accuracy: 0.7783 - val_loss: 2.2711 - val_accuracy: 0.3600\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.36033\n",
            "Epoch 19/30\n",
            "24/24 [==============================] - 13s 548ms/step - loss: 0.3958 - accuracy: 0.8095 - val_loss: 2.3047 - val_accuracy: 0.3580\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.36033\n",
            "Epoch 00019: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f909c2533d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5Qvebl8FHsG",
        "outputId": "08cbe35e-c504-4ea6-a7c6-5e4de644dd22"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(googledrive_path)\n",
        "transformer.load_weights(latest)"
      ],
      "id": "j5Qvebl8FHsG",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f90ac213250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8447490b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71032c1b-4058-481d-bee4-a118b7d8f370"
      },
      "source": [
        "ar_vocab = ar_vectorization.get_vocabulary()\n",
        "ar_index_lookup = dict(zip(range(len(ar_vocab)), ar_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ar_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ar_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in val_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts[:30])\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, '\\n', translated)\n",
        "    print('*'*50)"
      ],
      "id": "8447490b",
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For the purposes of making out the declarations referred to in Articles 2 and 4, \"other wines\" means wines obtained from grapes of varieties listed in the classification of vine varieties drawn up by the Member States in accordance with Article 19 of Regulation (EC) No 1493/1999 for the same administrative unit as both wine-grape varieties and, as the case may be, table-grape varieties, varieties for the production of dried grapes or varieties for the production of wine spirits. \n",
            " [start] لأغراض هذا التوجيه، يُقصد بمصطلح المركبات المزودة بمحركات أي فروقات بين المادتَيْن 2 و3 و4، يجب أن تكون مصنع التقطير\n",
            "**************************************************\n",
            "Whereas Directive No 64/221/EEC should therefore apply to persons to whom Directive No 75/34/EEC applies, \n",
            " [start] حيث أن توجيه رقم [UNK] المؤرخ في التوجيه رقم 67548 4، [end]\n",
            "**************************************************\n",
            "Article 1 \n",
            " [start] المادة 1 [end]\n",
            "**************************************************\n",
            "Whereas the expenditure incurred by the Member States as a result of the obligations arising out of the application of this Regulation fall on the Community in accordance with the provisions of Articles 2 and 3 of Council Regulation (EEC) No 729/70 (1) of 21 April 1970 on the financing of the common agricultural policy, as amended by Regulation (EEC) No 1566/72 (2), \n",
            " [start] حيث أن متطلبات الإخطار المقدمة من كل دولة عضو أخرى تستند إلى الدول الأعضاء الأخرى في حال غياب القيمة المضافة\n",
            "**************************************************\n",
            "- security features. \n",
            " [start] القيمة المرجعية [end]\n",
            "**************************************************\n",
            "Whereas the expenditure incurred by the Member States as a result of the obligations arising out of the application of this Regulation fall on the Community in accordance with the provisions of Articles 2 and 3 of Council Regulation (EEC) No 729/70 (1) of 21 April 1970 on the financing of the common agricultural policy, as amended by Regulation (EEC) No 1566/72 (2), \n",
            " [start] حيث أن متطلبات الإخطار المقدمة من كل دولة عضو أخرى تستند إلى الدول الأعضاء الأخرى في حال غياب القيمة المضافة\n",
            "**************************************************\n",
            "(b) the EURES partners, as provided for in Article 17(1) of Regulation (EEC) No 1612/68; namely: \n",
            " [start] ب يتم حذف مكتب التنسيق المنصوص عليه في المادة 8 [end]\n",
            "**************************************************\n",
            "However, the conditions under (b) and (c) shall not apply to printed matter relating to either goods for sale or hire or services offered by a person established in another Member State provided that the printed matter has been imported, and will be distributed, free of charge.' \n",
            " [start] مع ذلك، يجوز بموجب شروط وإجراءات الرقابة المستخدمة، ج و ج قد يتم الحصول على طرح أجهزة التبريد في الأسواق،\n",
            "**************************************************\n",
            "COMMISSION DECISION of 25 September 1992 establishing the form of cooperation between the Animo host centre and Member States (92/486/EEC) \n",
            " [start] قرار الهيئة المؤرخ في 25 حزيران 1992 الذي يضع المتطلبات والإجراءات المتخذة وعند الاقتضاء، [end]\n",
            "**************************************************\n",
            "- by virtue of the law applicable to him, or \n",
            " [start] بموجب المادة 32 2، [end]\n",
            "**************************************************\n",
            "For the goods referred to in the second sentence of (a), Member States may require: \n",
            " [start] لأغراض هذا التوجيه، أو الخدمات المشار إليها في المادة [end]\n",
            "**************************************************\n",
            "- by virtue of the law applicable to him, or \n",
            " [start] بموجب المادة 32 2، [end]\n",
            "**************************************************\n",
            "- by virtue of the law applicable to him, or \n",
            " [start] بموجب المادة 32 2، [end]\n",
            "**************************************************\n",
            "For the purposes of making out the declarations referred to in Articles 2 and 4, \"other wines\" means wines obtained from grapes of varieties listed in the classification of vine varieties drawn up by the Member States in accordance with Article 19 of Regulation (EC) No 1493/1999 for the same administrative unit as both wine-grape varieties and, as the case may be, table-grape varieties, varieties for the production of dried grapes or varieties for the production of wine spirits. \n",
            " [start] لأغراض هذا التوجيه، يُقصد بمصطلح المركبات المزودة بمحركات أي فروقات بين المادتَيْن 2 و3 و4، يجب أن تكون مصنع التقطير\n",
            "**************************************************\n",
            "Article 7 \n",
            " [start] المادة 7 [end]\n",
            "**************************************************\n",
            "Article 8 \n",
            " [start] المادة 8 [end]\n",
            "**************************************************\n",
            "1. This Directive shall apply to all sectors of activity, both public and private (industrial, agricultural, commercial, administrative, service, educational, cultural, leisure, etc.). \n",
            " [start] 1 يتعين على هذا التوجيه إلى كافة كافة وهو ينطبق على الدول الأعضاء والهيئة؛ [end]\n",
            "**************************************************\n",
            "COMMISSION DECISION of 25 September 1992 establishing the form of cooperation between the Animo host centre and Member States (92/486/EEC) \n",
            " [start] قرار الهيئة المؤرخ في 25 حزيران 1992 الذي يضع المتطلبات والإجراءات المتخذة وعند الاقتضاء، [end]\n",
            "**************************************************\n",
            "Article 7 \n",
            " [start] المادة 7 [end]\n",
            "**************************************************\n",
            "However, the conditions under (b) and (c) shall not apply to printed matter relating to either goods for sale or hire or services offered by a person established in another Member State provided that the printed matter has been imported, and will be distributed, free of charge.' \n",
            " [start] مع ذلك، يجوز بموجب شروط وإجراءات الرقابة المستخدمة، ج و ج قد يتم الحصول على طرح أجهزة التبريد في الأسواق،\n",
            "**************************************************\n",
            "- by virtue of the law applicable to him, or \n",
            " [start] بموجب المادة 32 2، [end]\n",
            "**************************************************\n",
            "- by virtue of the law applicable to him, or \n",
            " [start] بموجب المادة 32 2، [end]\n",
            "**************************************************\n",
            "COMMISSION DECISION of 30 November 1994 laying down special conditions for the import of live bivalve molluscs, echinoderms, tunicates and marine gastropods originating in Turkey (Text with EEA relevant) (94/777/EC) \n",
            " [start] قرار الهيئة المؤرخ في 30 تشرين الأول 1994 الذي يضع الشروط الخاصة باعتلالات الدماغ [UNK] يمكن استخدام مسبق وبطريقة مماثلة\n",
            "**************************************************\n",
            "- by virtue of the law applicable to him, or \n",
            " [start] بموجب المادة 32 2، [end]\n",
            "**************************************************\n",
            "COMMISSION REGULATION (EC) No 1245/1999 \n",
            " [start] لائحة المفوضية المفوضية الأوروبية رقم ‎7301999 [end]\n",
            "**************************************************\n",
            "(b) the EURES partners, as provided for in Article 17(1) of Regulation (EEC) No 1612/68; namely: \n",
            " [start] ب يتم حذف مكتب التنسيق المنصوص عليه في المادة 8 [end]\n",
            "**************************************************\n",
            "COMMISSION DECISION of 30 November 1994 laying down special conditions for the import of live bivalve molluscs, echinoderms, tunicates and marine gastropods originating in Turkey (Text with EEA relevant) (94/777/EC) \n",
            " [start] قرار الهيئة المؤرخ في 30 تشرين الأول 1994 الذي يضع الشروط الخاصة باعتلالات الدماغ [UNK] يمكن استخدام مسبق وبطريقة مماثلة\n",
            "**************************************************\n",
            "1. If a Member State notes, on the basis of a substantive justification, that one or more aerosol dispensers, although complying with the requirements of the Directive, represent a hazard to safety or health, it may provisionally prohibit the sale of the dispenser or dispensers in its territory or subject it or them to special conditions. It shall immediately inform the other Member States and the Commission thereof, stating the grounds for its decision. \n",
            " [start] 1 إذا لم يتم نقل المياه خارج الولاية القضائية أو إذا كان ذلك الأسهم في شهادة الموافقة على الوسائل القانونية\n",
            "**************************************************\n",
            "- natural spices, aromatic herbs and their extracts, and natural aromas. \n",
            " [start] التوابل الطبيعية والأعشاب العطرية ومستخلصاتها، والعطور الطبيعية [end]\n",
            "**************************************************\n",
            "(4) Whereas it is important that the security of oil supply is enhanced; \n",
            " [start] 4 حيث إنه قد تم الحصول على مستوى أفضل التواريخ المبينة في إطار [end]\n",
            "**************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}